[
    {
        "url": "https://paperity.org/p/229977174/toward-multi-label-sentiment-analysis-a-transfer-learning-based-approach",
        "title": "Toward multi-label sentiment analysis: a transfer learning based approach",
        "authors": [
            "Jie Tao",
            " Xing Fang"
        ],
        "date_article": "02-2020",
        "short_description": "Sentiment analysis is recognized as one of the most important sub-areas in Natural Language Processing (NLP) research, where understanding implicit or explicit sentiments expressed in social media contents is valuable to customers, business owners, and other stakeholders. Researchers have recognized that the generic sentiments extracted from the textual contents are inadequate...",
        "keywords": [
            "Transfer learning",
            "Multi-label classification",
            "Sentiment analysis",
            "Natural language processing",
            "Deep learning"
        ],
        "number_of_pages": "26",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0278-0.pdf",
        "abstract": "Sentiment analysis is recognized as one of the most important sub-areas in Natural Language Processing (NLP) research, where understanding implicit or explicit senti-ments expressed in social media contents is valuable to customers, business owners, and other stakeholders. Researchers have recognized that the generic sentiments extracted from the textual contents are inadequate, thus, Aspect Based Sentiment Analysis (ABSA) was coined to capture aspect sentiments expressed toward specific review aspects. Existing ABSA methods not only treat the analytical problem as single-label classification that requires a fairly large amount of labelled data for model training purposes, but also underestimate the entity aspects that are independent of certain sentiments. In this study, we propose a transfer learning based approach tackling the aforementioned shortcomings of existing ABSA methods. Firstly, the proposed approach extends the ABSA methods with multi-label classification capabilities. Sec-ondly, we propose an advanced sentiment analysis method, namely Aspect Enhanced Sentiment Analysis (AESA) to classify text into sentiment classes with consideration of the entity aspects. Thirdly, we extend two state-of-the-art transfer learning models as the analytical vehicles of multi-label ABSA and AESA tasks. We design an experi-ment that includes data from different domains to extensively evaluate the proposed approach. The empirical results undoubtedly exhibit that the proposed approach outperform all the baseline approaches."
    },
    {
        "url": "https://paperity.org/p/230095553/criminal-tendency-detection-from-facial-images-and-the-gender-bias-effect",
        "title": "Criminal tendency detection from facial images and the gender bias effect",
        "authors": [
            "Mahdi Hashemi",
            " Margeret H"
        ],
        "date_article": "02-2020",
        "short_description": "Explosive performance and memory space growth in computing machines, along with recent specialization of deep learning models have radically boosted the role of images in semantic pattern recognition. In the same way that a textual post on social media reveals individual characteristics of its author, facial images may manifest some personality traits. This work is the first...",
        "keywords": [
            "Image classification",
            "Facial images",
            "Convolutional neural network",
            "Deep learning",
            "Machine learning",
            "Personality traits"
        ],
        "number_of_pages": "16",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0282-4.pdf",
        "abstract": "Explosive performance and memory space growth in computing machines, along with recent specialization of deep learning models have radically boosted the role of images in semantic pattern recognition. In the same way that a textual post on social media reveals individual characteristics of its author, facial images may manifest some personality traits. This work is the first milestone in our attempt to infer personality traits from facial images. With this ultimate goal in mind, here we explore a new level of image understanding, inferring criminal tendency from facial images via deep learn-ing. In particular, two deep learning models, including a standard feedforward neural network (SNN) and a convolutional neural network (CNN) are applied to discriminate criminal and non-criminal facial images. Confusion matrix and training and test accura-cies are reported for both models, using tenfold cross-validation on a set of 10,000 facial images. The CNN was more consistent than the SNN in learning to reach its best test accuracy, which was 8% higher than the SNN’s test accuracy. Next, to explore the classifier’s hypothetical bias due to gender, we controlled for gender by applying only male facial images. No meaningful discrepancies in classification accuracies or learning consistencies were observed, suggesting little to no gender bias in the classifier. Finally, dissecting and visualizing convolutional layers in CNN showed that the shape of the face, eyebrows, top of the eye, pupils, nostrils, and lips are taken advantage of by CNN in order to classify the two sets of images."
    },
    {
        "url": "https://paperity.org/p/229951788/graphical-flow-based-spark-programming",
        "title": "Graphical Flow-based Spark Programming",
        "authors": [
            "Tanmaya Mahapatra",
            " Christian Prehofer"
        ],
        "date_article": "02-2020",
        "short_description": "Increased sensing data in the context of the Internet of Things (IoT) necessitates data analytics. It is challenging to write applications for Big Data systems due to complex, highly parallel software frameworks and systems. The inherent complexity in programming Big Data applications is also due to the presence of a wide range of target frameworks, with different data...",
        "keywords": [
            "Spark pipelines",
            "IoT mashup tools",
            "Graphical tools",
            "Stream analytics",
            "Flow-based programming"
        ],
        "number_of_pages": "47",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0273-5.pdf",
        "abstract": "Increased sensing data in the context of the Internet of Things (IoT ) necessitates data analytics. It is challenging to write applications for Big Data systems due to complex, highly parallel software frameworks and systems. The inherent complexity in pro-gramming Big Data applications is also due to the presence of a wide range of target frameworks, with different data abstractions and APIs. The paper aims to reduce this complexity and its ensued learning curve by enabling domain experts, that are not necessarily skilled Big Data programmers, to develop data analytics applications via domain-specific graphical tools. The approach follows the flow-based programming paradigm used in IoT mashup tools. The paper contributes to these aspects by (i) providing a thorough analysis and classification of the widely used Spark framework and selecting suitable data abstractions and APIs for use in a graphical flow-based programming paradigm and (ii) devising a novel, generic approach for programming Spark from graphical flows that comprises early-stage validation and code generation of Spark applications. Use cases for Spark have been prototyped and evaluated to dem-onstrate code-abstraction, automatic data abstraction interconversion and automatic generation of target Spark programs, which are the keys to lower the complexity and its ensued learning curve involved in the development of Big Data applications."
    },
    {
        "url": "https://paperity.org/p/229955155/data-science-developing-theoretical-contributions-in-information-systems-via-text",
        "title": "Data science: developing theoretical contributions in information systems via text analytics",
        "authors": [
            "Aya Rizk",
            " Ahmed Elrag"
        ],
        "date_article": "02-2020",
        "short_description": "Scholars have been increasingly calling for innovative research in the organizational sciences in general, and the information systems (IS) field in specific, one that breaks from the dominance of gap-spotting and specific methodical confinements. Hence, pushing the boundaries of information systems is needed, and one way to do so is by relying more on data and less on a priori...",
        "keywords": [
            "Data science",
            "Theory",
            "Contribution",
            "Information systems",
            "Text analytics",
            "Methodology"
        ],
        "number_of_pages": "26",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0280-6.pdf",
        "abstract": "Scholars have been increasingly calling for innovative research in the organizational sciences in general, and the information systems (IS) field in specific, one that breaks from the dominance of gap-spotting and specific methodical confinements. Hence, pushing the boundaries of information systems is needed, and one way to do so is by relying more on data and less on a priori theory. Data, being considered one of the most important resources in research, and society at large, requires the application of scientific methods to extract valuable knowledge towards theoretical development. However, the nature of knowledge varies from a scientific discipline to another, and the views on data science (DS) studies are substantially diverse. These views vary from being seen as a new scientific (fourth) paradigm, to an extension of existing paradigms with new tools and methods, to a phenomenon or object of study. In this paper, we review these perspectives and expand on the view of data science as a methodology for scientific inquiry. Motivated by the IS discipline’s history and accumulated knowl-edge in using DS methods for understanding organizational and societal phenom-ena, IS theory and theoretical contributions are given particular attention as the key outcome of adopting such methodology. Exemplar studies are analyzed to show how rigor can be achieved, and an illustrative example using text analytics to study digital innovation is provided to guide researchers."
    },
    {
        "url": "https://paperity.org/p/230073534/anomaly-detection-in-business-processes-using-process-mining-and-fuzzy-association-rule",
        "title": "Anomaly detection in business processes using process mining and fuzzy association rule learning",
        "authors": [
            "Riyanarto Sarno",
            " Fernandes Sinaga",
            " Kelly Rossa Sungkono"
        ],
        "date_article": "02-2020",
        "short_description": "Much corporate organization nowadays implement enterprise resource planning (ERP) to manage their business processes. Because the processes run continuously, ERP produces a massive log of processes. Manual observation will have difficulty monitoring the enormous log, especially detecting anomalies. It needs the method that can detect anomalies in the large log. This paper...",
        "keywords": [
            "Process mining",
            "Anomaly detection",
            "Fuzzy association rule learning"
        ],
        "number_of_pages": "19",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0277-1.pdf",
        "abstract": "Much corporate organization nowadays implement enterprise resource planning (ERP) to manage their business processes. Because the processes run continuously, ERP pro-duces a massive log of processes. Manual observation will have difficulty monitoring the enormous log, especially detecting anomalies. It needs the method that can detect anomalies in the large log. This paper proposes the integration of process mining, fuzzy multi-attribute decision making and fuzzy association rule learning to detect anoma-lies. Process mining analyses the conformance between recorded event logs and standard operating procedures. The fuzzy multi-attribute decision making is applied to determine the anomaly rates. Finally, the fuzzy association rule learning develops association rules that will be employed to detect anomalies. The results of our experi-ment showed that the accuracy of the association rule learning method was 0.975 with a minimum confidence level of 0.9 and that the accuracy of the fuzzy association rule learning method was 0.925 with a minimum confidence level of 0.3. Therefore, the fuzzy association rule learning method can detect fraud at low confidence levels."
    },
    {
        "url": "https://paperity.org/p/229929769/decreasing-the-execution-time-of-reducers-by-revising-clustering-based-on-the-futuristic",
        "title": "Decreasing the execution time of reducers by revising clustering based on the futuristic greedy approach",
        "authors": [
            "Ali Bakhthemmat",
            " Mohammad Izadi"
        ],
        "date_article": "02-2020",
        "short_description": "MapReduce is used within the Hadoop framework, which handles two important tasks: mapping and reducing. Data clustering in mappers and reducers can decrease the execution time, as similar data can be assigned to the same reducer with one key. Our proposed method decreases the overall execution time by clustering and lowering the number of reducers. Our proposed algorithm is...",
        "keywords": [
            "Futuristic greedy",
            "MapReduce",
            "Load balancing",
            "Decreasing the number of reducers"
        ],
        "number_of_pages": "21",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0279-z.pdf",
        "abstract": "MapReduce is used within the Hadoop framework, which handles two important tasks: mapping and reducing. Data clustering in mappers and reducers can decrease the execution time, as similar data can be assigned to the same reducer with one key. Our proposed method decreases the overall execution time by clustering and lowering the number of reducers. Our proposed algorithm is composed of five phases. In the first phase, data are stored in the Hadoop structure. In the second phase, we cluster data using the MR-DBSCAN-KD method in order to determine all of the outliers and clusters. Then, the outliers are assigned to the existing clusters using the futuristic greedy method. At the end of the second phase, similar clusters are merged together. In the third phase, clusters are assigned to the reducers. Note that fewer reducers are required for this task by applying approximated load balancing between the reducers. In the fourth phase, the reducers execute their jobs in each cluster. Eventually, in the final phase, reducers return the output. Decreasing the number of reducers and revising the clustering helped reducers to perform their jobs almost simultaneously. Our research results indicate that the proposed algorithm improves the execution time by about 3.9% less than the fastest algorithm in our experiments."
    },
    {
        "url": "https://paperity.org/p/230048148/learning-based-power-prediction-for-geo-distributed-data-centers-weather-parameter",
        "title": "Learning-based power prediction for geo-distributed Data Centers: weather parameter analysis",
        "authors": [
            "Somayyeh Taheri",
            " Maziar Goudarzi",
            " Osamu Yoshi"
        ],
        "date_article": "01-2020",
        "short_description": "Nowadays, the fast rate of technological advances, such as cloud computing, has led to the rapid growth of the Data Center (DC) market as well as their power consumption. Therefore, DC power management has become increasingly important. While power forecasting can greatly help DC power management and reduce energy consumption and cost. Power forecasting predicts the potential...",
        "keywords": [
            "Cloud computing",
            "Geo-distributed Data Center",
            "Machine learning",
            "Weather-based forecasting"
        ],
        "number_of_pages": "16",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-020-0284-2.pdf",
        "abstract": "Nowadays, the fast rate of technological advances, such as cloud computing, has led to the rapid growth of the Data Center (DC) market as well as their power consumption. Therefore, DC power management has become increasingly important. While power forecasting can greatly help DC power management and reduce energy consumption and cost. Power forecasting predicts the potential energy failures or sudden fluctua-tions in power intake from utility grid. However, it is hard especially when variable renewable energies (RE) as well as free cooling such as air economizers are also used. Geo-distributed DCs face an even harder issue: since in addition to local conditions, the overall status of the entire system of collaborating DCs should also be considered. The conventional approach to forecast power consumption in such complicated cases is to construct a closed form formula for power. This is a tedious task that not only needs expert knowledge of how each single cooling or RE system works, but also needs to be done individually for each DC and repeated all over again for each new DC or change of equipment. One alternative is to use machine learning so as to learn over time how the system consumes power in varying conditions of weather, workload, and internal structure in multiple geo-distributed locations. However, due to the wide range of effective features as well as trade-off between the accuracy and processing overhead, one important issue is to obtain an optimal set of more influential features. In this study, we analyze the correlation among geo-distributed DC power patterns with their weather parameters (based on different DC situations and infrastructure) and extract a set of influential features. Afterward, we apply the obtained features to provide a power consumption forecasting model that predict the power pattern of each collaborating DC in a cloud. Our experimental results show that the proposed prediction model for geo-distributed DCs reaches the accuracy of 87.2%."
    },
    {
        "url": "https://paperity.org/p/230098920/big-data-monetization-throughout-big-data-value-chain-a-comprehensive-review",
        "title": "Big data monetization throughout Big Data Value Chain: a comprehensive review",
        "authors": [
            "Abou Zakaria Faroukhi",
            " Imane El Alaoui",
            " Youssef Gahi"
        ],
        "date_article": "01-2020",
        "short_description": "Value Chain has been considered as a key model for managing efficiently value creation processes within organizations. However, with the digitization of the end-to-end processes which began to adopt data as a main source of value, traditional value chain models have become outdated. For this, researchers have developed new value chain models, called Data Value Chains, to carry...",
        "keywords": [
            "Value Chain",
            "Big Data",
            "Big Data Value Chain",
            "Big Data Management",
            "Big Data Monetization"
        ],
        "number_of_pages": "22",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0281-5.pdf",
        "abstract": "Value Chain has been considered as a key model for managing efficiently value crea-tion processes within organizations. However, with the digitization of the end-to-end processes which began to adopt data as a main source of value, traditional value chain models have become outdated. For this, researchers have developed new value chain models, called Data Value Chains, to carry out data driven organizations. Thereafter, new data value chains called Big Data Value chain have emerged with the emergence of Big Data in order to face new data-related challenges such as high volume, velocity, and variety. These Big Data Value Chains describe the data flow within organizations which rely on Big Data to extract valuable insights. It is a set of ordered steps using Big Data Analytics tools and mainly built for going from data generation to knowledge creation. The advances in Big Data and Big Data Value Chain, using clear processes for aggregation and exploitation of data, have given rise to what is called data monetiza-tion. Data monetization concept consists of using data from an organization to gener-ate profit. It may be selling the data directly for cash, or relying on that data to create value indirectly. It is important to mention that the concept of monetizing data is not as new as it looks, but with the era of Big Data and Big Data Value Chain it is becoming attractive. The aim of this paper is to provide a comprehensive review of value creation, data value, and Big Data value chains with their different steps. This literature has led us to construct an end-to-end exhaustive BDVC that regroup most of the addressed phases. Furthermore, we present a possible evolution of that generic BDVC to support Big Data Monetization. For this, we discuss different approaches that enable data mon-etization throughout data value chains. Finally, we highlight the need to adopt specific data monetization models to suit big data specificities."
    },
    {
        "url": "https://paperity.org/p/230048415/unsupervised-software-defect-prediction-using-median-absolute-deviation-threshold-based",
        "title": "Unsupervised software defect prediction using median absolute deviation threshold based spectral classifier on signed Laplacian matrix",
        "authors": [
            "Aris Marjuni",
            " Teguh B. Adji",
            " Ridi Ferdian"
        ],
        "date_article": "12-2019",
        "short_description": "The trend of current software inevitably leads to the big data era. There are much of large software developed from hundreds to thousands of modules. In software development projects, finding the defect proneness manually on each module in large software dataset is probably inefficient in resources. In this task, the use of a software defect prediction model becomes a popular...",
        "keywords": [
            "Unsupervised software defect prediction",
            "Spectral classifier",
            "Signed Laplacian matrix",
            "Zero thresholding",
            "Median absolute deviation thresholding"
        ],
        "number_of_pages": "20",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0250-z.pdf",
        "abstract": "Area of interest:The trend of current software inevitably leads to the big data era. There are much of large software developed from hundreds to thousands of modules. In software development projects, finding the defect proneness manually on each module in large software dataset is probably inefficient in resources. In this task, the use of a software defect prediction model becomes a popular solution with much more cost-effective rather than manual reviews. This study presents a specific machine learning algorithm, which is the spectral classifier, to develop a software defect predic-tion model using unsupervised learning approach.Background and objective:The spectral classifier has been successfully used in software defect prediction because of its reliability to consider the similarities between software entities. However, there are conditional issues when it uses the zero value as partitioning threshold. The classifier will produce the predominantly cluster when the eigenvector values are mostly positives. Besides, it will also generate low clusters com-pactness when the eigenvector contains outliers. The objective of this study is mainly to propose an alternative partitioning threshold in dealing with the zero threshold issues. Generally, the proposed method is expected to improve the spectral classifier based software defect prediction performances.Methods:This study proposes the median absolute deviation threshold based spec-tral classifier to carry out the zero value threshold issues. The proposed method consid-ers the eigenvector values dispersion measure as the new partitioning threshold, rather than using a central tendency measure (e.g., zero, mean, median). The baseline method of this study is the zero value threshold based spectral classifier. Both methods are performed on the signed Laplacian matrix to meet the non-negative Laplacian graph assumption. For classification, the heuristic row sum method is used to assign the entity class as the prediction label.Results and conclusion:In terms of clustering, the proposed method can produce better cluster memberships that affect the cluster compactness and the classifier performances improvement. The cluster compactness average of both the proposed and baseline methods are 1.4 DBI and 1.8 DBI, respectively. In classification perfor-mance, the proposed method performs better accuracy with lower error rates than Open Access©  The  Author(s)  2019.  This  article  is  distributed  under  the  terms  of  the  Creative  Commons  Attribution  4.0  International  License  (http://creat iveco mmons .org/licen ses/by/4.0/),  which  permits  unrestricted  use,  distribution,  and  reproduction  in  any  medium,  provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made.RESEARCHMarjuni et al. J Big Data            (2019) 6:87  https://doi.org/10.1186/s40537-019-0250-z*Correspondence:   adji@ugm.ac.id 1 Department of Electrical and Information Engineering, Faculty of Engineering, Universitas Gadjah Mada, Jl. Grafika No. 2, Kampus UGM, Yogyakarta 55281, IndonesiaFull list of author information is available at the end of the article\nPage 2 of 20Marjuni et al. J Big Data            (2019) 6:87 the baseline method. The proposed method also has high precision but low in the recall, which means that the proposed method can detect the software defect more precisely, although in the small number in detection. The proposed method has the accuracy, precision, recall, and error rates with average values of 0.79, 0.84, 0.72, and 0.21, respectively. While the baseline method has the accuracy, precision, recall, and error rates with average values of 0.74, 0.74, 0.89, and 0.26, respectively. Based on those results, the proposed method able to provide a viable solution to address the zero threshold issues in the spectral classifier. Hence, this study concludes that the use of the median absolute deviation threshold can improve the spectral based unsupervised software defect prediction method."
    },
    {
        "url": "https://paperity.org/p/229962156/big-data-analysis-and-distributed-deep-learning-for-next-generation-intrusion-detection",
        "title": "Big data analysis and distributed deep learning for next-generation intrusion detection system optimization",
        "authors": [
            "Khloud Al Jallad",
            " Mohamad Aljnidi",
            " Mohammad Said Desouki"
        ],
        "date_article": "12-2019",
        "short_description": "With the growing use of information technology in all life domains, hacking has become more negatively effective than ever before. Also with developing technologies, attacks numbers are growing exponentially every few months and become more sophisticated so that traditional IDS becomes inefficient detecting them. This paper proposes a solution to detect not only new threats with...",
        "keywords": [
            "Intrusion Detection System (IDS)",
            "Long Short Term Memory (LSTM)",
            "Recurrent neural network (RNN)",
            "Distributed deep learning",
            "Big data analysis",
            "Spark",
            "MAWI dataset",
            "MAWILAB gold standard",
            "AGURIM"
        ],
        "number_of_pages": "18",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0248-6.pdf",
        "abstract": "With the growing use of information technology in all life domains, hacking has become more negatively effective than ever before. Also with developing technolo-gies, attacks numbers are growing exponentially every few months and become more sophisticated so that traditional IDS becomes inefficient detecting them. This paper proposes a solution to detect not only new threats with higher detection rate and lower false positive than already used IDS, but also it could detect collective and con-textual security attacks. We achieve those results by using Networking Chatbot, a deep recurrent neural network: Long Short Term Memory (LSTM) on top of Apache Spark Framework that has an input of flow traffic and traffic aggregation and the output is a language of two words, normal or abnormal. We propose merging the concepts of language processing, contextual analysis, distributed deep learning, big data, anomaly detection of flow analysis. We propose a model that describes the network abstract normal behavior from a sequence of millions of packets within their context and analyzes them in near real-time to detect point, collective and contextual anomalies. Experiments are done on MAWI dataset, and it shows better detection rate not only than signature IDS, but also better than traditional anomaly IDS. The experiment shows lower false positive, higher detection rate and better point anomalies detection. As for prove of contextual and collective anomalies detection, we discuss our claim and the reason behind our hypothesis. But the experiment is done on random small subsets of the dataset because of hardware limitations, so we share experiment and our future vision thoughts as we wish that full prove will be done in future by other interested researchers who have better hardware infrastructure than ours."
    },
    {
        "url": "https://paperity.org/p/229940137/smart-literature-review-a-practical-topic-modelling-approach-to-exploratory-literature",
        "title": "Smart literature review: a practical topic modelling approach to exploratory literature review",
        "authors": [
            "Claus Boye Asmussen",
            " Charles Møller"
        ],
        "date_article": "12-2019",
        "short_description": "Manual exploratory literature reviews should be a thing of the past, as technology and development of machine learning methods have matured. The learning curve for using machine learning methods is rapidly declining, enabling new possibilities for all researchers. A framework is presented on how to use topic modelling on a large collection of papers for an exploratory literature...",
        "keywords": [
            "Supply chain management",
            "Latent Dirichlet Allocation",
            "Topic modelling",
            "Automatic literature review"
        ],
        "number_of_pages": "18",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0255-7.pdf",
        "abstract": "Manual exploratory literature reviews should be a thing of the past, as technology and development of machine learning methods have matured. The learning curve for using machine learning methods is rapidly declining, enabling new possibilities for all researchers. A framework is presented on how to use topic modelling on a large collection of papers for an exploratory literature review and how that can be used for a full literature review. The aim of the paper is to enable the use of topic modelling for researchers by presenting a step-by-step framework on a case and sharing a code template. The framework consists of three steps; pre-processing, topic modelling, and post-processing, where the topic model Latent Dirichlet Allocation is used. The frame-work enables huge amounts of papers to be reviewed in a transparent, reliable, faster, and reproducible way."
    },
    {
        "url": "https://paperity.org/p/230087269/rbsep-a-reassignment-and-buffer-based-streaming-edge-partitioning-approach",
        "title": "RBSEP: a reassignment and buffer based streaming edge partitioning approach",
        "authors": [
            "Monireh Taimouri",
            " Hamid Saadatfar"
        ],
        "date_article": "12-2019",
        "short_description": "In recent years, the rapid growth of the Internet has led to creation of massively large graphs. Since databases have become very large nowadays, they cannot be processed by a simple machine at an acceptable time anymore; therefore, traditional graph partitioning methods, which are often based on having a complete image of the entire graph, are not applicable to large datasets...",
        "keywords": [
            "Graph partitioning",
            "Streaming partitioning",
            "Edge partitioning",
            "Big Data Processing"
        ],
        "number_of_pages": "17",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0257-5.pdf",
        "abstract": "In recent years, the rapid growth of the Internet has led to creation of massively large graphs. Since databases have become very large nowadays, they cannot be processed by a simple machine at an acceptable time anymore; therefore, traditional graph partitioning methods, which are often based on having a complete image of the entire graph, are not applicable to large datasets. This challenge has led to the appearance of a new approach called streaming graph partitioning. In streaming graph partition-ing, a stream of input data is received by a partitioner, and partitioner decides which computational machine the data should be transferred to. Often, streaming partitioner does not have any information about the whole graph, and usually distributes the vertices based on some greedy heuristics which may not be optimal for incoming vertices. Hence, partitioner’s decision can be significantly improved if more information about the graph is utilized. In this paper, we present a new vertex-cut streaming graph partitioning approach. The proposed method uses the idea of postponing the decision for some of the edges (by means of an intelligent buffering) and corrects some of the past decisions to improve the quality of the graph partitioning. The proposed approach is evaluated using from real-world graphs. The experimental results show that the per-formance of the proposed method is superior in comparison with the previous HDRF method."
    },
    {
        "url": "https://paperity.org/p/229943504/hard-a-heterogeneity-aware-replica-deletion-for-hdfs",
        "title": "HaRD: a heterogeneity-aware replica deletion for HDFS",
        "authors": [
            "Hilmi Egemen Ciritoglu",
            " John Murphy",
            " Christina Thorp"
        ],
        "date_article": "12-2019",
        "short_description": "The Hadoop distributed file system (HDFS) is responsible for storing very large data-sets reliably on clusters of commodity machines. The HDFS takes advantage of replication to serve data requested by clients with high throughput. Data replication is a trade-off between better data availability and higher disk usage. Recent studies propose different data replication management...",
        "keywords": [
            "Hadoop distributed file system (HDFS)",
            "Replication factor",
            "Replica management framework",
            "Software performance"
        ],
        "number_of_pages": "21",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0256-6.pdf",
        "abstract": "The Hadoop distributed file system (HDFS) is responsible for storing very large data-sets reliably on clusters of commodity machines. The HDFS takes advantage of rep-lication to serve data requested by clients with high throughput. Data replication is a trade-off between better data availability and higher disk usage. Recent studies propose different data replication management frameworks that alter the replication factor of files dynamically in response to the popularity of the data, keeping more rep-licas for in-demand data to enhance the overall performance of the system. When data gets less popular, these schemes reduce the replication factor, which changes the data distribution and leads to unbalanced data distribution. Such an unbalanced data distri-bution causes hot spots, low data locality and excessive network usage in the cluster. In this work, we first confirm that reducing the replication factor causes unbalanced data distribution when using Hadoop’s default replica deletion scheme. Then, we show that even keeping a balanced data distribution using WBRD (data-distribution-aware replica deletion scheme) that we proposed in previous work performs sub-optimally on heterogeneous clusters. In order to overcome this issue, we propose a heterogeneity-aware replica deletion scheme (HaRD). HaRD considers the nodes’ processing capabili-ties when deleting replicas; hence it stores more replicas on the more powerful nodes. We implemented HaRD on top of HDFS and conducted a performance evaluation on a 23-node dedicated heterogeneous cluster. Our results show that HaRD reduced execu-tion time by up to 60%, and 17% when compared to Hadoop and WBRD, respectively."
    },
    {
        "url": "https://paperity.org/p/230094003/an-adaptive-and-real-time-based-architecture-for-financial-data-integration",
        "title": "An adaptive and real-time based architecture for financial data integration",
        "authors": [
            "Noussair Fikri",
            " Mohamed Rida",
            " Noureddine Abghour"
        ],
        "date_article": "12-2019",
        "short_description": "In this paper we are proposing an adaptive and real-time approach to resolve real-time financial data integration latency problems and semantic heterogeneity. Due to constraints that we have faced in some projects that requires real-time massive financial data integration and analysis, we decided to follow a new approach by combining a hybrid financial ontology, resilient...",
        "keywords": [
            "ETL",
            "OLAP",
            "Financial",
            "Ontology",
            "RDD",
            "Real-time",
            "Discretized",
            "Stream",
            "Apache spark",
            "Kafka",
            "Producer",
            "Consumer",
            "Containers"
        ],
        "number_of_pages": "25",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0260-x.pdf",
        "abstract": "In this paper we are proposing an adaptive and real-time approach to resolve real-time financial data integration latency problems and semantic heterogeneity. Due to con-straints that we have faced in some projects that requires real-time massive financial data integration and analysis, we decided to follow a new approach by combining a hybrid financial ontology, resilient distributed datasets and real-time discretized stream. We create a real-time data integration pipeline to avoid all problems of classic Extract-Transform-Load tools, which are data processing latency, functional miscomprehen-sions and metadata heterogeneity. This approach is considered as contribution to enhance reporting quality and availability in short time frames, the reason of the use of Apache Spark. We studied Extract-Transform-Load (ETL) concepts, data warehous-ing fundamentals, big data processing technics and oriented containers clustering architecture, in order to replace the classic data integration and analysis process by our new concept resilient distributed DataStream for online analytical process (RDD4OLAP) cubes which are consumed by using Spark SQL or Spark Core basics."
    },
    {
        "url": "https://paperity.org/p/230068617/social-network-analysis-in-telecom-data",
        "title": "Social network analysis in Telecom data",
        "authors": [
            "Nour Raeef Al-Molhem",
            " Yasser Rahal",
            " Mustapha Dakkak"
        ],
        "date_article": "12-2019",
        "short_description": "Many systems can be represented as networks or graph collections of nodes joined by edges. The social structures in these networks can be investigated using graph theory through a process called social network analysis (SNA). In this paper, networks and SNA concepts were applied using Telecom data such as call detail records (CDRs) and customers data to model our social network...",
        "keywords": [
            "Influencers",
            "Similarity",
            "Telecom social network analysis",
            "Multi-SIM",
            "CDR",
            "Big data"
        ],
        "number_of_pages": "17",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0264-6.pdf",
        "abstract": "Many systems can be represented as networks or graph collections of nodes joined by edges. The social structures in these networks can be investigated using graph theory through a process called social network analysis (SNA). In this paper, networks and SNA concepts were applied using Telecom data such as call detail records (CDRs) and cus‑tomers data to model our social network and to construct a weighed graph in which each relation carries a different weight, representing how close two subscribers are to each other. In addition, SNA is used to explore the Telecom network and calculate the centrality measures, which help determine the node importance in the network. Depending on centrality measures as well as influence capability of node measure, the influencers in network were detected and targeted by marketing campaigns resulting in 30% raise in growth rate of mobile traffic compared with traditional ways. Finding Multi‑SIM subscribers within the same operator or across different operators presents another important concern to Telecom companies because it allows to improve campaigns and churn prediction models. Social network similarity measures and social behavioral measures between nodes were calculated in the Telecom network to detect these Multi‑SIM subscribes and 85% accuracy result was achieved for subscribes from different operators and 92% for subscribes from the same operator. The paper is based on a real dataset of 3 months CDRs and customer data provided by a local Telecom operator. This dataset is used to build a network with more than 16 million nodes and more than 300 million edges on a big data platform."
    },
    {
        "url": "https://paperity.org/p/229924852/computational-storage-an-efficient-and-scalable-platform-for-big-data-and-hpc",
        "title": "Computational storage: an efficient and scalable platform for big data and HPC applications",
        "authors": [
            "Mahdi Torabzadehkashi",
            " Siavash Rezaei",
            " Ali HeydariGorji"
        ],
        "date_article": "12-2019",
        "short_description": "In the era of big data applications, the demand for more sophisticated data centers and high-performance data processing mechanisms is increasing drastically. Data are originally stored in storage systems. To process data, application servers need to fetch them from storage devices, which imposes the cost of moving data to the system. This cost has a direct relation with the...",
        "keywords": [
            "Computational storage",
            "In-storage processing",
            "Near-data processing",
            "SSD",
            "Big data",
            "Hadoop",
            "HPC",
            "DFT",
            "System-on-chip"
        ],
        "number_of_pages": "29",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0265-5.pdf",
        "abstract": "In the era of big data applications, the demand for more sophisticated data centers and high-performance data processing mechanisms is increasing drastically. Data are origi-nally stored in storage systems. To process data, application servers need to fetch them from storage devices, which imposes the cost of moving data to the system. This cost has a direct relation with the distance of processing engines from the data. This is the key motivation for the emergence of distributed processing platforms such as Hadoop, which move process closer to data. Computational storage devices (CSDs) push the “move process to data” paradigm to its ultimate boundaries by deploying embedded processing engines inside storage devices to process data. In this paper, we intro-duce Catalina, an efficient and flexible computational storage platform, that provides a seamless environment to process data in-place. Catalina is the first CSD equipped with a dedicated application processor running a full-fledged operating system that provides filesystem-level data access for the applications. Thus, a vast spectrum of applications can be ported for running on Catalina CSDs. Due to these unique features, to the best of our knowledge, Catalina CSD is the only in-storage processing platform that can be seamlessly deployed in clusters to run distributed applications such as Hadoop MapReduce and HPC applications in-place without any modifications on the underlying distributed processing framework. For the proof of concept, we build a fully functional Catalina prototype and a CSD-equipped platform using 16 Catalina CSDs to run Intel HiBench Hadoop and HPC benchmarks to investigate the benefits of deploying Catalina CSDs in the distributed processing environments. The experimental results show up to 2.2× improvement in performance and 4.3× reduction in energy consumption, respectively, for running Hadoop MapReduce benchmarks. Addition-ally, thanks to the Neon SIMD engines, the performance and energy efficiency of DFT algorithms are improved up to 5.4× and 8.9×, respectively."
    },
    {
        "url": "https://paperity.org/p/230071984/characterizing-popularity-dynamics-of-hot-topics-using-micro-blogs-spatio-temporal-data",
        "title": "Characterizing popularity dynamics of hot topics using micro-blogs spatio-temporal data",
        "authors": [
            "Lianren Wu",
            " Jinjie Li",
            " Jiayin Qi"
        ],
        "date_article": "12-2019",
        "short_description": "In this paper, a quantitative temporal and spatial analysis of the dynamics of hot topics popularity in Micro-blogging system was provided. Firstly, the popularity time series of 1167 hot topics were counted and calculated by Excel. Secondly, based on MATLAB software,the popularity time series were clustered into six clusters by K-spectral centroid (K-SC) clustering algorithm...",
        "keywords": [
            "Hot topics",
            "Micro-blogs",
            "Popularity dynamics",
            "Power-law distribution",
            "Statistical mechanics",
            "Spatio-temporal analysis"
        ],
        "number_of_pages": "16",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0266-4.pdf",
        "abstract": "In this paper, a quantitative temporal and spatial analysis of the dynamics of hot topics popularity in Micro-blogging system was provided. Firstly, the popularity time series of 1167 hot topics were counted and calculated by Excel. Secondly, based on MAT-LAB software,the popularity time series were clustered into six clusters by K-spectral centroid (K-SC) clustering algorithm. Thirdly, we analyzed temporal patterns and spatial patterns of popularity dynamics of topics by statistical methods. The results show that temporal popularity of micro-blogging topics is rapidly dying, and the distribution of popularity is subject to the power law form. In addition, most of the Micro-blogging topics are global topic. Our results can provide a literature reference for studying the influence of online hot topics and the evolution of public opinion."
    },
    {
        "url": "https://paperity.org/p/229928219/efficient-storage-of-heterogeneous-geospatial-data-in-spatial-databases",
        "title": "Efficient storage of heterogeneous geospatial data in spatial databases",
        "authors": [
            "Atle Frenvik Sveen"
        ],
        "date_article": "12-2019",
        "short_description": "The no-schema approach of NoSQL document stores is a tempting solution for importing heterogenous geospatial data to a spatial database. However, this approach means sacrificing the benefits of RDBMSes, such as existing integrations and the ACID principle. Previous comparisons of the document-store and table-based layout for storing geospatial data favours the document-store...",
        "keywords": [
            "NoSQL",
            "Document-store",
            "Geospatial data",
            "Spatial database",
            "Relational database",
            "Database benchmark"
        ],
        "number_of_pages": "14",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0262-8.pdf",
        "abstract": "The no-schema approach of NoSQL document stores is a tempting solution for import-ing heterogenous geospatial data to a spatial database. However, this approach means sacrificing the benefits of RDBMSes, such as existing integrations and the ACID princi-ple. Previous comparisons of the document-store and table-based layout for storing geospatial data favours the document-store approach but does not consider importing data that can be segmented into homogenous datasets. In this paper we propose “The Heterogeneous Open Geodata Storage (HOGS)” system. HOGS is a command line util-ity that automates the process of importing geospatial data to a PostgreSQL/PostGIS database. It is developed in order to compare the performance of a traditional storage layout adhering to the ACID principle, and a NoSQL-inspired document store. A col-lection of eight open geospatial datasets comprising 15 million features was imported and queried in order to compare the differences between the two storage layouts. The results from a quantitative experiment are presented and shows that large amounts of open geospatial data can be stored using traditional RDBMSes using a table-based layout without any performance penalties."
    },
    {
        "url": "https://paperity.org/p/229902833/a-new-internet-of-things-architecture-for-real-time-prediction-of-various-diseases-using",
        "title": "A new Internet of Things architecture for real-time prediction of various diseases using machine learning on big data environment",
        "authors": [
            "Abderrahmane Ed-daoudy",
            " Khalil Maalmi"
        ],
        "date_article": "12-2019",
        "short_description": "A number of technologies enabled by Internet of Thing (IoT) have been used for the prevention of various chronic diseases, continuous and real-time tracking system is a particularly important one. Wearable medical devices with sensor, health cloud and mobile applications have continuously generating a huge amount of data which is often called as streaming big data. Due to the...",
        "keywords": [
            "Healthcare",
            "Stream processing",
            "Big data",
            "Apache Spark",
            "Distributed machine learning",
            "Internet of Things"
        ],
        "number_of_pages": "25",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0271-7.pdf",
        "abstract": "A number of technologies enabled by Internet of Thing (IoT ) have been used for the prevention of various chronic diseases, continuous and real‑time tracking system is a particularly important one. Wearable medical devices with sensor, health cloud and mobile applications have continuously generating a huge amount of data which is often called as streaming big data. Due to the higher speed of the data generation, it is difficult to collect, process and analyze such massive data in real‑time in order to perform real‑time actions in case of emergencies and extracting hidden value. using traditional methods which are limited and time‑consuming. Therefore, there is a signifi‑cant need to real‑time big data stream processing to ensure an effective and scalable solution. In order to overcome this issue, this work proposes a new architecture for real‑time health status prediction and analytics system using big data technologies. The system focus on applying distributed machine learning model on streaming health data events ingested to Spark streaming through Kafka topics. Firstly, we transform the standard decision tree (DT ) (C4.5) algorithm into a parallel, distributed, scalable and fast DT using Spark instead of Hadoop MapReduce which becomes limited for real‑time computing. Secondly, this model is applied to streaming data coming from distributed sources of various diseases to predict health status. Based on several input attributes, the system predicts health status, send an alert message to care provid‑ers and store the details in a distributed database to perform health data analytics and stream reporting. We measure the performance of Spark DT against traditional machine learning tools including Weka. Finally, performance evaluation parameters such as throughput and execution time are calculated to show the effectiveness of the proposed architecture. The experimental results show that the proposed system is able to effectively process and predict real‑time and massive amount of medical data enabled by IoT from distributed and various diseases."
    },
    {
        "url": "https://paperity.org/p/230107471/severely-imbalanced-big-data-challenges-investigating-data-sampling-approaches",
        "title": "Severely imbalanced Big Data challenges: investigating data sampling approaches",
        "authors": [
            "Tawfiq Hasanin",
            " Taghi M. Khoshgoftaar",
            " Joffrey L. Leevy"
        ],
        "date_article": "12-2019",
        "short_description": "Severe class imbalance between majority and minority classes in Big Data can bias the predictive performance of Machine Learning algorithms toward the majority (negative) class. Where the minority (positive) class holds greater value than the majority (negative) class and the occurrence of false negatives incurs a greater penalty than false positives, the bias may lead to adverse...",
        "keywords": [
            "Big Data",
            "Class imbalance",
            "Machine Learning",
            "Medicare fraud",
            "Oversampling",
            "SlowlorisBig",
            "Undersampling"
        ],
        "number_of_pages": "25",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0274-4.pdf",
        "abstract": "Severe class imbalance between majority and minority classes in Big Data can bias the predictive performance of Machine Learning algorithms toward the majority (negative) class. Where the minority (positive) class holds greater value than the majority (nega-tive) class and the occurrence of false negatives incurs a greater penalty than false positives, the bias may lead to adverse consequences. Our paper incorporates two case studies, each utilizing three learners, six sampling approaches, two performance metrics, and five sampled distribution ratios, to uniquely investigate the effect of severe class imbalance on Big Data analytics. The learners (Gradient-Boosted Trees, Logistic Regression, Random Forest) were implemented within the Apache Spark framework. The first case study is based on a Medicare fraud detection dataset. The second case study, unlike the first, includes training data from one source (SlowlorisBig Dataset) and test data from a separate source (POST dataset). Results from the Medicare case study are not conclusive regarding the best sampling approach using Area Under the Receiver Operating Characteristic Curve and Geometric Mean performance metrics. However, it should be noted that the Random Undersampling approach performs adequately in the first case study. For the SlowlorisBig case study, Random Undersampling convincingly outperforms the other five sampling approaches (Random Oversampling, Synthetic Minority Over-sampling TEchnique, SMOTE-borderline1 , SMOTE-borderline2 , ADAptive SYNthetic) when measuring performance with Area Under the Receiver Operating Char-acteristic Curve and Geometric Mean metrics. Based on its classification performance in both case studies, Random Undersampling is the best choice as it results in models with a significantly smaller number of samples, thus reducing computational burden and training time."
    },
    {
        "url": "https://paperity.org/p/229963706/on-using-mapreduce-to-scale-algorithms-for-big-data-analytics-a-case-study",
        "title": "On using MapReduce to scale algorithms for Big Data analytics: a case study",
        "authors": [
            "Phongphun Kijsanayothin",
            " Gantaphon Chalumporn",
            " Rattikorn Hew"
        ],
        "date_article": "12-2019",
        "short_description": "Many data analytics algorithms are originally designed for in-memory data. Parallel and distributed computing is a natural first remedy to scale these algorithms to “Big algorithms” for large-scale data. Advances in many Big Data analytics algorithms are contributed by MapReduce, a programming paradigm that enables parallel and distributed execution of massive data processing on...",
        "keywords": [
            "Big Data analytics algorithms",
            "Association rules mining",
            "MapReduce",
            "Parallel computing"
        ],
        "number_of_pages": "20",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0269-1.pdf",
        "abstract": "Introduction:Many data analytics algorithms are originally designed for in-memory data. Parallel and distributed computing is a natural first remedy to scale these algo-rithms to “Big algorithms” for large-scale data. Advances in many Big Data analytics algorithms are contributed by MapReduce, a programming paradigm that enables parallel and distributed execution of massive data processing on large clusters of machines. Much research has focused on building efficient naive MapReduce-based algorithms or extending MapReduce mechanisms to enhance performance. How-ever, we argue that these should not be the only research directions to pursue. We conjecture that when naive MapReduce-based solutions do not perform well, it could be because certain classes of algorithms are not amendable to MapReduce model and one should find a fundamentally different approach to a new MapReduce-based solution.Case description:This paper investigates a case study of a scaling problem of “Big algorithms” for a popular association rule-mining algorithm, particularly the develop-ment of Apriori algorithm in MapReduce model.Discussion and evaluation:Formal and empirical illustrations are explored to compare our proposed MapReduce-based Apriori algorithm with previous solutions. The findings support our conjecture and our study shows promising results compared to the state-of-the-art performer with 7% increase in performance on the average of transactions ranging from 10,000 to 120,000.Conclusions:The results confirm that effective MapReduce implementation should avoid dependent iterations, such as that of the original sequential Apriori algorithm. These findings could lead to many more alternative non-naive MapReduce-based “Big algorithms”."
    },
    {
        "url": "https://paperity.org/p/229909567/evaluation-of-big-data-frameworks-for-analysis-of-smart-grids",
        "title": "Evaluation of big data frameworks for analysis of smart grids",
        "authors": [
            "Mohammad Hasan Ansari",
            " Vahid Tabatab Vakili",
            " Behnam Bahrak"
        ],
        "date_article": "12-2019",
        "short_description": "With the rapid development of smart grids and increasing data collected in these networks, analyzing this massive data for applications such as marketing, cyber-security, and performance analysis, has gained popularity. This paper focuses on analysis and performance evaluation of big data frameworks that are proposed for handling smart grid data. Since obtaining large amounts of...",
        "keywords": [
            "Smart grid",
            "Big data",
            "Data generator",
            "Performance"
        ],
        "number_of_pages": "14",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0270-8.pdf",
        "abstract": "With the rapid development of smart grids and increasing data collected in these networks, analyzing this massive data for applications such as marketing, cyber-secu-rity, and performance analysis, has gained popularity. This paper focuses on analysis and performance evaluation of big data frameworks that are proposed for handling smart grid data. Since obtaining large amounts of smart grid data is difficult due to privacy concerns, we propose and implement a large scale smart grid data generator to produce massive data under conditions similar to those in real smart grids. We use four open source big data frameworks namely Hadoop-Hbase, Cassandra, Elasticsearch, and MongoDB, in our implementation. Finally, we evaluate the performance of differ-ent frameworks on smart grid big data and present a performance benchmark that includes common data analysis techniques on smart grid data."
    },
    {
        "url": "https://paperity.org/p/229912934/internet-of-things-is-a-revolutionary-approach-for-future-technology-enhancement-a-review",
        "title": "Internet of Things is a revolutionary approach for future technology enhancement: a review",
        "authors": [
            "Sachin Kumar",
            " Prayag Tiwari",
            " Mikhail Zymbler"
        ],
        "date_article": "12-2019",
        "short_description": "Internet of Things (IoT) is a new paradigm that has changed the traditional way of living into a high tech life style. Smart city, smart homes, pollution control, energy saving, smart transportation, smart industries are such transformations due to IoT. A lot of crucial research studies and investigations have been done in order to enhance the technology through IoT. However...",
        "keywords": [
            "Internet of Things (IoT)",
            "IoT architecture",
            "IoT challenges",
            "IoT applications"
        ],
        "number_of_pages": "21",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0268-2.pdf",
        "abstract": "Internet of Things (IoT ) is a new paradigm that has changed the traditional way of living into a high tech life style. Smart city, smart homes, pollution control, energy saving, smart transportation, smart industries are such transformations due to IoT. A lot of crucial research studies and investigations have been done in order to enhance the technology through IoT. However, there are still a lot of challenges and issues that need to be addressed to achieve the full potential of IoT. These challenges and issues must be considered from various aspects of IoT such as applications, challenges, enabling technologies, social and environmental impacts etc. The main goal of this review article is to provide a detailed discussion from both technological and social perspective. The article discusses different challenges and key issues of IoT, architecture and impor-tant application domains. Also, the article bring into light the existing literature and illustrated their contribution in different aspects of IoT. Moreover, the importance of big data and its analysis with respect to IoT has been discussed. This article would help the readers and researcher to understand the IoT and its applicability to the real world."
    },
    {
        "url": "https://paperity.org/p/230120939/deep-convolutional-neural-network-based-medical-image-classification-for-disease",
        "title": "Deep convolutional neural network based medical image classification for disease diagnosis",
        "authors": [
            "Samir S. Yadav",
            " Shivajirao M. Jadhav"
        ],
        "date_article": "12-2019",
        "short_description": "Medical image classification plays an essential role in clinical treatment and teaching tasks. However, the traditional method has reached its ceiling on performance. Moreover, by using them, much time and effort need to be spent on extracting and selecting classification features. The deep neural network is an emerging machine learning method that has proven its potential for...",
        "keywords": [
            "CNN",
            "Transfer learning",
            "Capsule network",
            "ORB",
            "SVM",
            "Image classification"
        ],
        "number_of_pages": "18",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0276-2.pdf",
        "abstract": "Medical image classification plays an essential role in clinical treatment and teach-ing tasks. However, the traditional method has reached its ceiling on performance. Moreover, by using them, much time and effort need to be spent on extracting and selecting classification features. The deep neural network is an emerging machine learning method that has proven its potential for different classification tasks. Notably, the convolutional neural network dominates with the best results on varying image classification tasks. However, medical image datasets are hard to collect because it needs a lot of professional expertise to label them. Therefore, this paper researches how to apply the convolutional neural network (CNN) based algorithm on a chest X-ray dataset to classify pneumonia. Three techniques are evaluated through experiments. These are linear support vector machine classifier with local rotation and orienta-tion free features, transfer learning on two convolutional neural network models: Visual Geometry Group i.e., VGG16 and InceptionV3, and a capsule network training from scratch. Data augmentation is a data preprocessing method applied to all three methods. The results of the experiments show that data augmentation generally is an effective way for all three algorithms to improve performance. Also, Transfer learning is a more useful classification method on a small dataset compared to a support vector machine with oriented fast and rotated binary (ORB) robust independent elementary features and capsule network. In transfer learning, retraining specific features on a new target dataset is essential to improve performance. And, the second important factor is a proper network complexity that matches the scale of the dataset."
    },
    {
        "url": "https://paperity.org/p/230002560/a-novel-approach-for-big-data-processing-using-message-passing-interface-based-on-memory",
        "title": "A novel approach for big data processing using message passing interface based on memory mapping",
        "authors": [
            "Saad Ahmed Dheyab",
            " Mohammed Najm Abdullah",
            " Buthainah Fahran Abed"
        ],
        "date_article": "12-2019",
        "short_description": "The analysis and processing of big data are one of the most important challenges that researchers are working on to find the best approaches to handle it with high performance, low cost and high accuracy. In this paper, a novel approach for big data processing and management was proposed that differed from the existing ones; the proposed method employs not only the memory space...",
        "keywords": [
            "Big data",
            "Message passing",
            "Memory mapping",
            "High-performance computing",
            "Parallel AES"
        ],
        "number_of_pages": "17",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0275-3.pdf",
        "abstract": "The analysis and processing of big data are one of the most important challenges that researchers are working on to find the best approaches to handle it with high performance, low cost and high accuracy. In this paper, a novel approach for big data processing and management was proposed that differed from the existing ones; the proposed method employs not only the memory space to reads and handle big data, it also uses space of memory-mapped extended from memory storage. From a meth-odological viewpoint, the novelty of this paper is the segmentation stage of big data using memory mapping and broadcasting all segments to a number of processors using a parallel message passing interface. From an application viewpoint, the paper presents a high-performance approach based on a homogenous network which works parallelly to encrypt-decrypt big data using AES algorithm. This approach can be done on Windows Operating System using .NET libraries."
    },
    {
        "url": "https://paperity.org/p/230027946/using-transfer-learning-for-smart-building-management-system",
        "title": "Using transfer learning for smart building management system",
        "authors": [
            "Bens Pardamean",
            " Hery Harjono Muljo",
            " Tjeng Wawan Cenggoro"
        ],
        "date_article": "12-2019",
        "short_description": "In building management, energy optimization is one of the main concern that needs to be automated. For automation, an intelligent system needs to be developed. However, an intelligent system needs to be trained in a large dataset before it can be used reliably. In this paper, we present a transfer learning scheme to develop an intelligent system for smart building management...",
        "keywords": [
            "Transfer learning",
            "Deep learning",
            "Human counting",
            "Smart building",
            "Building management system"
        ],
        "number_of_pages": "12",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0272-6.pdf",
        "abstract": "In building management, energy optimization is one of the main concern that needs to be automated. For automation, an intelligent system needs to be developed. How-ever, an intelligent system needs to be trained in a large dataset before it can be used reliably. In this paper, we present a transfer learning scheme to develop an intelligent system for smart building management system. Specifically, the intelligent system is able to count human inside a room, which can be utilized to adaptively adjust energy usage in a room. The transfer learning scheme employs a deep learning model that is pretrained on ImageNet dataset. To enable the human counting capability, the model is trained on a dataset specifically collected for human counting case."
    },
    {
        "url": "https://paperity.org/p/229995826/lifelong-machine-learning-and-root-cause-analysis-for-large-scale-cancer-patient-data",
        "title": "Lifelong Machine Learning and root cause analysis for large-scale cancer patient data",
        "authors": [
            "Gautam Pal",
            " Xianbin Hong",
            " Zhuo Wang"
        ],
        "date_article": "12-2019",
        "short_description": "IntroductionThis paper presents a lifelong learning framework which constantly adapts with changing data patterns over time through incremental learning approach. In many big data systems, iterative re-training high dimensional data from scratch is computationally infeasible since constant data stream ingestion on top of a historical data pool increases the training time...",
        "keywords": [
            "Lifelong learning",
            "Real-time data processing",
            "Lambda Architecture",
            "Streaming k-means",
            "Random Decision Forest",
            "Dimension reduction"
        ],
        "number_of_pages": "29",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0261-9.pdf",
        "abstract": "Introduction:This paper presents a lifelong learning framework which constantly adapts with changing data patterns over time through incremental learning approach. In many big data systems, iterative re-training high dimensional data from scratch is computationally infeasible since constant data stream ingestion on top of a historical data pool increases the training time exponentially. Therefore, the need arises on how to retain past learning and fast update the model incrementally based on the new data. Also, the current machine learning approaches do the model prediction without providing a comprehensive root cause analysis. To resolve these limitations, our frame-work lays foundations on an ensemble process between stream data with historical batch data for an incremental lifelong learning (LML) model.Case description:A cancer patient’s pathological tests like blood, DNA, urine or tis-sue analysis provide a unique signature based on the DNA combinations. Our analysis allows personalized and targeted medications and achieves a therapeutic response. Model is evaluated through data from The National Cancer Institute’s Genomic Data Commons unified data repository. The aim is to prescribe personalized medicine based on the thousands of genotype and phenotype parameters for each patient.Discussion and evaluation:The model uses a dimension reduction method to reduce training time at an online sliding window setting. We identify the Gleason score as a determining factor for cancer possibility and substantiate our claim through Lilliefors and Kolmogorov–Smirnov test. We present clustering and Random Decision Forest results. The model’s prediction accuracy is compared with standard machine learning algorithms for numeric and categorical fields.Conclusion:We propose an ensemble framework of stream and batch data for incre-mental lifelong learning. The framework successively applies first streaming clustering technique and then Random Decision Forest Regressor/Classifier to isolate anomalous patient data and provides reasoning through root cause analysis by feature correlations with an aim to improve the overall survival rate. While the stream clustering technique creates groups of patient profiles, RDF further drills down into each group for com-parison and reasoning for useful actionable insights. The proposed MALA architecture retains the past learned knowledge and transfer to future learning and iteratively becomes more knowledgeable over time."
    },
    {
        "url": "https://paperity.org/p/230139591/hybsmrp-a-hybrid-scheduling-algorithm-in-hadoop-mapreduce-framework",
        "title": "HybSMRP: a hybrid scheduling algorithm in Hadoop MapReduce framework",
        "authors": [
            "Abolfazl Gandomi",
            " Midia Reshadi",
            " Ali Movaghar"
        ],
        "date_article": "11-2019",
        "short_description": "Due to the advent of new technologies, devices, and communication tools such as social networking sites, the amount of data produced by mankind is growing rapidly every year. Big data is a collection of large datasets that cannot be processed using traditional computing techniques. MapReduce has been introduced to solve large-data computational problems. It is specifically...",
        "keywords": [
            "MapReduce",
            "Scheduling",
            "Hybrid algorithm",
            "Data Locality",
            "Dynamic priority"
        ],
        "number_of_pages": "16",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0253-9.pdf",
        "abstract": "Due to the advent of new technologies, devices, and communication tools such as social networking sites, the amount of data produced by mankind is growing rapidly every year. Big data is a collection of large datasets that cannot be processed using traditional computing techniques. MapReduce has been introduced to solve large-data computational problems. It is specifically designed to run on commodity hard-ware, and it depends on dividing and conquering principles. Nowadays, the focus of researchers has shifted towards Hadoop MapReduce. One of the most outstanding characteristics of MapReduce is data locality-aware scheduling. Data locality-aware scheduler is a further efficient solution to optimize one or a set of performance metrics such as data locality, energy consumption and job completion time. Similar to all situa-tions, time and scheduling are the most important aspects of the MapReduce frame-work. Therefore, many scheduling algorithms have been proposed in the past decades. The main ideas of these algorithms are increasing data locality rate and decreasing the response and completion time. In this paper, a new hybrid scheduling algorithm has been proposed, which uses dynamic priority and localization ID techniques and focuses on increasing data locality rate and decreasing completion time. The proposed algorithm was evaluated and compared with Hadoop default schedulers (FIFO, Fair), by running concurrent workloads consisting of Wordcount and Terasort benchmarks. The experimental results show that the proposed algorithm is faster than FIFO and Fair scheduling, achieves higher data locality rate and avoids wasting resources."
    },
    {
        "url": "https://paperity.org/p/230046598/online-feature-selection-ofs-with-accelerated-bat-algorithm-aba-and-ensemble-incremental",
        "title": "Online Feature Selection (OFS) with Accelerated Bat Algorithm (ABA) and Ensemble Incremental Deep Multiple Layer Perceptron (EIDMLP) for big data streams",
        "authors": [
            "D. Renuka Devi",
            " S. Sasik"
        ],
        "date_article": "11-2019",
        "short_description": "Feature selection is mainly used to lessen the dispensation load of data mining models. To condense the time for processing voluminous data, parallel processing is carried out with MapReduce (MR) technique. However with the existing algorithms, the performance of the classifiers needs substantial improvement. MR method, which is recommended in this research work, will perform...",
        "keywords": [
            "Feature selection",
            "Online Feature Selection (OFS)",
            "Big data",
            "Ensemble Incremental Deep Multiple Layer Perceptron (EIDMLP)",
            "Accelerated Bat Algorithm (ABA)",
            "MapReduce (MR)"
        ],
        "number_of_pages": "20",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0267-3.pdf",
        "abstract": "Feature selection is mainly used to lessen the dispensation load of data mining models. To condense the time for processing voluminous data, parallel processing is car-ried out with MapReduce (MR) technique. However with the existing algorithms, the performance of the classifiers needs substantial improvement. MR method, which is recommended in this research work, will perform feature selection in parallel which progresses the performance. To enhance the efficacy of the classifier, this research work proposes an innovative Online Feature Selection (OFS)–Accelerated Bat Algo-rithm (ABA) and a framework for applications that streams the features in advance with indefinite knowledge of the feature space. The concrete OFS-ABA method is suggested to select significant and non-superfluous feature with MapReduce (MR) framework. Finally, Ensemble Incremental Deep Multiple Layer Perceptron (EIDMLP) classifier is applied to classify the dataset samples. The outputs of homogeneous IDMLP classifiers were combined using the EIDMPL classifier. The projected feature selection method along with the classifier is evaluated expansively on three datasets of high dimension-ality. In this research work, MR-OFS-ABA method has shown enhanced performance than the existing feature selection methods namely PSO, APSO and ASAMO (Acceler-ated Simulated Annealing and Mutation Operator). The result of the EIDMLP classifier is compared with other existing classifiers such as Naïve Bayes (NB), Hoeffding tree (HT ), and Fuzzy Minimal Consistent Class Subset Coverage (FMCCSC)-KNN (K Nearest Neigh-bour). The methodology is applied to three datasets and results were compared with four classifiers and three state-of-the-art feature selection algorithms. The outcome of this research work has shown enhanced performance in accuracy and less processing time."
    },
    {
        "url": "https://paperity.org/p/229950238/enlarging-smaller-images-before-inputting-into-convolutional-neural-network-zero-padding",
        "title": "Enlarging smaller images before inputting into convolutional neural network: zero-padding vs. interpolation",
        "authors": [
            "Mahdi Hashemi"
        ],
        "date_article": "11-2019",
        "short_description": "The input to a machine learning model is a one-dimensional feature vector. However, in recent learning models, such as convolutional and recurrent neural networks, two- and three-dimensional feature tensors can also be inputted to the model. During training, the machine adjusts its internal parameters to project each feature tensor close to its target. After training, the machine...",
        "keywords": [
            "Image classification",
            "Pattern recognition",
            "Artificial neural networks",
            "Computer vision",
            "Machine learning"
        ],
        "number_of_pages": "13",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0263-7.pdf",
        "abstract": "The input to a machine learning model is a one-dimensional feature vector. However, in recent learning models, such as convolutional and recurrent neural networks, two- and three-dimensional feature tensors can also be inputted to the model. During training, the machine adjusts its internal parameters to project each feature tensor close to its target. After training, the machine can be used to predict the target for previously unseen feature tensors. What this study focuses on is the requirement that feature tensors must be of the same size. In other words, the same number of features must be present for each sample. This creates a barrier in processing images and texts, as they usually have different sizes, and thus different numbers of features. In classifying an image using a convolutional neural network (CNN), the input is a three-dimensional tensor, where the value of each pixel in each channel is one feature. The three-dimen-sional feature tensor must be the same size for all images. However, images are not usually of the same size and so are not their corresponding feature tensors. Resizing images to the same size without deforming patterns contained therein is a major challenge. This study proposes zero-padding for resizing images to the same size and compares it with the conventional approach of scaling images up (zooming in) using interpolation. Our study showed that zero-padding had no effect on the classification accuracy but considerably reduced the training time. The reason is that neighboring zero input units (pixels) will not activate their corresponding convolutional unit in the next layer. Therefore, the synaptic weights on outgoing links from input units do not need to be updated if they contain a zero value. Theoretical justification along with experimental endorsements are provided in this paper."
    },
    {
        "url": "https://paperity.org/p/230061883/a-graphical-heuristic-for-reduction-and-partitioning-of-large-datasets-for-scalable",
        "title": "A graphical heuristic for reduction and partitioning of large datasets for scalable supervised training",
        "authors": [
            "Sumedh Yadav",
            " Mathis Bod"
        ],
        "date_article": "10-2019",
        "short_description": "A scalable graphical method is presented for selecting and partitioning datasets for the training phase of a classification task. For the heuristic, a clustering algorithm is required to get its computation cost in a reasonable proportion to the task itself. This step is succeeded by construction of an information graph of the underlying classification patterns using approximate...",
        "keywords": [
            "Training set selection/reduction",
            "Distributed machine learning",
            "Classification",
            "Training set partition",
            "Network architecture design",
            "Data variety",
            "Class imbalance"
        ],
        "number_of_pages": "35",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0259-3.pdf",
        "abstract": "A scalable graphical method is presented for selecting and partitioning datasets for the training phase of a classification task. For the heuristic, a clustering algorithm is required to get its computation cost in a reasonable proportion to the task itself. This step is succeeded by construction of an information graph of the underlying classifica-tion patterns using approximate nearest neighbor methods. The presented method consists of two approaches, one for reducing a given training set, and another for parti-tioning the selected/reduced set. The heuristic targets large datasets, since the primary goal is a significant reduction in training computation run-time without compromising prediction accuracy. Test results show that both approaches significantly speed-up the training task when compared against that of state-of-the-art shrinking heuristics available in LIBSVM. Furthermore, the approaches closely follow or even outperform in prediction accuracy. A network design is also presented for a partitioning based dis-tributed training formulation. Added speed-up in training run-time is observed when compared to that of serial implementation of the approaches."
    },
    {
        "url": "https://paperity.org/p/229946871/context-aware-rule-learning-from-smartphone-data-survey-challenges-and-future-directions",
        "title": "Context-aware rule learning from smartphone data: survey, challenges and future directions",
        "authors": [
            "Iqbal H. Sarker"
        ],
        "date_article": "10-2019",
        "short_description": "Smartphones are considered as one of the most essential and highly personal devices of individuals in our current world. Due to the popularity of context-aware technology and recent developments in smartphones, these devices can collect and process raw contextual data about users’ surrounding environment and their corresponding behavioral activities with their phones. Thus...",
        "keywords": [
            "Smartphone data",
            "Machine learning",
            "Data science",
            "Clustering",
            "Classification",
            "Association",
            "Rule learning",
            "Personalization",
            "Time-series",
            "User behavior modeling",
            "Predictive analytics",
            "Context-aware computing",
            "Mobile and IoT services",
            "Intelligent systems"
        ],
        "number_of_pages": "25",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0258-4.pdf",
        "abstract": "Smartphones are considered as one of the most essential and highly personal devices of individuals in our current world. Due to the popularity of context-aware technol-ogy and recent developments in smartphones, these devices can collect and process raw contextual data about users’ surrounding environment and their corresponding behavioral activities with their phones. Thus, smartphone data analytics and building data-driven context-aware systems have gained wide attention from both academia and industry in recent days. In order to build intelligent context-aware applications on smartphones, effectively learning a set of context-aware rules from smartphone data is the key. This requires advanced data analytical techniques with high precision and intelligent decision making strategies based on contexts. In comparison to traditional approaches, machine learning based techniques provide more effective and efficient results for smartphone data analytics and corresponding context-aware rule learning. Thus, this article first makes a survey on previous work in the area of contextual smart-phone data analytics and then presents a discussion of challenges and future directionsfor effectively learning context-aware rules from smartphone data, in order to build rule-based automated and intelligent systems."
    },
    {
        "url": "https://paperity.org/p/230026396/an-analytical-study-of-information-extraction-from-unstructured-and-multidimensional-big",
        "title": "An analytical study of information extraction from unstructured and multidimensional big data",
        "authors": [
            "Kiran Adnan",
            " Rehan Akbar"
        ],
        "date_article": "10-2019",
        "short_description": "Process of information extraction (IE) is used to extract useful information from unstructured or semi-structured data. Big data arise new challenges for IE techniques with the rapid growth of multifaceted also called as multidimensional unstructured data. Traditional IE systems are inefficient to deal with this huge deluge of unstructured big data. The volume and variety of big...",
        "keywords": [
            "Big data",
            "Information extraction (IE)",
            "Literature review",
            "Learning-based techniques",
            "Multimedia data",
            "Unstructured data"
        ],
        "number_of_pages": "38",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0254-8.pdf",
        "abstract": "Process of information extraction (IE) is used to extract useful information from unstruc-tured or semi-structured data. Big data arise new challenges for IE techniques with the rapid growth of multifaceted also called as multidimensional unstructured data. Tradi-tional IE systems are inefficient to deal with this huge deluge of unstructured big data. The volume and variety of big data demand to improve the computational capabilities of these IE systems. It is necessary to understand the competency and limitations of the existing IE techniques related to data pre-processing, data extraction and transfor-mation, and representations for huge volumes of multidimensional unstructured data. Numerous studies have been conducted on IE, addressing the challenges and issues for different data types such as text, image, audio and video. Very limited consolidated research work have been conducted to investigate the task-dependent and task-inde-pendent limitations of IE covering all data types in a single study. This research work address this limitation and present a systematic literature review of state-of-the-art techniques for a variety of big data, consolidating all data types. Recent challenges of IE are also identified and summarized. Potential solutions are proposed giving future research directions in big data IE. The research is significant in terms of recent trends and challenges related to big data analytics. The outcome of the research and recom-mendations will help to improve the big data analytics by making it more productive."
    },
    {
        "url": "https://paperity.org/p/230109288/improving-the-performance-of-support-vector-machine-by-selecting-the-best-features-by",
        "title": "Improving the performance of support-vector machine by selecting the best features by Gray Wolf algorithm to increase the accuracy of diagnosis of breast cancer",
        "authors": [
            "Seyed Reza Kamel",
            " Reyhaneh YaghoubZadeh",
            " Maryam Kheirabadi"
        ],
        "date_article": "10-2019",
        "short_description": "One of the most common diseases among women is breast cancer, the early diagnosis of which is of paramount importance. Given the time-consuming nature of the diagnosis process of the disease, using new methods such as computer science is extremely important for early detection of the condition. Today, the main emphasis is on the science of data mining as one of the computer...",
        "keywords": [
            "Breast cancer",
            "Feature selection",
            "Support vector machine",
            "Gray Wolf Optimizer"
        ],
        "number_of_pages": "15",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0247-7.pdf",
        "abstract": "One of the most common diseases among women is breast cancer, the early diagnosis of which is of paramount importance. Given the time-consuming nature of the diagno-sis process of the disease, using new methods such as computer science is extremely important for early detection of the condition. Today, the main emphasis is on the science of data mining as one of the computer methods in the field of diagnosis. In the present study, we used data mining as a combination of feature selection method by Gray Wolf Optimization (GWO) and support vector machine (SVM), which is a new tech-nique with high accuracy compared to other methods in this classification, to increase the accuracy of breast cancer diagnosis. The UCI dataset and functional parameters and various statistical criteria were applied to evaluate the proposed method and assess the validity of the results in MATLAB, respectively. Application of the proposed method increased the improvement of the evaluated criteria, which increased the accuracy of diagnosis by 27.68%, compared to former works in the field. As such, it could be concluded that the proposed method had a higher ability to diagnose breast cancer, compared to previous techniques."
    },
    {
        "url": "https://paperity.org/p/229936770/emotion-analysis-of-arabic-tweets-using-deep-learning-approach",
        "title": "Emotion analysis of Arabic tweets using deep learning approach",
        "authors": [
            "Massa Baali",
            " Nada Ghneim"
        ],
        "date_article": "10-2019",
        "short_description": "Nowadays, sharing moments on social networks have become something widespread. Sharing ideas, thoughts, and good memories to express our emotions through text without using a lot of words. Twitter, for instance, is a rich source of data that is a target for organizations for which they can use to analyze people’s opinions, sentiments and emotions. Emotion analysis normally gives...",
        "keywords": [
            "Deep learning",
            "Big Data—emotion recognition of Arabic texts",
            "CNN sentence classification",
            "Data mining",
            "SVM",
            "NB",
            "MLP"
        ],
        "number_of_pages": "12",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0252-x.pdf",
        "abstract": "Nowadays, sharing moments on social networks have become something widespread. Sharing ideas, thoughts, and good memories to express our emotions through text without using a lot of words. Twitter, for instance, is a rich source of data that is a target for organizations for which they can use to analyze people’s opinions, sentiments and emotions. Emotion analysis normally gives a more profound overview of the feelings of an author. In Arabic Social Media analysis, nearly all projects have focused on analyzing the expressions as positive, negative or neutral. In this paper we intend to categorize the expressions on the basis of emotions, namely happiness, anger, fear, and sadness. Different approaches have been carried out in the area of automatic textual emotion recognition in the case of other languages, but only a limited number were based on deep learning. Thus, we present our approach used to classify emotions in Arabic tweets. Our model implements a deep Convolutional Neural Networks (CNN) trained on top of trained word vectors specifically on our dataset for sentence classification tasks. We compared the results of this approach with three other machine learning algorithms which are SVM, NB and MLP. The architecture of our deep learning approach is an end-to-end network with word, sentence, and document vectorization steps. The deep learning proposed approach was evaluated on the Arabic tweets dataset provided by SemiEval for the EI-oc task, and the results-compared to the traditional machine learning approaches-were excellent."
    },
    {
        "url": "https://paperity.org/p/229930036/a-survey-on-driving-behavior-analysis-in-usage-based-insurance-using-big-data",
        "title": "A survey on driving behavior analysis in usage based insurance using big data",
        "authors": [
            "Subramanian Arumugam",
            " R. Bhargavi"
        ],
        "date_article": "09-2019",
        "short_description": "The emergence and growth of connected technologies and the adaptation of big data are changing the face of all industries. In the insurance industry, Usage-Based Insurance (UBI) is the most popular use case of big data adaptation. Initially UBI is started as a simple unitary Pay-As-You-Drive (PAYD) model in which the classification of good and bad drivers is an unresolved task...",
        "keywords": [
            "Big data",
            "Big data analytics",
            "Driving behavior",
            "Usage based insurance",
            "Pay-As-You-Drive",
            "Pay-How-You-Drive",
            "Manage-How-You-Drive",
            "Machine learning"
        ],
        "number_of_pages": "21",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0249-5.pdf",
        "abstract": "The emergence and growth of connected technologies and the adaptation of big data are changing the face of all industries. In the insurance industry, Usage-Based Insur-ance (UBI) is the most popular use case of big data adaptation. Initially UBI is started as a simple unitary Pay-As-You-Drive (PAYD) model in which the classification of good and bad drivers is an unresolved task. PAYD is progressed towards Pay-How-You-Drive (PHYD) model in which the premium is charged for the personal auto insurance depending on the post-trip analysis. Providing proactive alerts to guide the driver during the trip is the drawback of the PHYD model. PHYD model is further progressed towards Manage-How-You-Drive (MHYD) model in which the proactive engagement in the form of alerts is provided to the drivers while they drive. The evolution of PAYD, PHYD and MHYD models serve as the building blocks of UBI and facilitates the insur-ance industry to bridge the gap between insurer and the customer with the intro-duction of MHYD model. Increasing number of insurers are starting to launch PHYD or MHYD models all over the world and widespread customer adaptation is seen to improve the driver safety by monitoring the driving behavior. Consequently, the data flow between an insurer and their customers is increasing exponentially, which makes the need for big data adaptation, a foundational brick in the technology landscape of insurers. The focus of this paper is to perform a detailed survey about the categories of MHYD. The survey results in the need to address the aggressive driving behavior and road rage incidents of the drivers during short-term and long-term driving. The exhaus-tive survey is also used to propose a solution that finds the risk posed by aggressive driving and road rage incidents by considering the behavioral and emotional factors of a driver. The outcome of this research would help the insurance industries to assess the driving risk more accurately and to propose a solution to calculate the personalized premium based on the driving behavior with most importance towards prevention of risk."
    },
    {
        "url": "https://paperity.org/p/230016295/development-of-a-predictive-model-for-on-time-arrival-flight-of-airliner-by-discovering",
        "title": "Development of a predictive model for on-time arrival flight of airliner by discovering correlation between flight and weather data",
        "authors": [
            "Noriko Etani"
        ],
        "date_article": "09-2019",
        "short_description": "An important business of airlines is to get customer satisfaction. Due to bad weather, a mechanical reason, and the late arrival of the aircraft to the point of departure, flights delay and lead to customer dissatisfaction. A predictive model of on-time arrival flight is proposed with using flight data and weather data. The key research in this paper is to discover the...",
        "keywords": [
            "Predictive model",
            "On-time arrival flight of airliner",
            "Correlation between flight and weather data",
            "Pressure pattern",
            "Sea-level pressure",
            "Random Forest Classifier"
        ],
        "number_of_pages": "17",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0251-y.pdf",
        "abstract": "An important business of airlines is to get customer satisfaction. Due to bad weather, a mechanical reason, and the late arrival of the aircraft to the point of departure, flights delay and lead to customer dissatisfaction. A predictive model of on-time arrival flight is proposed with using flight data and weather data. The key research in this paper is to discover the correlation between flight data and weather data. The relation between pressure pattern and flight data of Peach Aviation, which is LCC (low-cost carrier) in Japan, are clarified, and it is found that the sea-level pressures of 3 weather observation spots, which are Wakkanai as the most northern spot, Minami-Torishima as the most eastern spot, and Yonagunijima as the most western spot, can classify the pressure patterns. As a result, on-time arrival fight is predicted at 77% of the accuracy with using Random Forest Classifier of machine learning. Furthermore, feasibility of the predictive model is evaluated by developing a tool of on-time arrival flight prediction."
    },
    {
        "url": "https://paperity.org/p/203253788/evaluating-the-performance-of-sentence-level-features-and-domain-sensitive-features-of",
        "title": "Evaluating the performance of sentence level features and domain sensitive features of product reviews on supervised sentiment analysis tasks",
        "authors": [
            "Bagus Setya Rintyarna",
            " Riyanarto Sarno",
            " Chastine Fatichah"
        ],
        "date_article": "09-2019",
        "short_description": "With the popularity of e-commerce, posting online product reviews expressing customer’s sentiment or opinion towards products has grown exponentially. Sentiment analysis is a computational method that plays an essential role in automating the extraction of subjective information i.e. customer’s sentiment or opinion from online product reviews. Two approaches commonly used in...",
        "keywords": [
            "Sentiment analysis",
            "Online product reviews",
            "Supervised approach",
            "Machine learning"
        ],
        "number_of_pages": "19",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0246-8.pdf",
        "abstract": "With the popularity of e-commerce, posting online product reviews expressing customer’s sentiment or opinion towards products has grown exponentially. Senti-ment analysis is a computational method that plays an essential role in automating the extraction of subjective information i.e. customer’s sentiment or opinion from online product reviews. Two approaches commonly used in Sentiment analysis tasks are supervised approaches and lexicon-based approaches. In supervised approaches, Sentiment analysis is seen as a text classification task. The result depends not only on the robustness of the machine learning algorithm but also on the utilized features. Bag-of-word is a common utilized features. As a statistical feature, bag-of-word does not take into account semantic of words. Previous research has indicated the potential of semantic in supervised SA task. To augment the result of sentiment analysis, this paper proposes a method to extract text features named sentence level features (SLF) and domain sensitive features (DSF) which take into account semantic of words in both sentence level and domain level of product reviews. A word sense disambiguation based method was adapted to extract SLF. For every similarity employed in generating SLF, the SentiCircle-based method was enhanced to generate DSF. Results of the exper-iments indicated that our proposed semantic features i.e. SLF and SLF + DSF  favorably increase the performance of supervised sentiment analysis on product reviews."
    },
    {
        "url": "https://paperity.org/p/203282541/advancing-community-detection-using-keyword-attribute-search",
        "title": "Advancing community detection using Keyword Attribute Search",
        "authors": [
            "Sanket Chobe",
            " Justin Zhan"
        ],
        "date_article": "09-2019",
        "short_description": "As social network structures evolve constantly, it is necessary to design an efficient mechanism to track the influential nodes and accurate communities in the networks. The attributed graph represents the information about properties of the nodes and relationships between different nodes, hence, this attribute information can be used for more accurate community detection...",
        "keywords": [
            "Keyword search",
            "Attributed graphs",
            "Attribute index",
            "Personalized community detection",
            "Generalized community detection"
        ],
        "number_of_pages": "33",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0243-y.pdf",
        "abstract": "As social network structures evolve constantly, it is necessary to design an efficient mechanism to track the influential nodes and accurate communities in the networks. The attributed graph represents the information about properties of the nodes and relationships between different nodes, hence, this attribute information can be used for more accurate community detection. Current techniques of community detection do not consider the attribute or keyword information associated with the nodes in a graph. In this paper, we propose a novel algorithm of online community detection using a technique of keyword search over the attributed graph. First, the influential attributes are derived based on the probability of occurrence of each attribute type-value pair on all nodes and edges, respectively. Then, a compact Keyword Attribute Sig-nature is created for each node based on the unique id of each influential attribute. The attributes on each node are classified into different classes, and this class information is assigned on each node to derive the strongest association among different nodes. Once the class information is assigned to all the nodes, we use a keyword search technique to derive a community of nodes belonging to the same class. The keyword search technique makes it possible to search community of nodes in an online and computationally efficient manner compared to the existing techniques. The experi-mental analysis shows that the proposed method derive the community of nodes in an online manner. The nodes in a community are strongly connected to each other and share common attributes. Thus, the community detection can be advanced by using keyword search method, which allows personalized and generalized communities to be retrieved in an online manner."
    },
    {
        "url": "https://paperity.org/p/203311294/multi-dimensional-geospatial-data-mining-in-a-distributed-environment-using-mapreduce",
        "title": "Multi-dimensional geospatial data mining in a distributed environment using MapReduce",
        "authors": [
            "Mazin Alkathiri",
            " Abdul Jhummarwala",
            " M. B. Potdar"
        ],
        "date_article": "09-2019",
        "short_description": "Data mining and machine learning techniques for processing raster data consider a single spectral band of data at a time. The individual results are combined to obtain the final output. The essence of related multi-spectral information is lost when the bands are considered independently. The proposed platform is based on Apache Hadoop ecosystem and supports performing analysis on...",
        "keywords": [
            "Multiband raster processing",
            "Multi-dimensional data processing",
            "Geospatial processing",
            "Spectral to geometrical space",
            "K-means clustering"
        ],
        "number_of_pages": "34",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0245-9.pdf",
        "abstract": "Data mining and machine learning techniques for processing raster data consider a single spectral band of data at a time. The individual results are combined to obtain the final output. The essence of related multi-spectral information is lost when the bands are considered independently. The proposed platform is based on Apache Hadoop ecosystem and supports performing analysis on large amounts of multispectral raster data using MapReduce. A novel technique of transforming the spectral space to the geometrical space is also proposed. The technique allows to consider multiple bands coherently. The results of clustering  106 pixels for multiband imagery with widely used GIS software have been tested and other machine learning methods are planned to be incorporated in the platform. The platform is scalable to support tens of spectral bands. The results from our platform were found to be better and are also available faster due to application of distributed processing."
    },
    {
        "url": "https://paperity.org/p/203340047/analysis-and-classification-of-heart-diseases-using-heartbeat-features-and-machine",
        "title": "Analysis and classification of heart diseases using heartbeat features and machine learning algorithms",
        "authors": [
            "Fajr Ibrahem Alarsan",
            " Mamoon Younes"
        ],
        "date_article": "08-2019",
        "short_description": "This study proposed an ECG (Electrocardiogram) classification approach using machine learning based on several ECG features. An electrocardiogram (ECG) is a signal that measures the electric activity of the heart. The proposed approach is implemented using ML-libs and Scala language on Apache Spark framework; MLlib is Apache Spark’s scalable machine learning library. The key...",
        "keywords": [
            "Heartbeats classification",
            "Electrocardiogram (ECG)",
            "Machine-learning libraries (MLlib)",
            "Spark–Scala"
        ],
        "number_of_pages": "15",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0244-x.pdf",
        "abstract": "This study proposed an ECG (Electrocardiogram) classification approach using machine learning based on several ECG features. An electrocardiogram (ECG) is a signal that measures the electric activity of the heart. The proposed approach is implemented using ML-libs and Scala language on Apache Spark framework; MLlib is Apache Spark’s scalable machine learning library. The key challenge in ECG classification is to handle the irregularities in the ECG signals which is very important to detect the patient status. Therefore, we have proposed an efficient approach to classify ECG signals with high accuracy Each heartbeat is a combination of action impulse waveforms produced by different specialized cardiac heart tissues. Heartbeats classification faces some difficul-ties because these waveforms differ from person to another, they are described by some features. These features are the inputs of machine learning algorithm. In general, using Spark–Scala tools simplifies the usage of many algorithms such as machine-learning (ML) algorithms. On other hand, Spark–Scala is preferred to be used more than other tools when size of processing data is too large. In our case, we have used a dataset with 205,146 records to evaluate the performance of our approach. Machine learning libraries in Spark–Scala provide easy ways to implement many classification algorithms (Decision Tree, Random Forests, Gradient-Boosted Trees (GDB), etc.). The proposed method is evaluated and validated on baseline MIT-BIH Arrhythmia and MIT-BIH Supraventricular Arrhythmia database. The results show that our approach achieved an overall accuracy of 96.75% using GDB Tree algorithm and 97.98% using random Forest for binary classification. For multi class classification, it achieved to 98.03% accuracy using Random Forest, Gradient Boosting tree supports only binary classification."
    },
    {
        "url": "https://paperity.org/p/203368800/using-electronic-transaction-data-to-add-geographic-granularity-to-official-estimates-of",
        "title": "Using electronic transaction data to add geographic granularity to official estimates of retail sales",
        "authors": [
            "Brian Dumbacher",
            " Darcy Steeg Morris",
            " Carma Hogu"
        ],
        "date_article": "08-2019",
        "short_description": "IntroductionEconomists are interested in more granular, more frequent data to aid in their understanding of the U.S. economy. The most frequent economic data currently available from the U.S. Census Bureau come from monthly economic indicators such as the Monthly Retail Trade Survey, which produces national estimates of retail sales. On the other hand, the most granular data (in...",
        "keywords": [
            "Bayesian methods",
            "Electronic transactions",
            "Model-based estimation",
            "Monthly Retail Trade Survey",
            "Official statistics",
            "Organic data",
            "Third-party data",
            "U.S. Census Bureau"
        ],
        "number_of_pages": "23",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0242-z.pdf",
        "abstract": "Introduction:Economists are interested in more granular, more frequent data to aid in their understanding of the U.S. economy. The most frequent economic data cur-rently available from the U.S. Census Bureau come from monthly economic indicators such as the Monthly Retail Trade Survey, which produces national estimates of retail sales. On the other hand, the most granular data (in terms of geographic and indus-try detail) come from the Economic Census, which is conducted every five years. The Census Bureau is researching whether organic, third-party Big Data sources, in conjunc-tion with survey data, allow for the production of retail sales estimates that are both monthly and subnational.Case description:This case study explores the feasibility of using aggregated elec-tronic transaction data from First Data (FD), a large payment processor, to calculate experimental regional and state-level monthly estimates of retail sales. Quality criteria are devised to understand this data source’s representativeness of the target popula-tion and consistency with existing survey data. Five retail industries in the FD transac-tion data are identified as having acceptable quality for estimation. Estimation meth-odology is developed based on linear mixed models in a Bayesian framework. These models try to take advantage of the timeliness of the FD transaction data and smooth over artifacts of FD’s business activity. Experimental estimates of retail sales are calcu-lated for the period January 2015 through March 2018.Discussion and evaluation:The experimental estimates are evaluated quantitatively via correlations between external estimates of the number of employees by industry and qualitatively with respect to additional information about the economy. Many fea-tures of the experimental estimates seem reasonable, but there are also caution flags such as anomalous trends related to identified FD quality issues.Conclusions:The FD transaction data offer insight into economic activity at a more granular level. However, using this data source to enhance official estimates of retail sales is challenging; the FD aggregates have limitations in terms of suppression, coverage, and trends. Consequently, fewer industries than expected are identified as having acceptable quality for estimation. Future work involves calculating experimental estimates for more recent months and researching alternative methods for evaluating their accuracy.Open Access© This is a U.S. Government work and not under copyright protection in the US; foreign copyright protection may apply 2019. This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (http://creat iveco mmons .org/licen ses/by/4.0/),  which  permits  unrestricted  use,  distribution,  and  reproduction  in  any  medium,  provided  you  give  appropriate  credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made.CASE STUDYDumbacher et al. J Big Data            (2019) 6:80  https://doi.org/10.1186/s40537-019-0242-z*Correspondence:   brian.dumbacher@census.govU.S. Census Bureau, 4600 Silver Hill Road, Washington, DC 20233, USA\nPage 2 of 23Dumbacher et al. J Big Data            (2019) 6:80 "
    },
    {
        "url": "https://paperity.org/p/203397553/feature-selection-methods-and-genomic-big-data-a-systematic-review",
        "title": "Feature selection methods and genomic big data: a systematic review",
        "authors": [
            "Khawla Tadist",
            " Said Najah",
            " Nikola S. Nikolov"
        ],
        "date_article": "08-2019",
        "short_description": "In the era of accelerating growth of genomic data, feature-selection techniques are believed to become a game changer that can help substantially reduce the complexity of the data, thus making it easier to analyze and translate it into useful information. It is expected that within the next decade, researchers will head towards analyzing the genomes of all living creatures making...",
        "keywords": [
            "Systematic review",
            "Mapping process",
            "Genomic big data",
            "Feature selection"
        ],
        "number_of_pages": "24",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0241-0.pdf",
        "abstract": "In the era of accelerating growth of genomic data, feature-selection techniques are believed to become a game changer that can help substantially reduce the complexity of the data, thus making it easier to analyze and translate it into useful information. It is expected that within the next decade, researchers will head towards analyzing the genomes of all living creatures making genomics the main generator of data. Feature selection techniques are believed to become a game changer that can help substan-tially reduce the complexity of genomic data, thus making it easier to analyze it and translating it into useful information. With the absence of a thorough investigation of the field, it is almost impossible for researchers to get an idea of how their work relates to existing studies as well as how it contributes to the research community. In this paper, we present a systematic and structured literature review of the feature-selection techniques used in studies related to big genomic data analytics."
    },
    {
        "url": "https://paperity.org/p/203164162/leveraging-resource-management-for-efficient-performance-of-apache-spark",
        "title": "Leveraging resource management for efficient performance of Apache Spark",
        "authors": [
            "Khadija Aziz",
            " Dounia Zaidouni",
            " Mostafa Bellafkih"
        ],
        "date_article": "08-2019",
        "short_description": "Apache Spark is one of the most widely used open source processing framework for big data, it allows to process large datasets in parallel using a large number of nodes. Often, applications of this framework use resource management systems like YARN, which provide jobs a specific amount of resources for their execution. In addition, a distributed file system such as HDFS stores...",
        "keywords": [
            "Resource management",
            "Performance",
            "Tuning",
            "Distributed data processing",
            "Machine learning algorithms",
            "Apache Spark",
            "MLlib"
        ],
        "number_of_pages": "23",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0240-1.pdf",
        "abstract": "Apache Spark is one of the most widely used open source processing framework for big data, it allows to process large datasets in parallel using a large number of nodes. Often, applications of this framework use resource management systems like YARN, which provide jobs a specific amount of resources for their execution. In addition, a distributed file system such as HDFS stores the data that is to be analyzed by the framework. This design allows sharing cluster resources effectively by running jobs on a single-node cluster or multi-nodes cluster infrastructure. Thus, one challenging issue is to realize effective resource management of these large cluster infrastructures in order to run distributed data analytics in an economically viable way. In this study, we use the Machine Learning library (MLlib) of Spark to implement different machine learning algorithms, then we manage the resources (CPU, memory, and Disk) in order to assess the performance of Apache Spark. In this paper, we present a review of various works that focus on resource management and data processing in Big Data platforms. Fur-thermore, we perform a scalability analysis using Spark. We analyze the speedup and processing time. We deduce that from a certain number of nodes in the cluster, it is no longer necessary to add additional nodes to improve the speedup and the processing Time. Then, we investigate the tuning of the resource allocation in Spark. We showed that it is not only by allocating all the available resources we get better performance but it depends on how to tune the resource allocation. We propose new managed parameters and we show that they give better total processing time than the default parameters used by Spark. Finally, we study the Persistence of Resilient Distributed Datasets (RDDs) in Spark using machine learning algorithms. We show that one storage level gives the best execution time among all tested storage levels."
    },
    {
        "url": "https://paperity.org/p/203250421/big-data-clustering-with-varied-density-based-on-mapreduce",
        "title": "Big data clustering with varied density based on MapReduce",
        "authors": [
            "Safanaz Heidari",
            " Mahmood Alborzi",
            " Reza Radfar"
        ],
        "date_article": "08-2019",
        "short_description": "The DBSCAN algorithm is a prevalent method of density-based clustering algorithms, the most important feature of which is the ability to detect arbitrary shapes and varied clusters and noise data. Nevertheless, this algorithm faces a number of challenges, including failure to find clusters of varied densities. On the other hand, with the rapid development of the information age...",
        "keywords": [
            "Map-Reduce",
            "Density-based clustering",
            "Big data"
        ],
        "number_of_pages": "16",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0236-x.pdf",
        "abstract": "The DBSCAN algorithm is a prevalent method of density-based clustering algorithms, the most important feature of which is the ability to detect arbitrary shapes and varied clusters and noise data. Nevertheless, this algorithm faces a number of challenges, including failure to find clusters of varied densities. On the other hand, with the rapid development of the information age, plenty of data are produced every day, such that a single machine alone cannot process this volume of data; hence, new technologies are required to store and extract information from this volume of data. A large volume of data that is beyond the capabilities of existing software is called Big data. In this paper, we have attempted to introduce a new algorithm for clustering big data with varied density using a Hadoop platform running MapReduce. The main idea of this research is the use of local density to find each point’s density. This strategy can avoid the situation of connecting clusters with varying densities. The proposed algorithm is implemented and compared with other algorithms using the MapReduce paradigm and shows the best varying density clustering capability and scalability."
    },
    {
        "url": "https://paperity.org/p/203221668/positive-and-negative-association-rule-mining-in-hadoops-mapreduce-environment",
        "title": "Positive and negative association rule mining in Hadoop’s MapReduce environment",
        "authors": [
            "Sikha Bagui",
            " Probal Chandra Dhar"
        ],
        "date_article": "08-2019",
        "short_description": "In this paper, we present a Hadoop implementation of the Apriori algorithm. Using Hadoop’s distributed and parallel MapReduce environment, we present an architecture to mine positive as well as negative association rules in big data using frequent itemset mining and the Apriori algorithm. We also analyze and present the results of a few optimization parameters in Hadoop’s...",
        "keywords": [
            "Positive association rule mining",
            "Negative association rule mining",
            "Hadoop",
            "MapReduce",
            "Apriori",
            "Big data",
            "Frequent itemset mining",
            "Parallel environment",
            "Hadoop’s Distributed File System (HDFS)"
        ],
        "number_of_pages": "16",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0238-8.pdf",
        "abstract": "In this paper, we present a Hadoop implementation of the Apriori algorithm. Using Hadoop’s distributed and parallel MapReduce environment, we present an architec-ture to mine positive as well as negative association rules in big data using frequent itemset mining and the Apriori algorithm. We also analyze and present the results of a few optimization parameters in Hadoop’s MapReduce environment as it relates to this algorithm. The results are presented based on the number of rules generated as well as the run-time efficiency. We find that, a higher amount of parallelization, which means larger block sizes, will increase the run-time efficiency of the Hadoop implementation of the Apriori algorithm."
    },
    {
        "url": "https://paperity.org/p/203192915/correction-to-a-novel-adaptable-approach-for-sentiment-analysis-on-big-social-data",
        "title": "Correction to: A novel adaptable approach for sentiment analysis on big social data",
        "authors": [
            "Imane El Alaoui",
            " Youssef Gahi",
            " Rochdi Messoussi"
        ],
        "date_article": "08-2019",
        "short_description": "",
        "keywords": [
            "-"
        ],
        "number_of_pages": "3",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0239-7.pdf",
        "abstract": "tion to: A novel adaptable approach for sentiment analysis on big social dataImane El Alaoui1,2*, Youssef Gahi3, Rochdi Messoussi1, Youness Chaabi1, Alexis Todoskoff2and Abdessamad Kobi2Correction to:  J Big Data (2018) 5:12  https ://doi.org/10.1186/s4053 7-018-0120-0The authors note a correction to the article [1]. Table 5 of the original article is incom-plete.  Few  percentage  values  are  missing.  This  article  presents  the  corrected  version  of  Table 5.Open Access©  The  Author(s)  2019.  This  article  is  distributed  under  the  terms  of  the  Creative  Commons  Attribution  4.0  International  License  (http://creat iveco mmons .org/licen ses/by/4.0/),  which  permits  unrestricted  use,  distribution,  and  reproduction  in  any  medium,  provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made.CORRECTIONEl Alaoui et al. J Big Data            (2019) 6:76  https://doi.org/10.1186/s40537-019-0239-7*Correspondence:   imane.el.alaoui@uit.ac.ma 1 Laboratoire des Systèmes de Télécommunications et Ingénierie de la Décision, University of Ibn Tofail, Kenitra, MoroccoFull list of author information is available at the end of the article\nPage 2 of 3El Alaoui et al. J Big Data            (2019) 6:76 Table 5 Classification accuracyProposed methodNaïve BayesGoogle prediction APIAccuracyMean accuracyMacro precisionMacro recallMacro F-measureAccuracyMean accuracyMacro precisionMacro recallMacro F-measureMean accuracyMacro precisionMacro recallMacro F-measureHighly positive Trump88.52%85.71%Moderately positive Trump88%35.71%Lightly positive Trump88%14.21%Highly positive Hillary98%21.42%Moderately positive Hillary86%14.28%Lightly positive Hillary86%90.21%90.23%89.57%89.98%57.14%34.45%33.92%31.85%32.86%66.66%64.02%65.31%64.65%Highly negative Trump90%0.70%Moderately negative Trump96%57.14%Lightly negative Trump88%42.28%Highly negative Hillary98%14.28%Moderately negative Hillary88%14.28%Lightly negative Hillary88%57.14%"
    },
    {
        "url": "https://paperity.org/p/203279174/vada-an-architecture-for-end-user-informed-data-preparation",
        "title": "VADA: an architecture for end user informed data preparation",
        "authors": [
            "Nikolaos Konstantinou",
            " Edward Abel",
            " Luigi Bellomarini"
        ],
        "date_article": "08-2019",
        "short_description": "BackgroundData scientists spend considerable amounts of time preparing data for analysis. Data preparation is labour intensive because the data scientist typically takes fine grained control over each aspect of each step in the process, motivating the development of techniques that seek to reduce this burden.ResultsThis paper presents an architecture in which the data scientist...",
        "keywords": [
            "Data preparation",
            "Data quality",
            "Data integration"
        ],
        "number_of_pages": "32",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0237-9.pdf",
        "abstract": "Background:Data scientists spend considerable amounts of time preparing data for analysis. Data preparation is labour intensive because the data scientist typically takes fine grained control over each aspect of each step in the process, motivating the devel-opment of techniques that seek to reduce this burden.Results:This paper presents an architecture in which the data scientist need only describe the intended outcome of the data preparation process, leaving the software to determine how best to bring about the outcome. Key wrangling decisions on matching, mapping generation, mapping selection, format transformation and data repair are taken by the system, and the user need only provide: (i) the schema of the data target; (ii) partial representative instance data aligned with the target; (iii) criteria to be prioritised when populating the target; and (iv) feedback on candidate results. To support this, the proposed architecture dynamically orchestrates a collection of loosely coupled wrangling components, in which the orchestration is declaratively specified and includes self-tuning of component parameters.Conclusion:This paper describes a data preparation architecture that has been designed to reduce the cost of data preparation through the provision of a central role for automation. An empirical evaluation with deep web and open government data investigates the quality and suitability of the wrangling result, the cost-effectiveness of the approach, the impact of self-tuning, and scalability with respect to the numbers of sources."
    },
    {
        "url": "https://paperity.org/p/203307927/traffic-flow-estimation-with-data-from-a-video-surveillance-camera",
        "title": "Traffic flow estimation with data from a video surveillance camera",
        "authors": [
            "Aleksandr Fedorov",
            " Kseniia Nikolskaia",
            " Sergey Ivanov"
        ],
        "date_article": "08-2019",
        "short_description": "This study addresses the problem of traffic flow estimation based on the data from a video surveillance camera. Target problem here is formulated as counting and classifying vehicles by their driving direction. This subject area is in early development, and the focus of this work is only one of the busiest crossroads in city Chelyabinsk, Russia. To solve the posed problem, we...",
        "keywords": [
            "Traffic flow estimation",
            "Traffic analysis",
            "Vehicle detection",
            "Convolutional neural network",
            "Surveillance data"
        ],
        "number_of_pages": "15",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0234-z.pdf",
        "abstract": "This study addresses the problem of traffic flow estimation based on the data from a video surveillance camera. Target problem here is formulated as counting and classify-ing vehicles by their driving direction. This subject area is in early development, and the focus of this work is only one of the busiest crossroads in city Chelyabinsk, Russia. To solve the posed problem, we employed the state-of-the-art Faster R-CNN two-stage detector together with SORT tracker. A simple regions-based heuristic algorithm was used to classify vehicles movement direction. The baseline performance of the Faster R-CNN was enhanced by several modifications: focal loss, adaptive feature pooling, additional mask branch, and anchors optimization. To train and evaluate detector, we gathered 982 video frames with more than 60,000 objects presented in various condi-tions. The experimental results show that the proposed system can count vehicles and classify their driving direction during weekday rush hours with mean absolute percent-age error that is less than 10%. The dataset presented here might be further used by other researches as a challenging test or additional training data."
    },
    {
        "url": "https://paperity.org/p/203394186/is-bigger-always-better-a-controversial-journey-to-the-center-of-machine-learning-design",
        "title": "Is bigger always better? A controversial journey to the center of machine learning design, with uses and misuses of big data for predicting water meter failures",
        "authors": [
            "Marco Roccetti",
            " Giovanni Delnevo",
            " Luca Casini"
        ],
        "date_article": "08-2019",
        "short_description": "In this paper, we describe the design of a machine learning-based classifier, tailored to predict whether a water meter will fail or need a replacement. Our initial attempt to train a recurrent deep neural network (RNN), based on the use of 15 million of readings gathered from 1 million of mechanical water meters, spread throughout Northern Italy, led to non-positive results. We...",
        "keywords": [
            "Smart data",
            "Machine learning design",
            "Human–machine–bigdata interaction loop",
            "Human-in-the-loop methods",
            "Water metering and consumption"
        ],
        "number_of_pages": "23",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0235-y.pdf",
        "abstract": "In this paper, we describe the design of a machine learning-based classifier, tailored to predict whether a water meter will fail or need a replacement. Our initial attempt to train a recurrent deep neural network (RNN), based on the use of 15 million of read-ings gathered from 1 million of mechanical water meters, spread throughout Northern Italy, led to non-positive results. We learned this was due to a lack of specific attention devoted to the quality of the analyzed data. We, hence, developed a novel methodol-ogy, based on a new semantics which we enforced on the training data. This allowed us to extract only those samples which are representative of the complex phenome-non of defective water meters. Adopting such a methodology, the accuracy of our RNN exceeded the 80% threshold. We simultaneously realized that the new training dataset differed significantly, in statistical terms, from the initial dataset, leading to an apparent paradox. Thus, with our contribution, we have demonstrated how to reconcile such a paradox, showing that our classifier can help detecting defective meters, while simpli-fying replacement procedures."
    },
    {
        "url": "https://paperity.org/p/203365433/big-data-analytics-a-link-between-knowledge-management-capabilities-and-superior-cyber",
        "title": "Big data analytics: a link between knowledge management capabilities and superior cyber protection",
        "authors": [
            "Peter Oluseyi Obitad"
        ],
        "date_article": "08-2019",
        "short_description": "As cybersecurity threats increase in frequency and sophistication, organizations are realizing that one of their strongest resources to combat cyberattacks lies in the growing volume of data at their disposal. However, traditional knowledge management technologies have limited capabilities to effectively process and analyze these larger data volumes that can provide managers with...",
        "keywords": [
            "Big data analytics",
            "Business intelligence",
            "Knowledge management processes and structures",
            "Cyber agility",
            "Information technology capability",
            "Organizational capabilities"
        ],
        "number_of_pages": "28",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0229-9.pdf",
        "abstract": "As cybersecurity threats increase in frequency and sophistication, organizations are realizing that one of their strongest resources to combat cyberattacks lies in the growing volume of data at their disposal. However, traditional knowledge manage-ment technologies have limited capabilities to effectively process and analyze these larger data volumes that can provide managers with pertinent information to make better-informed security decisions. Using survey data from 479 business and informa-tion security executives and drawing from the resource-based view, I find that 69% of organizations that deployed big data analytics reported significant improvements in their cyber knowledge management capabilities. Second, the findings also confirm a significantly positive association between big data analytics and cyber agility. In particular, 72% of organizations that deployed big data analytics solutions reported significant improvements in their ability to detect and respond faster to cyber threats. Lastly, compared to 56% of organizations without big data analytics, 78% of companies that have deployed big data analytics technologies considered their cyber security capabilities adequate and robust enough to protect critical information assets from cyberattacks. Overall, the results demonstrate that organizations need advanced data analytics capabilities to effectively leverage IT resources, allowing them to respond to cyber incidents with speed and agility which ultimately cumulates in the superior cyber protection of valuable information assets. The results also provide some useful implications for research and managerial practices."
    },
    {
        "url": "https://paperity.org/p/203336680/evaluation-of-maxout-activations-in-deep-learning-across-several-big-data-domains",
        "title": "Evaluation of maxout activations in deep learning across several big data domains",
        "authors": [
            "Gabriel Castaneda",
            " Paul Morris",
            " Taghi M. Khoshgoftaar"
        ],
        "date_article": "08-2019",
        "short_description": "This study investigates the effectiveness of multiple maxout activation function variants on 18 datasets using Convolutional Neural Networks. A network with maxout activation has a higher number of trainable parameters compared to networks with traditional activation functions. However, it is not clear if the activation function itself or the increase in the number of trainable...",
        "keywords": [
            "Maxout networks",
            "Activation functions",
            "Big data",
            "Deep learning"
        ],
        "number_of_pages": "35",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0233-0.pdf",
        "abstract": "This study investigates the effectiveness of multiple maxout activation function vari-ants on 18 datasets using Convolutional Neural Networks. A network with maxout activation has a higher number of trainable parameters compared to networks with traditional activation functions. However, it is not clear if the activation function itself or the increase in the number of trainable parameters is responsible in yielding the best performance for different entity recognition tasks. This paper investigates if an increase in the number of convolutional filters on traditional activation functions performs equal-to or better-than maxout networks. Our experiments compare the Rectified Linear Unit, Leaky Rectified Linear Unit, Scaled Exponential Linear Unit, and Hyperbolic Tangent activations to four maxout function variants. We observe that maxout net-works train relatively slower than networks with traditional activation functions, e.g. Rectified Linear Unit. In addition, we found that on average, across all datasets, the Rectified Linear Unit activation function performs better than any maxout activation when the number of convolutional filters is increased. Furthermore, adding more filters enhances the classification accuracy of the Rectified Linear Unit networks, without adversely affecting their advantage over maxout activations with respect to network-training speed."
    },
    {
        "url": "https://paperity.org/p/203422939/examining-characteristics-of-predictive-models-with-imbalanced-big-data",
        "title": "Examining characteristics of predictive models with imbalanced big data",
        "authors": [
            "Tawfiq Hasanin",
            " Taghi M. Khoshgoftaar",
            " Joffrey L. Leevy"
        ],
        "date_article": "07-2019",
        "short_description": "High class imbalance between majority and minority classes in datasets can skew the performance of Machine Learning algorithms and bias predictions in favor of the majority (negative) class. This bias, for cases where the minority (positive) class is of greater interest and the occurrence of false negatives is costlier than false positives, may result in adverse consequences. Our...",
        "keywords": [
            "Big data",
            "Feature Importance",
            "Feature Selection",
            "Class Imbalance",
            "Machine Learning",
            "Random Undersampling",
            "ECBDL’14",
            "Slowloris",
            "POST"
        ],
        "number_of_pages": "21",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0231-2.pdf",
        "abstract": "High class imbalance between majority and minority classes in datasets can skew the performance of Machine Learning algorithms and bias predictions in favor of the major-ity (negative) class. This bias, for cases where the minority (positive) class is of greater interest and the occurrence of false negatives is costlier than false positives, may result in adverse consequences. Our paper presents two case studies, each utilizing a unique, combined approach of Random Undersampling and Feature Selection to investigate the effect of class imbalance on big data analytics. Random Undersampling is used to gen-erate six class distributions ranging from balanced to moderately imbalanced, and Fea-ture Importance is used as our Feature Selection method. Classification performance was reported for the Random Forest, Gradient-Boosted Trees, and Logistic Regression learners, as implemented within the Apache Spark framework. The first case study utilized a training dataset and a test dataset from the ECBDL’14 bioinformatics competition. The training and test datasets contain about 32 million instances and 2.9 million instances, respectively. For the first case study, Gradient-Boosted Trees obtained the best results, with either a features-set of 60 or the full set, and a negative-to-positive ratio of either 45:55 or 40:60. The second case study, unlike the first, included training data from one source (POST dataset) and test data from a separate source (Slowloris dataset), where POST and Slowloris are two types of Denial of Service attacks. The POST dataset con-tains about 1.7 million instances, while the Slowloris dataset contains about 0.2 million instances. For the second case study, Logistic Regression obtained the best results, with a features-set of 5 and any of the following negative-to-positive ratios: 40:60, 45:55, 50:50, 65:35, and 75:25. We conclude that combining Feature Selection with Random Undersampling improves the classification performance of learners with imbalanced big data from different application domains."
    },
    {
        "url": "https://paperity.org/p/203218301/impact-of-class-distribution-on-the-detection-of-slow-http-dos-attacks-using-big-data",
        "title": "Impact of class distribution on the detection of slow HTTP DoS attacks using Big Data",
        "authors": [
            "Chad L. Calvert",
            " Taghi M. Khoshgoftaar"
        ],
        "date_article": "07-2019",
        "short_description": "The integrity of modern network communications is constantly being challenged by more sophisticated intrusion techniques. Attackers are consistently shifting to stealthier and more complex forms of attacks in an attempt to bypass known mitigation strategies. In recent years, attackers have begun to focus their attack efforts on the application layer, allowing them to produce...",
        "keywords": [
            "Class imbalance",
            "Slow HTTP DoS",
            "Class imbalance",
            "Big Data"
        ],
        "number_of_pages": "18",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0230-3.pdf",
        "abstract": "The integrity of modern network communications is constantly being challenged by more sophisticated intrusion techniques. Attackers are consistently shifting to stealthier and more complex forms of attacks in an attempt to bypass known mitigation strate-gies. In recent years, attackers have begun to focus their attack efforts on the appli-cation layer, allowing them to produce attacks that can exploit known issues within specific application protocols. Slow HTTP Denial of Service attacks are one such attack variant, which targets the HTTP protocol and can imitate legitimate user traffic in order to deny resources from a service. Successful mitigation of this attack type requires net-work analysts to evaluate large quantities of network traffic to identify and block intru-sive traffic. The issue, is that the number of legitimate traffic instances can far outnum-ber the amount of attack instances, making detection problematic. Machine learning techniques can be used to aid in detection, but the large level of imbalance between normal (majority) and attack (minority) instances can lead to inaccurate detection results. In this work, we evaluate the use of data sampling to produce varying class dis-tributions in order to counteract the effects of severely imbalanced Slow HTTP DoS big datasets. We also detail our process for collecting real-world representative Slow HTTP DoS attack traffic from a live network environment to create our datasets. Five class distributions are generated to evaluate the Slow HTTP DoS detection performance of eight machine learning techniques. Our results show that the optimal learner and class distribution combination is that of Random Forest with a 65:35 distribution ratio, obtaining an AUC value of 0.99904. Further, we determine through the use of signifi-cance testing, that the use of sampling techniques can significantly increase learner performance when detecting Slow HTTP DoS attack traffic."
    },
    {
        "url": "https://paperity.org/p/203189548/random-forest-implementation-and-optimization-for-big-data-analytics-on-lexisnexiss-high",
        "title": "Random forest implementation and optimization for Big Data analytics on LexisNexis’s high performance computing cluster platform",
        "authors": [
            "Victor M. Herrera",
            " Taghi M. Khoshgoftaar",
            " Flavio Villanustr"
        ],
        "date_article": "07-2019",
        "short_description": "In this paper, we comprehensively explain how we built a novel implementation of the Random Forest algorithm on the High Performance Computing Cluster (HPCC) Systems Platform from LexisNexis. The algorithm was previously unavailable on that platform. Random Forest’s learning process is based on the principle of recursive partitioning and although recursion per se is not allowed...",
        "keywords": [
            "Random forest",
            "LexisNexis’s high performance computing cluster (HPCC) systems platform",
            "Optimization for Big Data",
            "Distributed machine learning",
            "Turning recursion into iteration"
        ],
        "number_of_pages": "36",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0232-1.pdf",
        "abstract": "In this paper, we comprehensively explain how we built a novel implementation of the Random Forest algorithm on the High Performance Computing Cluster (HPCC) Systems Platform from LexisNexis. The algorithm was previously unavailable on that platform. Random Forest’s learning process is based on the principle of recursive partitioning and although recursion per se is not allowed in ECL (HPCC’s programming language), we were able to implement the recursive partition algorithm as an iterative split/partition process. In addition, we analyze the flaws found in our initial implemen-tation and we thoroughly describe all the modifications required to overcome the bottleneck within the iterative split/partition process, i.e., the optimization of the data gathering of selected independent variables which are used for the node’s best-split analysis. Essentially, we describe how our initial Random Forest implementation has been optimized and has become an efficient distributed machine learning implemen-tation for Big Data. By taking full advantage of the HPCC Systems Platform’s Big Data processing and analytics capabilities, we succeed in enhancing the data gathering method from an inefficient Pass them All and Filter approach into an effective and com-pletely parallelized Fetching on Demand approach. Finally, based upon the results of our learning process runtime comparison between these two approaches, we confirm the speed up of our optimized Random Forest implementation."
    },
    {
        "url": "https://paperity.org/p/203304560/exploring-crime-patterns-in-mexico-city",
        "title": "Exploring crime patterns in Mexico City",
        "authors": [
            "C. A. Piña-García",
            " Leticia Ramírez-Ramírez"
        ],
        "date_article": "07-2019",
        "short_description": "IntroductionLife in the city generates data on human behavior in many different ways. Measuring human behavior in terms of criminal offenses plays a critical role on identifying the most common crime type in each urban area. A primary concern for the population of the largest cities in the world is to avoid specific city zones that could show a significant risk. This case of...",
        "keywords": [
            "Big Data",
            "Crime patterns",
            "Mexico City",
            "Predictive model",
            "Twitter",
            "Google Trends"
        ],
        "number_of_pages": "21",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0228-x.pdf",
        "abstract": "Introduction:Life in the city generates data on human behavior in many different ways. Measuring human behavior in terms of criminal offenses plays a critical role on identifying the most common crime type in each urban area. A primary concern for the population of the largest cities in the world is to avoid specific city zones that could show a significant risk. This case of study systematically explores different sources of data including social media related to crime reports. In addition, this research seeks to examine and predict the most frequent crimes in Mexico City.Case description:This case study was conducted in the form of an exploratory research. One of the parties involved is the Mexico City Police Department which released the approximate locations and categories of all crime reports from January 2013 to September 2016. We analyze the impact of crime in Mexico City based on 13 official crime categories: Robbery passerby, Theft of motor vehicle, Robbery of business property, Card fraud, Homicide, Domestic burglary, Robbery on public transportation, Rape, Firearm injuries, Robbery in subway, Robbery on taxi, Robbery to carrier, and Rob‑bery to deliver person. We compare and analyze how people report a crime through the traditional system and using social media.Discussion and evaluation:This research uses a quantitative case study approach to investigate, how our predictive model is able to forecast the total number of reported crimes in the following week based on its previous weekly aggregated observations and Google Trends series. Similarly, this case study seeks to determine whether Twitter performs correctly as a “social crime sensor” in terms of detecting certain areas and boroughs that are more likely to show criminal behavior.Conclusions:In this study we used a linear predictive model in order to evaluate the performance of Google Trends in predicting crime rates based on weekly analysis. In addition, Twitter showed a suitable performance to discover the spatial distribution of crime frequency in Mexico City. Finally, this study provides an important opportunity to develop and encourage tailored strategies to tackle crime."
    },
    {
        "url": "https://paperity.org/p/203275807/reality-mining-and-predictive-analytics-for-building-smart-applications",
        "title": "Reality mining and predictive analytics for building smart applications",
        "authors": [
            "Hiba Asri",
            " Hajar Mousannif",
            " Hassan Al Moatassim"
        ],
        "date_article": "07-2019",
        "short_description": "BackgroundMobile phone and sensors have become very useful to understand and analyze human lifestyle because of the huge amount of data they can collect every second. This triggered the idea of combining benefits and advantages of reality mining, machine learning and big data predictive analytics tools, applied to smartphones/sensors real time. The main goal of our study is to...",
        "keywords": [
            "Big data",
            "Predictive analytics",
            "Data mining",
            "Spark",
            "Databricks",
            "Kmeans"
        ],
        "number_of_pages": "25",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0227-y.pdf",
        "abstract": "Background:Mobile phone and sensors have become very useful to understand and analyze human lifestyle because of the huge amount of data they can collect every second. This triggered the idea of combining benefits and advantages of reality mining, machine learning and big data predictive analytics tools, applied to smartphones/sen-sors real time. The main goal of our study is to build a system that interacts with mobile phones and wearable healthcare sensors to predict patterns.Methods:Wearable healthcare sensors (heart rate sensor, temperature sensor and activity sensor) and mobile phone are used for gathering real time data. All sensors are managed using IoT systems; we used Arduino for collecting data from health sensors and Raspberry Pi 3 for programming and processing. Kmeans clustering algorithm is used for patterns prediction and predicted clusters (partitions) are transmitted to the user in his front-end interface in the mobile application. Real world data and clustering validation statistics (Elbow method and Silhouette method) are used to validate the proposed system and assess its performance and effectiveness. All data management and processing tasks are conducted over Apache Spark Databricks.Results:This system relies on real time gathered data and can be applied to any pre-diction case making use of sensors and mobile generated data. As a proof of concept, we worked on predicting miscarriages to help pregnant women make quick decisions in case of miscarriage or probable miscarriage by creating a real time system prediction of miscarriage using wearable healthcare sensors, mobile tools, data mining algorithms and big data technologies. 9 risk factors contribute vastly in prediction, the Elbow method asserts that the optimal number of cluster is 2 and we achieve a higher value (0, 95) of Silhouette width that validates the good matching between clusters and observations. K-means algorithm gives good results in clustering the data."
    },
    {
        "url": "https://paperity.org/p/203247054/on-combining-big-data-and-machine-learning-to-support-eco-driving-behaviours",
        "title": "On combining Big Data and machine learning to support eco-driving behaviours",
        "authors": [
            "Giovanni Delnevo",
            " Pietro Di Lena",
            " Silvia Mirri"
        ],
        "date_article": "07-2019",
        "short_description": "A conscious use of the battery is one of the key elements to consider while driving an electric vehicle. Hence, supporting the drivers, with information about it, can be strategic in letting them drive in a better way, with the purpose of optimizing the energy consumption. In the context of electric vehicles, equipped with regenerative brakes, the driver’s braking style can make...",
        "keywords": [
            "Machine learning",
            "Human–Machine Interface",
            "Eco-driving behaviours"
        ],
        "number_of_pages": "15",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0226-z.pdf",
        "abstract": "A conscious use of the battery is one of the key elements to consider while driving an electric vehicle. Hence, supporting the drivers, with information about it, can be strategic in letting them drive in a better way, with the purpose of optimizing the energy consumption. In the context of electric vehicles, equipped with regenerative brakes, the driver’s braking style can make a significant difference. In this paper, we propose an approach which is based on the combination of big data and machine learning techniques, with the aim of enhancing the driver’s braking style through visual elements (displayed in the vehicle dashboard, as a Human–Machine Interface), actuat-ing eco-driving behaviours. We have designed and developed a system prototype, by exploiting big data coming from an electric vehicle and a machine learning algorithm. Then, we have conducted a set of tests, with simulated and real data, and here we discuss the results we have obtained that can open interesting discussions about the use of big data, together with machine learning, so as to improve drivers’ awareness of eco-behaviours."
    },
    {
        "url": "https://paperity.org/p/203333313/medicare-fraud-detection-using-neural-networks",
        "title": "Medicare fraud detection using neural networks",
        "authors": [
            "Justin M. Johnson",
            " Taghi M. Khoshgoftaar"
        ],
        "date_article": "07-2019",
        "short_description": "Access to affordable healthcare is a nationwide concern that impacts a large majority of the United States population. Medicare is a Federal Government healthcare program that provides affordable health insurance to the elderly population and individuals with select disabilities. Unfortunately, there is a significant amount of fraud, waste, and abuse within the Medicare system...",
        "keywords": [
            "Class imbalance",
            "Big data",
            "Thresholding",
            "Deep learning",
            "Medicare",
            "CMS",
            "LEIE",
            "Fraud detection"
        ],
        "number_of_pages": "35",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0225-0.pdf",
        "abstract": "Access to affordable healthcare is a nationwide concern that impacts a large majority of the United States population. Medicare is a Federal Government healthcare program that provides affordable health insurance to the elderly population and individuals with select disabilities. Unfortunately, there is a significant amount of fraud, waste, and abuse within the Medicare system that costs taxpayers billions of dollars and puts beneficiaries’ health and welfare at risk. Previous work has shown that publicly available Medicare claims data can be leveraged to construct machine learning models capable of automating fraud detection, but challenges associated with class-imbalanced big data hinder performance. With a minority class size of 0.03% and an opportunity to improve existing results, we use the Medicare fraud detection task to compare six deep learning methods designed to address the class imbalance problem. Data-level tech-niques used in this study include random over-sampling (ROS), random under-sam-pling (RUS), and a hybrid ROS–RUS. The algorithm-level techniques evaluated include a cost-sensitive loss function, the Focal Loss, and the Mean False Error Loss. A range of class ratios are tested by varying sample rates and desirable class-wise performance is achieved by identifying optimal decision thresholds for each model. Neural networks are evaluated on a 20% holdout test set, and results are reported using the area under the receiver operating characteristic curve (AUC). Results show that ROS and ROS–RUS perform significantly better than baseline and algorithm-level methods with aver-age AUC scores of 0.8505 and 0.8509, while ROS–RUS maximizes efficiency with a 4×speedup in training time. Plain RUS outperforms baseline methods with up to 30×improvements in training time, and all algorithm-level methods are found to produce more stable decision boundaries than baseline methods. Thresholding results suggest that the decision threshold always be optimized using a validation set, as we observe a strong linear relationship between the minority class size and the optimal threshold. To the best of our knowledge, this is the first study to compare multiple data-level and algorithm-level deep learning methods across a range of class distributions. Additional contributions include a unique analysis of the relationship between minority class size and optimal decision threshold and state-of-the-art performance on the given Medi-care fraud detection task."
    },
    {
        "url": "https://paperity.org/p/203362066/a-machine-learning-approach-to-analyze-customer-satisfaction-from-airline-tweets",
        "title": "A machine learning approach to analyze customer satisfaction from airline tweets",
        "authors": [
            "Sachin Kumar",
            " Mikhail Zymbler"
        ],
        "date_article": "07-2019",
        "short_description": "Customer’s experience is one of the important concern for airline industries. Twitter is one of the popular social media platform where flight travelers share their feedbacks in the form of tweets. This study presents a machine learning approach to analyze the tweets to improve the customer’s experience. Features were extracted from the tweets using word embedding with Glove...",
        "keywords": [
            "Twitter",
            "Machine learning",
            "Convolutional neural network",
            "Association analysis",
            "Customer satisfaction"
        ],
        "number_of_pages": "16",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0224-1.pdf",
        "abstract": "Customer’s experience is one of the important concern for airline industries. Twitter is one of the popular social media platform where flight travelers share their feedbacks in the form of tweets. This study presents a machine learning approach to analyze the tweets to improve the customer’s experience. Features were extracted from the tweets using word embedding with Glove dictionary approach and n-gram approach. Further, SVM (support vector machine) and several ANN (artificial neural network) architectures were considered to develop classification model that maps the tweet into positive and negative category. Additionally, convolutional neural network (CNN) were developed to classify the tweets and the results were compared with the most accurate model among SVM and several ANN architectures. It was found that CNN outperformed SVM and ANN models. In the end, association rule mining have been performed on different categories of tweets to map the relationship with sentiment categories. The results show that interesting associations were identified that certainly helps the airline indus-tries to improve their customer’s experience."
    },
    {
        "url": "https://paperity.org/p/203390819/resolving-intravoxel-white-matter-structures-in-the-human-brain-using-regularized",
        "title": "Resolving intravoxel white matter structures in the human brain using regularized regression and clustering",
        "authors": [
            "Andrea Hart",
            " Brianna Smith",
            " Sean Smith"
        ],
        "date_article": "07-2019",
        "short_description": "The human brain is a complex system of neural tissue that varies significantly between individuals. Although the technology that delineates these neural pathways does not currently exist, medical imaging modalities, such as diffusion magnetic resonance imaging (dMRI), can be leveraged for mathematical identification. The purpose of this work is to develop a novel method employing...",
        "keywords": [
            "Ball-and-stick model",
            "Diffusion MRI",
            "Tractography",
            "Nerve",
            "Neural tracts",
            "Brain imaging"
        ],
        "number_of_pages": "12",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0223-2.pdf",
        "abstract": "The human brain is a complex system of neural tissue that varies significantly between individuals. Although the technology that delineates these neural pathways does not currently exist, medical imaging modalities, such as diffusion magnetic resonance imaging (dMRI), can be leveraged for mathematical identification. The purpose of this work is to develop a novel method employing machine learning techniques to determine intravoxel nerve number and direction from dMRI data. The method was tested on multiple synthetic datasets and showed promising estimation accuracy and robustness for multi‑nerve systems under a variety of conditions, including highly noisy data and imprecision in parameter assumptions."
    },
    {
        "url": "https://paperity.org/p/203419572/a-survey-on-image-data-augmentation-for-deep-learning",
        "title": "A survey on Image Data Augmentation for Deep Learning",
        "authors": [
            "Connor Shorten",
            " Taghi M. Khoshgoftaar"
        ],
        "date_article": "07-2019",
        "short_description": "Deep convolutional neural networks have performed remarkably well on many Computer Vision tasks. However, these networks are heavily reliant on big data to avoid overfitting. Overfitting refers to the phenomenon when a network learns a function with very high variance such as to perfectly model the training data. Unfortunately, many application domains do not have access to big...",
        "keywords": [
            "Data Augmentation",
            "Big data",
            "Image data",
            "Deep Learning",
            "GANs"
        ],
        "number_of_pages": "48",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0197-0.pdf",
        "abstract": "Deep convolutional neural networks have performed remarkably well on many Computer Vision tasks. However, these networks are heavily reliant on big data to avoid overfitting. Overfitting refers to the phenomenon when a network learns a function with very high variance such as to perfectly model the training data. Unfor-tunately, many application domains do not have access to big data, such as medical image analysis. This survey focuses on Data Augmentation, a data-space solution to the problem of limited data. Data Augmentation encompasses a suite of techniques that enhance the size and quality of training datasets such that better Deep Learning models can be built using them. The image augmentation algorithms discussed in this survey include geometric transformations, color space augmentations, kernel filters, mixing images, random erasing, feature space augmentation, adversarial training, generative adversarial networks, neural style transfer, and meta-learning. The applica-tion of augmentation methods based on GANs are heavily covered in this survey. In addition to augmentation techniques, this paper will briefly discuss other character-istics of Data Augmentation such as test-time augmentation, resolution impact, final dataset size, and curriculum learning. This survey will present existing methods for Data Augmentation, promising developments, and meta-level decisions for implementing Data Augmentation. Readers will understand how Data Augmentation can improve the performance of their models and expand limited datasets to take advantage of the capabilities of big data."
    },
    {
        "url": "https://paperity.org/p/203186181/the-anatomy-of-the-data-driven-smart-sustainable-city-instrumentation-datafication",
        "title": "The anatomy of the data-driven smart sustainable city: instrumentation, datafication, computerization and related applications",
        "authors": [
            "Simon Elias Bibri"
        ],
        "date_article": "07-2019",
        "short_description": "We are moving into an era where instrumentation, datafication, and computerization are routinely pervading the very fabric of cities, coupled with the interlinking, integration, and coordination of their systems and domains. As a result, vast troves of data are generated and exploited to operate, manage, organize, and regulate urban life, or a deluge of contextual and actionable...",
        "keywords": [
            "Data-driven smart sustainable cities",
            "Data-driven smart sustainable urbanism",
            "Big data analytics",
            "Big data applications",
            "Datafication",
            "Urban science",
            "Urban sustainability",
            "Sustainable development",
            "Innovation labs",
            "Urban operation centers",
            "Urban intelligence functions"
        ],
        "number_of_pages": "43",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0221-4.pdf",
        "abstract": ""
    },
    {
        "url": "https://paperity.org/p/203214934/tree-stream-mining-algorithm-with-chernoff-bound-and-standard-deviation-approach-for-big",
        "title": "Tree stream mining algorithm with Chernoff-bound and standard deviation approach for big data stream",
        "authors": [
            "Ari Wibisono",
            " Devvi Sarwinda",
            " Petrus Mursanto"
        ],
        "date_article": "07-2019",
        "short_description": "We propose a Chernoff-bound approach and examine standard deviation value to enhance the accuracy of the existing fast incremental model tree with the drift detection (FIMT-DD) algorithm. It is a data stream mining algorithm that can observe and form a model tree from a large dataset. The standard FIMT-DD algorithm uses the Hoeffding bound for its splitting criterion. The use of...",
        "keywords": [
            "Big data",
            "Data stream",
            "Chernoff bound",
            "FIMT-DD",
            "Intelligent systems",
            "Standard deviation"
        ],
        "number_of_pages": "21",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0220-5.pdf",
        "abstract": ""
    },
    {
        "url": "https://paperity.org/p/203243687/effectiveness-analysis-of-machine-learning-classification-models-for-predicting",
        "title": "Effectiveness analysis of machine learning classification models for predicting personalized context-aware smartphone usage",
        "authors": [
            "Iqbal H. Sarker",
            " A. S. M. Kayes",
            " Paul Watters"
        ],
        "date_article": "07-2019",
        "short_description": "Due to the increasing popularity of recent advanced features and context-awareness in smart mobile phones, the contextual data relevant to users’ diverse activities with their phones are recorded through the device logs. Modeling and predicting individual’s smartphone usage based on contexts, such as temporal, spatial, or social information, can be used to build various context...",
        "keywords": [
            "Machine learning",
            "Mobile data mining",
            "User behavior modeling",
            "Smartphone analytics",
            "Classification",
            "Personalization",
            "Predictive analytics",
            "Context-aware computing",
            "IoT and mobile services",
            "Intelligent systems"
        ],
        "number_of_pages": "28",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0219-y.pdf",
        "abstract": ""
    },
    {
        "url": "https://paperity.org/p/198788290/feature-visualization-in-comic-artist-classification-using-deep-neural-networks",
        "title": "Feature visualization in comic artist classification using deep neural networks",
        "authors": [
            "Kim Young-Min"
        ],
        "date_article": "06-2019",
        "short_description": "Deep neural networks have become a standard framework for image analytics. Besides the traditional applications, such as object classification and detection, the latest studies have started to expand the scope of the applications to include artworks. However, popular art forms, such as comics, have been ignored in this trend. This study investigates visual features for comic...",
        "keywords": [
            "Artistic styles",
            "Comic classification",
            "Convolutional neural networks",
            "Deep neural networks",
            "Feature visualization"
        ],
        "number_of_pages": "18",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0222-3.pdf",
        "abstract": ""
    },
    {
        "url": "https://paperity.org/p/198817043/stvg-an-evolutionary-graph-framework-for-analyzing-fast-evolving-networks",
        "title": "STVG: an evolutionary graph framework for analyzing fast-evolving networks",
        "authors": [
            "Ikechukwu Maduako",
            " Monica Wachowicz",
            " Trevor Hanson"
        ],
        "date_article": "06-2019",
        "short_description": "Sequence of graph snapshots have been commonly utilized in literature to represent changes in a dynamic graph. This approach may be suitable for small-size and slowly evolving graphs; however, it is associated with high storage overhead in massive and fast-evolving graphs because of replication of the entire graph from one snapshot to another at shorter temporal resolutions. This...",
        "keywords": [
            "Space–Time varying graph",
            "Evolutionary graph analytics",
            "Fast-evolving networks",
            "Transit networks"
        ],
        "number_of_pages": "24",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0218-z.pdf",
        "abstract": ""
    },
    {
        "url": "https://paperity.org/p/198845796/big-data-in-healthcare-management-analysis-and-future-prospects",
        "title": "Big data in healthcare: management, analysis and future prospects",
        "authors": [
            "Sabyasachi Dash",
            " Sushil Kumar Shakyawar",
            " Mohit Sharm"
        ],
        "date_article": "06-2019",
        "short_description": "‘Big data’ is massive amounts of information that can work wonders. It has become a topic of special interest for the past two decades because of a great potential that is hidden in it. Various public and private sector industries generate, store, and analyze big data with an aim to improve the services they provide. In the healthcare industry, various sources for big data...",
        "keywords": [
            "Healthcare",
            "Biomedical research",
            "Big data analytics",
            "Internet of things",
            "Personalized medicine",
            "Quantum computing"
        ],
        "number_of_pages": "25",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0217-0.pdf",
        "abstract": ""
    },
    {
        "url": "https://paperity.org/p/198874549/gnss-based-navigation-systems-of-autonomous-drone-for-delivering-items",
        "title": "GNSS-based navigation systems of autonomous drone for delivering items",
        "authors": [
            "Aurello Patrik",
            " Gaudi Utama",
            " Alexander Agung Santoso Gunawan"
        ],
        "date_article": "06-2019",
        "short_description": "This paper presents our research on the development of navigation systems of autonomous drone for delivering items that uses a GNSS (Global Navigation Satellite System) and a compass as the main tools in drone. The grand purpose of our research is to deliver important medical aids for patients in emergency situations and implementation in agriculture in Indonesia, as part of the...",
        "keywords": [
            "Drone",
            "GNSS",
            "Navigation systems",
            "Big data"
        ],
        "number_of_pages": "14",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0214-3.pdf",
        "abstract": ""
    },
    {
        "url": "https://paperity.org/p/198903302/evaluation-of-distributed-stream-processing-frameworks-for-iot-applications-in-smart",
        "title": "Evaluation of distributed stream processing frameworks for IoT applications in Smart Cities",
        "authors": [
            "Hamid Nasiri",
            " Saeed Nasehi",
            " Maziar Goudarzi"
        ],
        "date_article": "06-2019",
        "short_description": "The widespread growth of Big Data and the evolution of Internet of Things (IoT) technologies enable cities to obtain valuable intelligence from a large amount of real-time produced data. In a Smart City, various IoT devices generate streams of data continuously which need to be analyzed within a short period of time; using some Big Data technique. Distributed stream processing...",
        "keywords": [
            "Distributed stream processing",
            "Smart City",
            "IoT applications",
            "Latency",
            "Throughput"
        ],
        "number_of_pages": "24",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0215-2.pdf",
        "abstract": ""
    },
    {
        "url": "https://paperity.org/p/198960808/cyber-risk-prediction-through-social-media-big-data-analytics-and-statistical-machine",
        "title": "Cyber risk prediction through social media big data analytics and statistical machine learning",
        "authors": [
            "Athor Subroto",
            " Andri Apriyan"
        ],
        "date_article": "06-2019",
        "short_description": "As a natural outcome of achieving equilibrium, digital economic progress will most likely be subject to increased cyber risks. Therefore, the purpose of this study is to present an algorithmic model that utilizes social media big data analytics and statistical machine learning to predict cyber risks. The data for this study consisted of 83,015 instances from the common...",
        "keywords": [
            "Predictive analytics",
            "Machine learning",
            "Big data",
            "Cyber risks",
            "Social media",
            "Non-traditional actuary"
        ],
        "number_of_pages": "19",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0216-1.pdf",
        "abstract": "As a natural outcome of achieving equilibrium, digital economic progress will most likely be subject to increased cyber risks. Therefore, the purpose of this study is to present an algorithmic model that utilizes social media big data analytics and statisti-cal machine learning to predict cyber risks. The data for this study consisted of 83,015 instances from the common vulnerabilities and exposures (CVE) database (early 1999 to March 2017) and 25,599 cases of cyber risks from Twitter (early 2016 to March 2017), after which 1000 instances from both platforms were selected. The predictions were made by analyzing the software vulnerabilities to threats, based on social media conversations, while prediction accuracy was measured by comparing the cyber risk data from Twitter with that from the CVE database. Utilizing confusion matrix, we can achieve the best prediction by using Rweka package to carry out machine learning (ML) experimentation and artificial neural network (ANN) with the accuracy rate of 96.73%. Thus, in this paper, we offer new insights into cyber risks and how such vulner-abilities can be adequately understood and predicted. The findings of this study can be used by managers of public and private companies to formulate effective strategies for reducing cyber risks to critical infrastructures."
    },
    {
        "url": "https://paperity.org/p/198932055/ally-patches-for-spoliation-of-adversarial-patches",
        "title": "Ally patches for spoliation of adversarial patches",
        "authors": [
            "Alaa E. Abdel-Hakim"
        ],
        "date_article": "06-2019",
        "short_description": "Adversarial attacks represent a serious evolving threat to the operation of deep neural networks. Recently, adversarial algorithms were developed to facilitate hallucination of deep neural networks for ordinary attackers. State-of-the-arts algorithms could generate offline printable adversarial patches that can be interspersed within fields of view of the capturing cameras in an...",
        "keywords": [
            "Adversarial patches",
            "Ally patches",
            "CNN",
            "Deep neural networks"
        ],
        "number_of_pages": "14",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0213-4.pdf",
        "abstract": "Adversarial attacks represent a serious evolving threat to the operation of deep neural networks. Recently, adversarial algorithms were developed to facilitate hallucination of deep neural networks for ordinary attackers. State‑of‑the‑arts algorithms could gener‑ate offline printable adversarial patches that can be interspersed within fields of view of the capturing cameras in an innocently unnoticeable action. In this paper, we propose an algorithm to ravage the operation of these adversarial patches. The proposed algorithm uses intrinsic information contents of the input image to extract a set of ally patches. The extracted patches break the salience of the attacking adversarial patch to the network. To our knowledge, this is the first time to address the defense problem against such kinds of adversarial attacks by counter‑processing the input image in order to ravage the effect of any possible adversarial patches. The classification decision is taken according to a late‑fusion strategy applied to the independent classifications generated by the extracted patch alliance. Evaluation experiments were conducted on the 1000 classes of the ILSVRC benchmark. Different convolutional neural network models and varying‑scale adversarial patches were used in the experimentation. Evalu‑ation results showed the effectiveness of the proposed ally patches in reducing the success rates of adversarial patches."
    },
    {
        "url": "https://paperity.org/p/198813676/recencyminer-mining-recency-based-personalized-behavior-from-contextual-smartphone-data",
        "title": "RecencyMiner: mining recency-based personalized behavior from contextual smartphone data",
        "authors": [
            "Iqbal H. Sarker",
            " Alan Colman",
            " Jun Han"
        ],
        "date_article": "06-2019",
        "short_description": "Due to the advanced features in recent smartphones and context-awareness in mobile technologies, users’ diverse behavioral activities with their phones and associated contexts are recorded through the device logs. Behavioral patterns of smartphone users may vary greatly between individuals in different contexts—for example, temporal, spatial, or social contexts. However, an...",
        "keywords": [
            "Mobile data mining",
            "Machine learning",
            "Association rule learning",
            "Data science",
            "User behavior modeling",
            "Context-awareness",
            "Recency",
            "Personalization",
            "Predictive modeling",
            "IoT analytics",
            "Smartphone",
            "Intelligent service"
        ],
        "number_of_pages": "21",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0211-6.pdf",
        "abstract": "Due to the advanced features in recent smartphones and context-awareness in mobile technologies, users’ diverse behavioral activities with their phones and associated con-texts are recorded through the device logs. Behavioral patterns of smartphone users may vary greatly between individuals in different contexts—for example, temporal, spatial, or social contexts. However, an individual’s phone usage behavior may not be static in the real-world changing over time. The volatility of usage behavior will also vary from user-to-user. Thus, an individual’s recent behavioral patterns and correspond-ing machine learning rules are more likely to be interesting and significant than older ones for modeling and predicting their phone usage behavior. Based on this concept of recency, in this paper, we present an approach for mining recency-based personalized behavior, and name it “RecencyMiner” for short, utilizing individual’s contextual smart-phone data, in order to build a context-aware personalized behavior prediction model. The effectiveness of RecencyMiner is examined by considering individual smartphone user’s real-life contextual datasets. The experimental results show that our proposed recency-based approach better predicts individual’s phone usage behavior than exist-ing baseline models, by minimizing the error rate in various context-aware test cases."
    },
    {
        "url": "https://paperity.org/p/198784923/scalable-architecture-for-big-data-financial-analytics-user-defined-functions-vs-sql",
        "title": "Scalable architecture for Big Data financial analytics: user-defined functions vs. SQL",
        "authors": [
            "Kurt Stockinger",
            " Nils Bundi",
            " Jonas Heitz"
        ],
        "date_article": "06-2019",
        "short_description": "Large financial organizations have hundreds of millions of financial contracts on their balance sheets. Moreover, highly volatile financial markets and heterogeneous data sets within and across banks world-wide make near real-time financial analytics very challenging and their handling thus requires cutting edge financial algorithms. However, due to a lack of data modeling...",
        "keywords": [
            "Financial analytics",
            "Query processing",
            "User-defined functions",
            "Performance evaluation"
        ],
        "number_of_pages": "24",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0209-0.pdf",
        "abstract": "Large financial organizations have hundreds of millions of financial contracts on their balance sheets. Moreover, highly volatile financial markets and heterogeneous data sets within and across banks world-wide make near real-time financial analytics very challenging and their handling thus requires cutting edge financial algorithms. However, due to a lack of data modeling standards, current financial risk algorithms are typically inconsistent and non-scalable. In this paper, we present a novel implementa-tion of a real-world use case for performing large-scale financial analytics leveraging Big Data technology. We first provide detailed background information on the financial underpinnings of our framework along with the major financial calculations. After-wards we analyze the performance of different parallel implementations in Apache Spark based on existing computation kernels that apply the ACTUS data and algorith-mic standard for financial contract modeling. The major contribution is a detailed dis-cussion of the design trade-offs between applying user-defined functions on existing computation kernels vs. partially re-writing the kernel in SQL and thus taking advan-tage of the underlying SQL query optimizer. Our performance evaluation demonstrates almost linear scalability for the best design choice."
    },
    {
        "url": "https://paperity.org/p/198756170/big-data-stream-analysis-a-systematic-literature-review",
        "title": "Big data stream analysis: a systematic literature review",
        "authors": [
            "Taiwo Kolajo",
            " Olawande Daramola",
            " Ayodele Adebiyi"
        ],
        "date_article": "06-2019",
        "short_description": "Recently, big data streams have become ubiquitous due to the fact that a number of applications generate a huge amount of data at a great velocity. This made it difficult for existing data mining tools, technologies, methods, and techniques to be applied directly on big data streams due to the inherent dynamic characteristics of big data. In this paper, a systematic review of big...",
        "keywords": [
            "Big data stream analysis",
            "Stream computing",
            "Big data streaming tools and technologies"
        ],
        "number_of_pages": "30",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0210-7.pdf",
        "abstract": "Recently, big data streams have become ubiquitous due to the fact that a number of applications generate a huge amount of data at a great velocity. This made it difficult for existing data mining tools, technologies, methods, and techniques to be applied directly on big data streams due to the inherent dynamic characteristics of big data. In this paper, a systematic review of big data streams analysis which employed a rigorous and methodical approach to look at the trends of big data stream tools and technolo-gies as well as methods and techniques employed in analysing big data streams. It provides a global view of big data stream tools and technologies and its comparisons. Three major databases, Scopus, ScienceDirect and EBSCO, which indexes journals and conferences that are promoted by entities such as IEEE, ACM, SpringerLink, and Elsevier were explored as data sources. Out of the initial 2295 papers that resulted from the first search string, 47 papers were found to be relevant to our research questions after implementing the inclusion and exclusion criteria. The study found that scalability, privacy and load balancing issues as well as empirical analysis of big data streams and technologies are still open for further research efforts. We also found that although, sig-nificant research efforts have been directed to real-time analysis of big data stream not much attention has been given to the preprocessing stage of big data streams. Only a few big data streaming tools and technologies can do all of the batch, streaming, and iterative jobs; there seems to be no big data tool and technology that offers all the key features required for now and standard benchmark dataset for big data streaming ana-lytics has not been widely adopted. In conclusion, it was recommended that research efforts should be geared towards developing scalable frameworks and algorithms that will accommodate data stream computing mode, effective resource allocation strategy and parallelization issues to cope with the ever-growing size and complexity of data."
    },
    {
        "url": "https://paperity.org/p/198727417/intelligent-video-surveillance-a-review-through-deep-learning-techniques-for-crowd",
        "title": "Intelligent video surveillance: a review through deep learning techniques for crowd analysis",
        "authors": [
            "G. Sreenu",
            " M. A. Saleem Durai"
        ],
        "date_article": "06-2019",
        "short_description": "Big data applications are consuming most of the space in industry and research area. Among the widespread examples of big data, the role of video streams from CCTV cameras is equally important as other sources like social media data, sensor data, agriculture data, medical data and data evolved from space research. Surveillance videos have a major contribution in unstructured big...",
        "keywords": [
            "Big data",
            "Video surveillance",
            "Deep learning",
            "Crowd analysis"
        ],
        "number_of_pages": "27",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0212-5.pdf",
        "abstract": "Big data applications are consuming most of the space in industry and research area. Among the widespread examples of big data, the role of video streams from CCTV cameras is equally important as other sources like social media data, sensor data, agri-culture data, medical data and data evolved from space research. Surveillance videos have a major contribution in unstructured big data. CCTV cameras are implemented in all places where security having much importance. Manual surveillance seems tedious and time consuming. Security can be defined in different terms in different contexts like theft identification, violence detection, chances of explosion etc. In crowded public places the term security covers almost all type of abnormal events. Among them violence detection is difficult to handle since it involves group activity. The anomalous or abnormal activity analysis in a crowd video scene is very difficult due to several real world constraints. The paper includes a deep rooted survey which starts from object recognition, action recognition, crowd analysis and finally violence detection in a crowd environment. Majority of the papers reviewed in this survey are based on deep learning technique. Various deep learning methods are compared in terms of their algorithms and models. The main focus of this survey is application of deep learning techniques in detecting the exact count, involved persons and the happened activity in a large crowd at all climate conditions. Paper discusses the underlying deep learning implementation technology involved in various crowd video analysis methods. Real time processing, an important issue which is yet to be explored more in this field is also considered. Not many methods are there in handling all these issues simultaneously. The issues recognized in existing methods are identified and summarized. Also future direction is given to reduce the obstacles identified. The survey provides a biblio-graphic summary of papers from ScienceDirect, IEEE Xplore and ACM digital library."
    },
    {
        "url": "https://paperity.org/p/198871182/uncertainty-in-big-data-analytics-survey-opportunities-and-challenges",
        "title": "Uncertainty in big data analytics: survey, opportunities, and challenges",
        "authors": [
            "Reihaneh H. Hariri",
            " Erik M. Fredericks",
            " Kate M. Bowers"
        ],
        "date_article": "06-2019",
        "short_description": "Big data analytics has gained wide attention from both academia and industry as the demand for understanding trends in massive datasets increases. Recent developments in sensor networks, cyber-physical systems, and the ubiquity of the Internet of Things (IoT) have increased the collection of data (including health care, social media, smart cities, agriculture, finance, education...",
        "keywords": [
            "Big data",
            "Uncertainty",
            "Big data analytics",
            "Artificial intelligence"
        ],
        "number_of_pages": "16",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0206-3.pdf",
        "abstract": "Big data analytics has gained wide attention from both academia and industry as the demand for understanding trends in massive datasets increases. Recent developments in sensor networks, cyber-physical systems, and the ubiquity of the Internet of Things (IoT ) have increased the collection of data (including health care, social media, smart cities, agriculture, finance, education, and more) to an enormous scale. However, the data collected from sensors, social media, financial records, etc. is inherently uncer-tain due to noise, incompleteness, and inconsistency. The analysis of such massive amounts of data requires advanced analytical techniques for efficiently reviewing and/or predicting future courses of action with high precision and advanced decision-making strategies. As the amount, variety, and speed of data increases, so too does the uncertainty inherent within, leading to a lack of confidence in the resulting analytics process and decisions made thereof. In comparison to traditional data techniques and platforms, artificial intelligence techniques (including machine learning, natural lan-guage processing, and computational intelligence) provide more accurate, faster, and scalable results in big data analytics. Previous research and surveys conducted on big data analytics tend to focus on one or two techniques or specific application domains. However, little work has been done in the field of uncertainty when applied to big data analytics as well as in the artificial intelligence techniques applied to the datasets. This article reviews previous work in big data analytics and presents a discussion of open challenges and future directions for recognizing and mitigating uncertainty in this domain."
    },
    {
        "url": "https://paperity.org/p/198842429/exploring-and-cleaning-big-data-with-random-sample-data-blocks",
        "title": "Exploring and cleaning big data with random sample data blocks",
        "authors": [
            "Salman Salloum",
            " Joshua Zhexue Huang",
            " Yulin H"
        ],
        "date_article": "06-2019",
        "short_description": "Data scientists need scalable methods to explore and clean big data before applying advanced data analysis and mining algorithms. In this paper, we propose the RSP-Explore method to enable data scientists to iteratively explore big data on small computing clusters. We address three main tasks: statistical estimation, error detection, and data cleaning. The Random Sample Partition...",
        "keywords": [
            "Big data",
            "Exploratory data analysis",
            "Statistical estimation",
            "Data cleaning",
            "Block-level sampling",
            "Random sample partition",
            "Distributed",
            "Parallel and cluster computing"
        ],
        "number_of_pages": "28",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0205-4.pdf",
        "abstract": "Data scientists need scalable methods to explore and clean big data before apply-ing advanced data analysis and mining algorithms. In this paper, we propose the RSP-Explore method to enable data scientists to iteratively explore big data on small computing clusters. We address three main tasks: statistical estimation, error detection, and data cleaning. The Random Sample Partition (RSP) distributed data model is used to represent the data as a set of ready-to-use random sample data blocks (called RSP blocks) of the entire data. Block-level samples of RSP blocks are selected to understand the data, identify potential types of value errors, and get samples of clean data. We provide a theoretical analysis on using RSP blocks for statistical estimation and demon-strate empirically the advantages of the RSP-Explore method. The experimental results of three real data sets show that the approximate results from RSP-Explore can rapidly converge toward the true values. Furthermore, cleaning a sample of RSP blocks is suf-ficient to estimate the statistical properties of the unknown clean data."
    },
    {
        "url": "https://paperity.org/p/198899935/dencast-distributed-density-based-clustering-for-multi-target-regression",
        "title": "DENCAST: distributed density-based clustering for multi-target regression",
        "authors": [
            "Roberto Corizzo",
            " Gianvito Pio",
            " Michelangelo Ceci"
        ],
        "date_article": "06-2019",
        "short_description": "Recent developments in sensor networks and mobile computing led to a huge increase in data generated that need to be processed and analyzed efficiently. In this context, many distributed data mining algorithms have recently been proposed. Following this line of research, we propose the DENCAST system, a novel distributed algorithm implemented in Apache Spark, which performs...",
        "keywords": [
            "Distributed clustering",
            "Multi-target regression",
            "Apache Spark"
        ],
        "number_of_pages": "27",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0207-2.pdf",
        "abstract": "Recent developments in sensor networks and mobile computing led to a huge increase in data generated that need to be processed and analyzed efficiently. In this context, many distributed data mining algorithms have recently been proposed. Following this line of research, we propose the DENCAST system, a novel distributed algorithm implemented in Apache Spark, which performs density-based clustering and exploits the identified clusters to solve both single- and multi-target regres-sion tasks (and thus, solves complex tasks such as time series prediction). Contrary to existing distributed methods, DENCAST does not require a final merging step (usually performed on a single machine) and is able to handle large-scale, high-dimensional data by taking advantage of locality sensitive hashing. Experiments show that DEN-CAST performs clustering more efficiently than a state-of-the-art distributed clustering algorithm, especially when the number of objects increases significantly. The quality of the extracted clusters is confirmed by the predictive capabilities of DENCAST on several datasets: It is able to significantly outperform (p-value <0.05 ) state-of-the-art distrib-uted regression methods, in both single and multi-target settings."
    },
    {
        "url": "https://paperity.org/p/198928688/oscillation-of-tweet-sentiments-in-the-election-of-joao-doria-jr-for-mayor",
        "title": "Oscillation of tweet sentiments in the election of João Doria Jr. for Mayor",
        "authors": [
            "Rubens Mussi Cury"
        ],
        "date_article": "06-2019",
        "short_description": "The purpose of this work is to identify and analyze the oscillation of sentiments expressed by users of the Twitter social media through their direct replies to posts by user @jdoriajr that took place before, during and after the elections for mayor of the city of São Paulo in the year 2016. In order to make this research possible, we used Python 3.6.4 and the Searchtweets 1.6.1...",
        "keywords": [
            "Sentiment analysis",
            "Twitter",
            "João Doria Jr.",
            "Data mining",
            "Big data in politics"
        ],
        "number_of_pages": "15",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0208-1.pdf",
        "abstract": "The purpose of this work is to identify and analyze the oscillation of sentiments expressed by users of the Twitter social media through their direct replies to posts by user @jdoriajr that took place before, during and after the elections for mayor of the city of São Paulo in the year 2016. In order to make this research possible, we used Python 3.6.4 and the Searchtweets 1.6.1 library for consumption of the API Search Twit-ter, from which it was possible to extract 76,690 tweets. Text sentiment analysis was carried out through the Lexicon-Based Approach method and the Laplacian Smooth-ing calculation algorithm-which generated a rate that would represent a negative and a positive sentiment ranging from −0.1306 (minimum) to 0.1489 (maximum) respectively, throughout the observed period. As additional tools, WordCloud and t-SNE (t-Distributed Stochastic Neighbor Embedding) Corpus Visualization were used for visualization of the word cloud and cluster, respectively, with both functionalities available at the Yellowbrick 0.8 package also for Python."
    },
    {
        "url": "https://paperity.org/p/193390285/diftong-a-tool-for-validating-big-data-workflows",
        "title": "Diftong: a tool for validating big data workflows",
        "authors": [
            "Raya Rizk",
            " Steve McKeever",
            " Johan Petrini"
        ],
        "date_article": "05-2019",
        "short_description": "Data validation is about verifying the correctness of data. When organisations update and refine their data transformations to meet evolving requirements, it is imperative to ensure that the new version of a workflow still produces the correct output. We motivate the need for workflows and describe the implementation of a validation tool called Diftong. This tool compares two...",
        "keywords": [
            "Big data",
            "Data testing",
            "Data validation",
            "Data quality",
            "Big data validation process",
            "Big data validation tool",
            "Big data workflow"
        ],
        "number_of_pages": "27",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0204-5.pdf",
        "abstract": "Data validation is about verifying the correctness of data. When organisations update and refine their data transformations to meet evolving requirements, it is imperative to ensure that the new version of a workflow still produces the correct output. We motivate the need for workflows and describe the implementation of a validation tool called Diftong. This tool compares two tabular databases resulting from different ver-sions of a workflow to detect and prevent potential unwanted alterations. Row-based and column-based statistics are used to quantify the results of the database compari-son. Diftong was shown to provide accurate results in test scenarios, bringing benefits to companies that need to validate the outputs of their workflows. By automating this process, the risk of human error is also eliminated. Compared to the more labour-intensive manual alternative, it has the added benefit of improved turnaround time for the validation process. Together this allows for a more agile way of updating data transformation workflows."
    },
    {
        "url": "https://paperity.org/p/193361532/verifying-big-data-topologies-by-design-a-semi-automated-approach",
        "title": "Verifying big data topologies by-design: a semi-automated approach",
        "authors": [
            "Marcello M. Bersani",
            " Francesco Marconi",
            " Damian A. Tamburri"
        ],
        "date_article": "05-2019",
        "short_description": "Big data architectures have been gaining momentum in recent years. For instance, Twitter uses stream processing frameworks like Apache Storm to analyse billions of tweets per minute and learn the trending topics. However, architectures that process big data involve many different components interconnected via semantically different connectors. Such complex architectures make...",
        "keywords": [
            "Big data architectures",
            "Software design and analysis",
            "Big data systems verification"
        ],
        "number_of_pages": "23",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0199-y.pdf",
        "abstract": "Big data architectures have been gaining momentum in recent years. For instance, Twitter uses stream processing frameworks like Apache Storm to analyse billions of tweets per minute and learn the trending topics. However, architectures that process big data involve many different components interconnected via semantically different connectors. Such complex architectures make possible refactoring of the applications a difficult task for software architects, as applications might be very different with respect to the initial designs. As an aid to designers and developers, we developed OSTIA (Ordinary Static Topology Inference Analysis) that allows detecting the occur-rence of common anti-patterns across big data architectures and exploiting software verification techniques on the elicited architectural models. This paper illustrates OSTIA and evaluates its uses and benefits on three industrial-scale case-studies."
    },
    {
        "url": "https://paperity.org/p/193214400/detecting-taxi-movements-using-random-swap-clustering-and-sequential-pattern-mining",
        "title": "Detecting taxi movements using Random Swap clustering and sequential pattern mining",
        "authors": [
            "Rami Ibrahim",
            " M. Omair Shafiq"
        ],
        "date_article": "05-2019",
        "short_description": "Moving objects such as people, animals, and vehicles have generated a large amount of spatiotemporal data by using location-capture technologies and mobile devices. This collected data needs to be processed, visualized and analyzed to transform raw trajectory data into useful knowledge. In this study, we build a system to deliver a set of traffic insights and recommendations by...",
        "keywords": [
            "Random Swap",
            "HDBSCAN",
            "Sequential pattern mining"
        ],
        "number_of_pages": "26",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0203-6.pdf",
        "abstract": "Moving objects such as people, animals, and vehicles have generated a large amount of spatiotemporal data by using location-capture technologies and mobile devices. This collected data needs to be processed, visualized and analyzed to transform raw trajectory data into useful knowledge. In this study, we build a system to deliver a set of traffic insights and recommendations by applying two techniques, clustering, and sequential pattern mining. This system has three stages, the first stage preprocesses and samples the dataset into 168 subsets, the second stage applies two clustering techniques, the hierarchical density-based spatial clustering (HDBSCAN) and the Random Swap clustering (RS). We compare these two clustering algorithms in terms of processing time and quality of clusters. In the comparative analysis, the Silhouette coefficient shows that RS clustering outperforms HDBSCAN in terms of clusters quality. Moreover, the analysis shows that RS outperforms K-means in terms of the mean of square error (MSE) reduction. After that, we use a Google Maps approach to label the traffic districts and apply sequential pattern mining to extract taxi trips flow. The system can detect 146 sequential patterns in different areas of the city. In the last stage, we visualize traffic clusters generated from the RS algorithm. Furthermore, we visualize the taxi trips heatmap per weekday and hour of the day in Porto city. This system can be integrated with the current traffic control applications to provide useful guidelines for taxi drivers, passengers, and transportation authorities."
    },
    {
        "url": "https://paperity.org/p/193447791/fast-implementation-of-pattern-mining-algorithms-with-time-stamp-uncertainties-and",
        "title": "Fast implementation of pattern mining algorithms with time stamp uncertainties and temporal constraints",
        "authors": [
            "Sofya S. Titarenko",
            " Valeriy N. Titarenko",
            " Georgios Aivaliotis"
        ],
        "date_article": "05-2019",
        "short_description": "Pattern mining is a powerful tool for analysing big datasets. Temporal datasets include time as an additional parameter. This leads to complexity in algorithmic formulation, and it can be challenging to process such data quickly and efficiently. In addition, errors or uncertainty can exist in the timestamps of data, for example in manually recorded health data. Sometimes we wish...",
        "keywords": [
            "Pattern mining",
            "Temporal data",
            "Uncertainty",
            "Optimisation",
            "OpenMP"
        ],
        "number_of_pages": "34",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0200-9.pdf",
        "abstract": "Pattern mining is a powerful tool for analysing big datasets. Temporal datasets include time as an additional parameter. This leads to complexity in algorithmic formulation, and it can be challenging to process such data quickly and efficiently. In addition, errors or uncertainty can exist in the timestamps of data, for example in manually recorded health data. Sometimes we wish to find patterns only within a certain temporal range. In some cases real-time processing and decision-making may be desirable. All these issues increase algorithmic complexity, processing times and storage requirements. In addition, it may not be possible to store or process confidential data on public clusters or the cloud that can be accessed by many people. Hence it is desirable to optimise algorithms for standalone systems. In this paper we present an integrated approach which can be used to write efficient codes for pattern mining problems. The approach includes: (1) cleaning datasets with removal of infrequent events, (2) presenting a new scheme for time-series data storage, (3) exploiting the presence of prior information about a dataset when available, (4) utilising vectorisation and multicore parallelisation. We present two new algorithms, FARPAM (FAst Robust PAttern Mining) and FARPAMp (FARPAM with prior information about prior uncertainty, allowing faster searching). The algorithms are applicable to a wide range of temporal datasets. They implement a new formulation of the pattern searching function which reproduces and extends existing algorithms (such as SPAM and RobustSPAM), and allows for significantly faster calcula-tion. The algorithms also include an option of temporal restrictions in patterns, which is available neither in SPAM nor in RobustSPAM. The searching algorithm is designed to be flexible for further possible extensions. The algorithms are coded in C++, and are highly optimised and parallelised for a modern standalone multicore workstation, thus avoiding security issues connected with transfers of confidential data onto clusters. FARPAM has been successfully tested on a publicly available weather dataset and on a confidential adult social care dataset, reproducing results obtained by previous algo-rithms in both cases. It has been profiled against the widely used SPAM algorithm (for sequential pattern mining) and RobustSPAM (developed for datasets with errors in time points). The algorithm outperforms SPAM by up to 20 times and RobustSPAM by up to 6000 times. In both cases the new algorithm has better scalability."
    },
    {
        "url": "https://paperity.org/p/193419038/the-intrinsically-linked-future-for-human-and-artificial-intelligence-interaction",
        "title": "The intrinsically linked future for human and Artificial Intelligence interaction",
        "authors": [
            "Anthony Miller"
        ],
        "date_article": "05-2019",
        "short_description": "No sleep! No sick days! No Holidays! alongside 24/7 around the clock tireless work, welcome to the world of Artificial Intelligence, how can humans match the phenomena of AI. Humanity is in the grip of an unstoppable, and exciting future with the future development of AI, humans must decide whether to ride on the coat-tails of AI or resist the inevitable change of the world as we...",
        "keywords": [
            "Quantum Computing",
            "Artificial Intelligence",
            "Big Data",
            "Human and AI interaction"
        ],
        "number_of_pages": "9",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0202-7.pdf",
        "abstract": "No sleep! No sick days! No Holidays! alongside 24/7 around the clock tireless work, welcome to the world of Artificial Intelligence, how can humans match the phenom-ena of AI. Humanity is in the grip of an unstoppable, and exciting future with the future development of AI, humans must decide whether to ride on the coat-tails of AI or resist the inevitable change of the world as we know it. The real game changer will be the commercial availability of Quantum Computing. Humans must learn to work, live, learn and interact with AI or become second class citizens. Big data is the lifeblood of AI and Quantum Computing processing power will enable future computers to process incredible amounts of big data. AI is taking over the job’s human do, receptionists, driv-ers, chefs and in the future doctors and accountants. Quantum Supremacy will soon be achieved, and AI will soon reach and far exceed the “Singularity”. Will humans grasp the opportunity to develop with AI or resist? humans have the chance to develop with AI through brain computer interface (BCI) technology. Security and regulations have to be put in place, with questions of “Who is responsible for AI security and regula-tions”? and “can AI be trusted as an autonomous entity” also the ethical use of AI has to be addressed, “What about the rights and ethics of AI”?. The human race is on an inevitable path of AI dominance the question is “will humans and AI be friends or adversaries”?"
    },
    {
        "url": "https://paperity.org/p/193243153/library-adoption-in-public-software-repositories",
        "title": "Library adoption in public software repositories",
        "authors": [
            "Rachel Krohn",
            " Tim Weninger"
        ],
        "date_article": "05-2019",
        "short_description": "We study the the spread and adoption of libraries within Python projects hosted in public software repositories on GitHub. By modelling the use of Git pull, merge, commit, and other actions as deliberate cognitive activities, we are able to better understand the dynamics of what happens when users adopt new and cognitively demanding information. For this task we introduce a large...",
        "keywords": [
            "Information adoption",
            "Software libraries",
            "GitHub",
            "Python",
            "StackOverflow",
            "Classification",
            "SVM",
            "Modelling",
            "Git",
            "Repository",
            "Commit",
            "Software development",
            "Cognitive science",
            "Text mining"
        ],
        "number_of_pages": "19",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0201-8.pdf",
        "abstract": "We study the the spread and adoption of libraries within Python projects hosted in public software repositories on GitHub. By modelling the use of Git pull, merge, com-mit, and other actions as deliberate cognitive activities, we are able to better under-stand the dynamics of what happens when users adopt new and cognitively demand-ing information. For this task we introduce a large corpus containing all commits, diffs, messages, and source code from 259,690 Python repositories (about 13% of all Python projects on Github), including all Git activity data from 89,311 contributing users. In this initial work we ask two primary questions: (1) What kind of behavior change occurs near an adoption event? (2) Can we model future adoption activity of a user? Using a fine-grained analysis of user behavior, we show that library adoptions are followed by higher than normal activity within the first 6 h, implying that a higher than normal cog-nitive effort is involved with an adoption. Further study is needed to understand the specific types of events that surround the adoption of new information, and the cause of these dynamics. We also show that a simple linear model is capable of classifying future commits as being an adoption or not, based on the commit contents and the preceding history of the user and repository. Additional work in this vein may be able to predict the content of future commits, or suggest new libraries to users."
    },
    {
        "url": "https://paperity.org/p/193300659/evaluating-partitioning-and-bucketing-strategies-for-hive-based-big-data-warehousing",
        "title": "Evaluating partitioning and bucketing strategies for Hive-based Big Data Warehousing systems",
        "authors": [
            "Eduarda Costa",
            " Carlos Costa",
            " Maribel Yasmina Santos"
        ],
        "date_article": "05-2019",
        "short_description": "Hive has long been one of the industry-leading systems for Data Warehousing in Big Data contexts, mainly organizing data into databases, tables, partitions and buckets, stored on top of an unstructured distributed file system like HDFS. Some studies were conducted for understanding the ways of optimizing the performance of several storage systems for Big Data Warehousing. However...",
        "keywords": [
            "Big Data",
            "Big Data Warehouse",
            "Hive",
            "Partitions",
            "Buckets"
        ],
        "number_of_pages": "38",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0196-1.pdf",
        "abstract": "Hive has long been one of the industry-leading systems for Data Warehousing in Big Data contexts, mainly organizing data into databases, tables, partitions and buckets, stored on top of an unstructured distributed file system like HDFS. Some studies were conducted for understanding the ways of optimizing the performance of several stor-age systems for Big Data Warehousing. However, few of them explore the impact of data organization strategies on query performance, when using Hive as the storage technology for implementing Big Data Warehousing systems. Therefore, this paper evaluates the impact of data partitioning and bucketing in Hive-based systems, testing different data organization strategies and verifying the efficiency of those strategies in query performance. The obtained results demonstrate the advantages of implement-ing Big Data Warehouses based on denormalized models and the potential benefit of using adequate partitioning strategies. Defining the partitions aligned with the attrib-utes that are frequently used in the conditions/filters of the queries can significantly increase the efficiency of the system in terms of response time. In the more intensive workload benchmarked in this paper, overall decreases of about 40% in processing time were verified. The same is not verified with the use of bucketing strategies, which shows potential benefits in very specific scenarios, suggesting a more restricted use of this functionality, namely in the context of bucketing two tables by the join attribute of these tables."
    },
    {
        "url": "https://paperity.org/p/193271906/exploring-the-applicability-of-low-shot-learning-in-mining-software-repositories",
        "title": "Exploring the applicability of low-shot learning in mining software repositories",
        "authors": [
            "Jordan Ott",
            " Abigail Atchison",
            " Erik J. Linstead"
        ],
        "date_article": "05-2019",
        "short_description": "BackgroundDespite the well-documented and numerous recent successes of deep learning, the application of standard deep architectures to many classification problems within empirical software engineering remains problematic due to the large volumes of labeled data required for training. Here we make the argument that, for some problems, this hurdle can be overcome by taking...",
        "keywords": [
            "Deep learning",
            "Low-shot learning",
            "UML"
        ],
        "number_of_pages": "10",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0198-z.pdf",
        "abstract": "Background:Despite the well-documented and numerous recent successes of deep learning, the application of standard deep architectures to many classification prob-lems within empirical software engineering remains problematic due to the large vol-umes of labeled data required for training. Here we make the argument that, for some problems, this hurdle can be overcome by taking advantage of low-shot learning in combination with simpler deep architectures that reduce the total number of param-eters that need to be learned.Findings:We apply low-shot learning to the task of classifying UML class and sequence diagrams from Github, and demonstrate that surprisingly good performance can be achieved by using only tens or hundreds of examples for each category when paired with an appropriate architecture. Using a large, off-the-shelf architecture, on the other hand, doesn’t perform beyond random guessing even when trained on thou-sands of samples.Conclusion:Our findings suggest that identifying problems within empirical software engineering that lend themselves to low-shot learning could accelerate the adoption of deep learning algorithms within the empirical software engineering community."
    },
    {
        "url": "https://paperity.org/p/192806896/arabia-felix-2-0-a-cross-linguistic-twitter-analysis-of-happiness-patterns-in-the-united",
        "title": "Arabia Felix 2.0: a cross-linguistic Twitter analysis of happiness patterns in the United Arab Emirates",
        "authors": [
            "Aamna Al Shehhi",
            " Justin Thomas",
            " Roy Welsch"
        ],
        "date_article": "04-2019",
        "short_description": "The global popularity of social media platforms has given rise to unprecedented amounts of data, much of which reflects the thoughts, opinions and affective states of individual users. Systematic explorations of these large datasets can yield valuable information about a variety of psychological and sociocultural variables. The global nature of these platforms makes it important...",
        "keywords": [
            "United Arab Emirates",
            "Sentiment analysis",
            "Happiness",
            "Hedonometer",
            "Valence Shift Word Graph",
            "Big Data",
            "Twitter",
            "Arabic tweets",
            "English tweets"
        ],
        "number_of_pages": "20",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0195-2.pdf",
        "abstract": "The global popularity of social media platforms has given rise to unprecedented amounts of data, much of which reflects the thoughts, opinions and affective states of individual users. Systematic explorations of these large datasets can yield valuable information about a variety of psychological and sociocultural variables. The global nature of these platforms makes it important to extend this type of exploration across cultures and languages as each situation is likely to present unique methodologi-cal challenges and yield findings particular to the specific sociocultural context. To date, very few studies exploring large social media datasets have focused on the Arab world. This study examined social media use in Arabic and English across the United Arab Emirates (UAE), looking specifically at indicators of subjective wellbeing (hap-piness) across both languages. A large social media dataset, spanning 2013 to 2017, was extracted from Twitter. More than 17 million Twitter messages (tweets), written in Arabic and English and posted by users based in the UAE, were analyzed. Numer-ous differences were observed between individuals posting messages (tweeting) in English compared with those posting in Arabic. These differences included significant variations in the mean number of tweets posted, and the mean size of users networks (e.g. the number of followers). Additionally, using lexicon-based sentiment analytic tools (Hedonometer and Valence Shift Word Graphs), temporal patterns of happiness (expressions of positive sentiment) were explored in both languages across all seven regions (Emirates) of the UAE. Findings indicate that 7:00 am was the happiest hour, and Friday was the happiest day for both languages (the least happy day varied by language). The happiest months differed based on language, and there were also significant variations in sentiment patterns, peaks and troughs in happiness, associated with events of sociopolitical and religio-cultural significance for the UAE."
    },
    {
        "url": "https://paperity.org/p/192591446/an-intelligent-alzheimers-disease-diagnosis-method-using-unsupervised-feature-learning",
        "title": "An intelligent Alzheimer’s disease diagnosis method using unsupervised feature learning",
        "authors": [
            "Firouzeh Razavi",
            " Mohammad Jafar Tarokh",
            " Mahmood Alborzi"
        ],
        "date_article": "04-2019",
        "short_description": "Today, the diagnosis of Alzheimer’s disease (AD) or mild cognitive impairment (MCI) has attracted the attention of researchers in this field owing to the increase in the occurrence of the diseases and the need for early diagnosis. Unfortunately, the nature of high dimension of neural data and few available samples led to the creation of a precise computer diagnostic system...",
        "keywords": [
            "Alzheimer’s disease",
            "Sparse filtering",
            "Unsupervised feature learning",
            "Intelligent diagnosis",
            "SoftMax regression"
        ],
        "number_of_pages": "16",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0190-7.pdf",
        "abstract": "Today, the diagnosis of Alzheimer’s disease (AD) or mild cognitive impairment (MCI) has attracted the attention of researchers in this field owing to the increase in the occurrence of the diseases and the need for early diagnosis. Unfortunately, the nature of high dimension of neural data and few available samples led to the creation of a precise computer diagnostic system. Machine learning techniques, especially deep learning, have been considered as a useful tool in this field. Inspired by the concept of unsupervised feature learning that uses artificial intelligence to learn features from raw data, a two-stage method was presented for an intelligent diagnosis of Alzhei-mer’s disease. At the first stage of learning, scattered filtering, an uncontrolled two-layer neural network was used to directly learn features from raw data. At the second stage, SoftMax regression was used to categorize health statuses based on the learned features. The proposed method was validated by the data sets of Alzheimer’s Brain Images. The results showed that the proposed method achieved very good diagnos-tic accuracy and was better than the existing methods for brain image data sets. The proposed method reduces the need for human work and makes it easy to intelligently diagnose for big data processing, because the learning features are adaptive. In our experiments with the Alzheimer’s Disease Neuroimaging Initiative (ADNI) data, a dual and multi-class classification was conducted for AD/MCI diagnosis and the superiority of the proposed method in comparison with the advanced methods was shown."
    },
    {
        "url": "https://paperity.org/p/192192982/detecting-high-indoor-crowd-density-with-wi-fi-localization-a-statistical-mechanics",
        "title": "Detecting high indoor crowd density with Wi-Fi localization: a statistical mechanics approach",
        "authors": [
            "Sonja Georgievska",
            " Philip Rutten",
            " Jan Amor"
        ],
        "date_article": "03-2019",
        "short_description": "We address the problem of detecting highly raised crowd density in situations such as indoor dance events. We propose a new method for estimating crowd density by anonymous, non-participatory, indoor Wi-Fi localization of smart phones. Using a probabilistic model inspired by statistical mechanics, and relying only on big data analytics, we tackle three challenges: (1) the...",
        "keywords": [
            "Big data analytics",
            "Crowd density estimation",
            "Probabilistic modeling",
            "Indoor Wi-Fi localization"
        ],
        "number_of_pages": "23",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0194-3.pdf",
        "abstract": "We address the problem of detecting highly raised crowd density in situations such as indoor dance events. We propose a new method for estimating crowd density by anonymous, non-participatory, indoor Wi-Fi localization of smart phones. Using a probabilistic model inspired by statistical mechanics, and relying only on big data ana-lytics, we tackle three challenges: (1) the ambiguity of Wi-Fi based indoor positioning, which appears regardless of whether the latter is performed with machine learning or with optimization, (2) the MAC address randomization when a device is not connected, and (3) the volatility of packet interarrival times. The main result is that our estimation becomes more—rather than less—accurate when the crowd size increases. This prop-erty is crucial for detecting dangerous crowd density."
    },
    {
        "url": "https://paperity.org/p/192221735/enhanced-secured-map-reduce-layer-for-big-data-privacy-and-security",
        "title": "Enhanced Secured Map Reduce layer for Big Data privacy and security",
        "authors": [
            "Priyank Jain",
            " Manasi Gyanchandani",
            " Nilay Khar"
        ],
        "date_article": "03-2019",
        "short_description": "The publication and dissemination of raw data are crucial elements in commercial, academic, and medical applications. With an increasing number of open platforms, such as social networks and mobile devices from which data may be collected, the volume of such data has also increased over time move toward becoming as Big Data. The traditional model of Big Data does not specify any...",
        "keywords": [
            "Big Data",
            "Security and privacy",
            "Privacy preserving",
            "HDFS",
            "HADOOP",
            "HIVE",
            "Secured Map Reduce (SMR) layer"
        ],
        "number_of_pages": "17",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0193-4.pdf",
        "abstract": "The publication and dissemination of raw data are crucial elements in commercial, academic, and medical applications. With an increasing number of open platforms, such as social networks and mobile devices from which data may be collected, the volume of such data has also increased over time move toward becoming as Big Data. The traditional model of Big Data does not specify any level for capturing the sensi-tivity of data both structured and unstructured. It additionally needs to incorporate the notion of privacy and security where the risk of exposing personal information is probabilistically minimized. This paper introduced security and privacy layer between HDFS and MR Layer (Map Reduce) known as new proposed Secured Map Reduce (SMR) Layer and this model is known as SMR model. The core benefit of this work is to promote data sharing for knowledge mining. This model creates a privacy and security guarantee, resolve scalability issues of privacy and maintain the privacy-utility tradeoff for data miners. In this SMR model, running time and information loss have a remark-able improvement over the existing approaches and CPU and memory usage are also optimized."
    },
    {
        "url": "https://paperity.org/p/191750140/bayesian-mixture-models-and-their-big-data-implementations-with-application-to-invasive",
        "title": "Bayesian mixture models and their Big Data implementations with application to invasive species presence-only data",
        "authors": [
            "Insha Ullah",
            " Kerrie Mengersen"
        ],
        "date_article": "03-2019",
        "short_description": "Due to their conceptual simplicity and flexibility, non-parametric mixture models are widely used to identify latent clusters in data. However, when it comes to Big Data, such as Landsat imagery, such model fitting is computationally prohibitive. To overcome this issue, we fit Bayesian non-parametric models to pre-smoothed data, thereby reducing the computational time from days...",
        "keywords": [
            "Dirichlet process mixture models",
            "k-means clustering",
            "Tree-based clustering",
            "Satellite imagery data",
            "Fire-ants habitat",
            "One-class support vector machine"
        ],
        "number_of_pages": "25",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0188-1.pdf",
        "abstract": "Due to their conceptual simplicity and flexibility, non-parametric mixture models are widely used to identify latent clusters in data. However, when it comes to Big Data, such as Landsat imagery, such model fitting is computationally prohibitive. To over-come this issue, we fit Bayesian non-parametric models to pre-smoothed data, thereby reducing the computational time from days to minutes, while disregarding little of the useful information. Tree based clustering is used to partition the clusters into smaller and smaller clusters in order to identify clusters of high, medium and low interest. The tree-based clustering method is applied to Landsat images from the Brisbane region, which were the actual sources of motivation for development of the method. The images are taken as a part of the red imported fire-ant eradication program that was launched in September 2001 and which is funded by all Australian states and territo-ries, along with the federal government. To satisfy budgetary constraints, modelling is performed to estimate the risk of fire-ant incursion in each cluster so that the eradica-tion program focuses on high risk clusters. The likelihood of containment is successfully derived by combining the fieldwork survey data with the results obtained from the proposed method."
    },
    {
        "url": "https://paperity.org/p/191778893/customer-churn-prediction-in-telecom-using-machine-learning-in-big-data-platform",
        "title": "Customer churn prediction in telecom using machine learning in big data platform",
        "authors": [
            "Abdelrahim Kasem Ahmad",
            " Assef Jafar",
            " Kadan Aljoum"
        ],
        "date_article": "03-2019",
        "short_description": "Customer churn is a major problem and one of the most important concerns for large companies. Due to the direct effect on the revenues of the companies, especially in the telecom field, companies are seeking to develop means to predict potential customer to churn. Therefore, finding factors that increase customer churn is important to take necessary actions to reduce this churn...",
        "keywords": [
            "Customer churn prediction",
            "Churn in telecom",
            "Machine learning",
            "Feature selection",
            "Classification",
            "Mobile Social Network Analysis",
            "Big data"
        ],
        "number_of_pages": "24",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0191-6.pdf",
        "abstract": "Customer churn is a major problem and one of the most important concerns for large companies. Due to the direct effect on the revenues of the companies, especially in the telecom field, companies are seeking to develop means to predict potential customer to churn. Therefore, finding factors that increase customer churn is important to take necessary actions to reduce this churn. The main contribution of our work is to develop a churn prediction model which assists telecom operators to predict cus-tomers who are most likely subject to churn. The model developed in this work uses machine learning techniques on big data platform and builds a new way of features’ engineering and selection. In order to measure the performance of the model, the Area Under Curve (AUC) standard measure is adopted, and the AUC value obtained is 93.3%. Another main contribution is to use customer social network in the prediction model by extracting Social Network Analysis (SNA) features. The use of SNA enhanced the performance of the model from 84 to 93.3% against AUC standard. The model was prepared and tested through Spark environment by working on a large dataset created by transforming big raw data provided by SyriaTel telecom company. The dataset con-tained all customers’ information over 9 months, and was used to train, test, and evalu-ate the system at SyriaTel. The model experimented four algorithms: Decision Tree, Random Forest, Gradient Boosted Machine Tree “GBM” and Extreme Gradient Boosting “XGBOOST”. However, the best results were obtained by applying XGBOOST algorithm. This algorithm was used for classification in this churn predictive model."
    },
    {
        "url": "https://paperity.org/p/191807646/survey-on-deep-learning-with-class-imbalance",
        "title": "Survey on deep learning with class imbalance",
        "authors": [
            "Justin M. Johnson",
            " Taghi M. Khoshgoftaar"
        ],
        "date_article": "03-2019",
        "short_description": "The purpose of this study is to examine existing deep learning techniques for addressing class imbalanced data. Effective classification with imbalanced data is an important area of research, as high class imbalance is naturally inherent in many real-world applications, e.g., fraud detection and cancer detection. Moreover, highly imbalanced data poses added difficulty, as most...",
        "keywords": [
            "Deep learning",
            "Deep neural networks",
            "Class imbalance",
            "Big data"
        ],
        "number_of_pages": "54",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0192-5.pdf",
        "abstract": "The purpose of this study is to examine existing deep learning techniques for address-ing class imbalanced data. Effective classification with imbalanced data is an impor-tant area of research, as high class imbalance is naturally inherent in many real-world applications, e.g., fraud detection and cancer detection. Moreover, highly imbalanced data poses added difficulty, as most learners will exhibit bias towards the majority class, and in extreme cases, may ignore the minority class altogether. Class imbalance has been studied thoroughly over the last two decades using traditional machine learning models, i.e. non-deep learning. Despite recent advances in deep learning, along with its increasing popularity, very little empirical work in the area of deep learning with class imbalance exists. Having achieved record-breaking performance results in several complex domains, investigating the use of deep neural networks for problems contain-ing high levels of class imbalance is of great interest. Available studies regarding class imbalance and deep learning are surveyed in order to better understand the efficacy of deep learning when applied to class imbalanced data. This survey discusses the imple-mentation details and experimental results for each study, and offers additional insight into their strengths and weaknesses. Several areas of focus include: data complexity, architectures tested, performance interpretation, ease of use, big data application, and generalization to other domains. We have found that research in this area is very limited, that most existing work focuses on computer vision tasks with convolutional neural networks, and that the effects of big data are rarely considered. Several tradi-tional methods for class imbalance, e.g. data sampling and cost-sensitive learning, prove to be applicable in deep learning, while more advanced methods that exploit neural network feature learning abilities show promising results. The survey concludes with a discussion that highlights various gaps in deep learning from class imbalanced data for the purpose of guiding future research."
    },
    {
        "url": "https://paperity.org/p/191836399/hpcc-based-framework-for-copd-readmission-risk-analysis",
        "title": "HPCC based framework for COPD readmission risk analysis",
        "authors": [
            "Piyush Jain",
            " Ankur Agarwal",
            " Ravi Behar"
        ],
        "date_article": "03-2019",
        "short_description": "Prevention of hospital readmissions has the potential of providing better quality of care to the patients and deliver significant cost savings. A review of existing readmission analysis frameworks based on data type, data size, disease conditions, algorithms and other features shows that existing frameworks do not address the issue of using large amounts of data that is...",
        "keywords": [
            "COPD readmission",
            "Prediction",
            "Nave Bayes",
            "HPCC",
            "Big Data"
        ],
        "number_of_pages": "13",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0189-0.pdf",
        "abstract": "Prevention of hospital readmissions has the potential of providing better quality of care to the patients and deliver significant cost savings. A review of existing readmis-sion analysis frameworks based on data type, data size, disease conditions, algorithms and other features shows that existing frameworks do not address the issue of using large amounts of data that is fundamental to readmission prediction analysis. Avail-able patient data for readmission risk analysis has high dimensionality and number of instances. Further, there is more new data produced everyday which can be used on a continuous basis to improve the prediction power of risk models. This study proposes a High Performance Computing Cluster based Big Data readmission risk analysis frame-work which uses Nave Bayes classification algorithm. The study shows that the over-all evaluation time using Big Data and a parallel computing platform can be significantly decreased, while maintaining model performance."
    },
    {
        "url": "https://paperity.org/p/191865152/on-the-sustainability-of-smart-and-smarter-cities-in-the-era-of-big-data-an",
        "title": "On the sustainability of smart and smarter cities in the era of big data: an interdisciplinary and transdisciplinary literature review",
        "authors": [
            "Simon Elias Bibri"
        ],
        "date_article": "03-2019",
        "short_description": "There has recently been a conscious push for cities across the globe to be smart and even smarter and thus more sustainable by developing and implementing big data technologies and their applications across various urban domains in the hopes of reaching the required level of sustainability and improving the living standard of citizens. Having gained momentum and traction as a...",
        "keywords": [
            "Smart cities",
            "Smarter cities",
            "ICT of pervasive computing",
            "Big data analytics",
            "Big data applications",
            "Urban intelligence functions",
            "Sustainability",
            "Sustainable development",
            "Urban systems and domains"
        ],
        "number_of_pages": "64",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0182-7.pdf",
        "abstract": "There has recently been a conscious push for cities across the globe to be smart and even smarter and thus more sustainable by developing and implementing big data technologies and their applications across various urban domains in the hopes of reaching the required level of sustainability and improving the living standard of citi-zens. Having gained momentum and traction as a promising response to the needed transition towards sustainability and to the challenges of urbanisation, smart and smarter cities as approaches to data-driven urbanism are increasingly adopting the advanced forms of ICT to improve their performance in line with the goals of sustain-able development and the requirements of urban growth. One of such forms that has tremendous potential to enhance urban operations, functions, services, designs, strategies, and policies in this direction is big data analytics and its application. This is due to the kind of well-informed decision-making and enhanced insights enabled by big data computing in the form of applied intelligence. However, topical studies on big data technologies and their applications in the context of smart and smarter cities tend to deal largely with economic growth and the quality of life in terms of service efficiency and betterment while overlooking and barely exploring the untapped potential of such applications for advancing sustainability. In fact, smart and smarter cities raise several issues and involve significant challenges when it comes to their development and implementation in the context of sustainability. With that in regard, this paper provides a comprehensive, state-of-the-art review and synthesis of the field of smart and smarter cities in relation to sustainability and related big data analytics and its application in terms of the underlying foundations and assumptions, research issues and debates, opportunities and benefits, technological developments, emerging trends, future practices, and challenges and open issues. This study shows that smart and smarter cities are associated with misunderstanding and deficiencies as regards their incorporation of, and contribution to, sustainability. Nevertheless, as also revealed by this study, tremendous opportunities are available for utilising big data analytics and its application in smart cities of the future to improve their contribution to the goals of sustainable development by optimising and enhancing urban operations, functions, services, designs, strategies, and policies, as well as by finding answers to challeng-ing analytical questions and thereby advancing knowledge forms. However, just as there are immense opportunities ahead to embrace and exploit, there are enormous challenges and open issues ahead to address and overcome in order to achieve a Open Access©  The  Author(s)  2019.  This  article  is  distributed  under  the  terms  of  the  Creative  Commons  Attribution  4.0  International  License  (http://creat iveco mmons .org/licen ses/by/4.0/),  which  permits  unrestricted  use,  distribution,  and  reproduction  in  any  medium,  provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made.SURVEY PAPERBibri J Big Data            (2019) 6:25  https://doi.org/10.1186/s40537-019-0182-7*Correspondence:   simoe@ntnu.no Department of Computer Science and Department of Urban Planning and Design, The Norwegian University of Science and Technology, Sem Saelands veie 9, 7491 Trondheim, Norway\nPage 2 of 64Bibri J Big Data            (2019) 6:25 successful implementation of big data technology and its novel applications in such cities."
    },
    {
        "url": "https://paperity.org/p/191631761/adaptive-network-diagram-constructions-for-representing-big-data-event-streams-on",
        "title": "Adaptive network diagram constructions for representing big data event streams on monitoring dashboards",
        "authors": [
            "Alexander V. Mantzaris",
            " Thomas G. Walker",
            " Cameron E. Taylor"
        ],
        "date_article": "03-2019",
        "short_description": "Critical systems that produce big data streams can require human operators to monitor these event streams for changes of interest. Automated systems which oversee many tasks can still have a need for the ‘human-in-the-loop’ operator to evaluate whether an intervention is required due to a lack of suitable training data initially offered to the system which would allow a correct...",
        "keywords": [
            "Event streams",
            "Big data",
            "Networks",
            "Graph visualization",
            "Co-occurrence networks",
            "Visual analytics",
            "Dash boards"
        ],
        "number_of_pages": "19",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0187-2.pdf",
        "abstract": "Critical systems that produce big data streams can require human operators to monitor these event streams for changes of interest. Automated systems which oversee many tasks can still have a need for the ‘human-in-the-loop’ operator to evaluate whether an intervention is required due to a lack of suitable training data initially offered to the system which would allow a correct course of actions to be taken. In order for an operator to be capable of reacting to real-time events, the visual depiction of the event data must be in a form which captures essential associations and is readily understood by visual inspection. A similar requirement can be found during inspections on activity protocols in a large organization where a code of correct conduct is prescribed and there is a need to oversee whether the activity traces match the expectations, with minimal delay. The methodology presented here addresses these concerns by provid-ing an adaptive window sizing measurement for subsetting the data, and subsequently produces a set of network diagrams based upon event label co-occurrence networks. With an intuitive method of network construction the amount of time required for operators to learn how to monitor complex event streams of big datasets can be reduced."
    },
    {
        "url": "https://paperity.org/p/191718020/mining-aspects-of-customers-review-on-the-social-network",
        "title": "Mining aspects of customer’s review on the social network",
        "authors": [
            "Tu Nguyen Thi Ngoc",
            " Ha Nguyen Thi Thu",
            " Viet Anh Nguyen"
        ],
        "date_article": "02-2019",
        "short_description": "This study represents an efficient method for extracting product aspects from customer reviews and give solutions for inferring aspect ratings and aspect weights. Aspect ratings often reflect the user’s satisfaction on aspects of a product and aspect weights reflect the degree of importance of the aspects posed by the user. These tasks therefore play a very important role for...",
        "keywords": [
            "Aspect extraction",
            "Aspect rating",
            "Aspect weight",
            "Conditional probability",
            "Core term",
            "Naive Bayes"
        ],
        "number_of_pages": "21",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0184-5.pdf",
        "abstract": "This study represents an efficient method for extracting product aspects from cus-tomer reviews and give solutions for inferring aspect ratings and aspect weights. Aspect ratings often reflect the user’s satisfaction on aspects of a product and aspect weights reflect the degree of importance of the aspects posed by the user. These tasks therefore play a very important role for manufacturers to better understand their customers’ opinion on their products and services. The study addresses the problem of aspect extraction by using aspect words based on conditional probability com-bined with the bootstrap technique. To infer the user’s rating for aspects, a supervised approach called the Naïve Bayes classification method is proposed to learn the aspect ratings in which sentiment words are considered as features. The weight of an aspect is estimated by leveraging the frequencies of aspect words within each review and the aspect consistency across all reviews. Experimental results show that the proposed method obtains very good performance on real world datasets in comparison with other state-of-the-art methods."
    },
    {
        "url": "https://paperity.org/p/191689267/the-effects-of-class-rarity-on-the-evaluation-of-supervised-healthcare-fraud-detection",
        "title": "The effects of class rarity on the evaluation of supervised healthcare fraud detection models",
        "authors": [
            "Matthew Herland",
            " Richard A. Bauder",
            " Taghi M. Khoshgoftaar"
        ],
        "date_article": "02-2019",
        "short_description": "The United States healthcare system produces an enormous volume of data with a vast number of financial transactions generated by physicians administering healthcare services. This makes healthcare fraud difficult to detect, especially when there are considerably less fraudulent transactions (documented and readily available) than non-fraudulent. The ability to successfully...",
        "keywords": [
            "Big Data",
            "Medicare",
            "LEIE",
            "Fraud detection",
            "Cross-Validation",
            "Test set",
            "Class imbalance",
            "Rarity",
            "Random Undersampling"
        ],
        "number_of_pages": "33",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0181-8.pdf",
        "abstract": "The United States healthcare system produces an enormous volume of data with a vast number of financial transactions generated by physicians administering healthcare ser-vices. This makes healthcare fraud difficult to detect, especially when there are consid-erably less fraudulent transactions (documented and readily available) than non-fraud-ulent. The ability to successfully detect fraudulent activities in healthcare, given such discrepancies, can garner up to $350 billion in recovered monetary losses. In machine learning, when one class has a substantially larger number of instances (majority) compared to the other (minority), this is known as class imbalance. In this paper, we focus specifically on Medicare, utilizing three ‘Big Data’ Medicare claims datasets with real-world fraudulent physicians. We create a training and test dataset for all three Medicare parts, both separately and combined, to assess fraud detection performance. To emulate class rarity, which indicates particularly severe levels of class imbalance, we generate additional datasets, by removing fraud instances, to determine the effects of rarity on fraud detection performance. Before a machine learning model can be distrib-uted for real-world use, a performance evaluation is necessary to determine the best configuration (e.g. learner, class sampling ratio) and whether the associated error rates are low, indicating good detection rates. With our research, we demonstrate the effects of severe class imbalance and rarity using a training and testing (Train_Test) evaluation method via a hold-out set, and provide our recommendations based on the supervised machine learning results. Additionally, we repeat the same experiments using Cross-Validation, and determine it is a viable substitute for Medicare fraud detection. For machine learning with the severe class imbalance datasets, we found that, as expected, fraud detection performance decreased as the fraudulent instances became more rare. We apply Random Undersampling to both Train_Test and Cross-Validation, for all original and generated datasets, in order to assess potential improvements in fraud detection by reducing the adverse effects of class imbalance and rarity. Overall, our results indicate that the Train_Test method significantly outperforms Cross-Validation."
    },
    {
        "url": "https://paperity.org/p/191660514/selecting-a-representative-decision-tree-from-an-ensemble-of-decision-tree-models-for",
        "title": "Selecting a representative decision tree from an ensemble of decision-tree models for fast big data classification",
        "authors": [
            "Abraham Itzhak Weinberg",
            " Mark Las"
        ],
        "date_article": "02-2019",
        "short_description": "The goal of this paper is to reduce the classification (inference) complexity of tree ensembles by choosing a single representative model out of ensemble of multiple decision-tree models. We compute the similarity between different models in the ensemble and choose the model, which is most similar to others as the best representative of the entire dataset. The similarity-based...",
        "keywords": [
            "Big data",
            "Ensemble learning",
            "Lazy ensemble evaluation",
            "Decision trees",
            "Editing distance",
            "Tree similarity"
        ],
        "number_of_pages": "17",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0186-3.pdf",
        "abstract": "The goal of this paper is to reduce the classification (inference) complexity of tree ensembles by choosing a single representative model out of ensemble of multiple decision-tree models. We compute the similarity between different models in the ensemble and choose the model, which is most similar to others as the best represent-ative of the entire dataset. The similarity-based approach is implemented with three different similarity metrics: a syntactic, a semantic, and a linear combination of the two. We compare this tree selection methodology to a popular ensemble algorithm (majority voting) and to the baseline of randomly choosing one of the local models. In addition, we evaluate two alternative tree selection strategies: choosing the tree hav-ing the highest validation accuracy and reducing the original ensemble to five most representative trees. The comparative evaluation experiments are performed on six big datasets using two popular decision-tree algorithms (J48 and CART ) and splitting each dataset horizontally into six different amounts of equal-size slices (from 32 to 1024). In most experiments, the syntactic similarity approach, named SySM—Syntactic Similar-ity Method, provides a significantly higher testing accuracy than the semantic and the combined ones. The mean accuracy of SySM over all datasets is 0.835±0.065 for CART and 0.769±0.066 for J48. On the other hand, we find no statistically significant differ-ence between the testing accuracy of the trees selected by SySM and the trees having the highest validation accuracy. Comparing to ensemble algorithms, the representative models selected by the proposed methods provide a higher speed for big data clas-sification along with being more compact and interpretable."
    },
    {
        "url": "https://paperity.org/p/191746773/gapprox-using-gallup-approach-for-approximation-in-big-data-processing",
        "title": "Gapprox: using Gallup approach for approximation in Big Data processing",
        "authors": [
            "Hossein Ahmadvand",
            " Maziar Goudarzi",
            " Fouzhan Foroutan"
        ],
        "date_article": "02-2019",
        "short_description": "As Big Data processing often takes a long time and needs a lot of resources, sampling and approximate computing techniques may be used to generate a desired Quality of Result. On the other hand, due to not considering data variety, available sample-based approximation approaches suffer from poor accuracy. Data variety is one of the key features of Big Data which causes various...",
        "keywords": [
            "Data variety",
            "Quality of Result",
            "Approximation",
            "Cluster sampling"
        ],
        "number_of_pages": "24",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0185-4.pdf",
        "abstract": "As Big Data processing often takes a long time and needs a lot of resources, sampling and approximate computing techniques may be used to generate a desired Quality of Result. On the other hand, due to not considering data variety, available sample-based approximation approaches suffer from poor accuracy. Data variety is one of the key features of Big Data which causes various parts of data to have different impact on the final result. To address this problem, we develop a data variety aware approxima-tion approach called Gapprox. Our idea is to use a kind of cluster sampling to improve the accuracy of estimation. Our approach can decrease the amount of data to be processed to achieve the desired Quality of Result with acceptable error bound and confidence interval. We divide the input data into some blocks considering the intra/inter cluster variance. The size of the block and the sample size are determined in such a way that by processing small amount of input data, an acceptable confidence inter-val and error bound is achieved. We compared our work with two well-known state of the art. The experimental results show that our result surpasses the state of the art and improve processing time up to 17× compared to ApproxHadoop and 8× compared to Sapprox when the user can tolerate an error of 5% with 95% confidence."
    },
    {
        "url": "https://paperity.org/p/191775526/hierarchical-data-fusion-for-smart-healthcare",
        "title": "Hierarchical data fusion for Smart Healthcare",
        "authors": [
            "Rustem Dautov",
            " Salvatore Distefano",
            " Rajkumaar Buyy"
        ],
        "date_article": "02-2019",
        "short_description": "The Internet of Things (IoT) facilitates creation of smart spaces by converting existing environments into sensor-rich data-centric cyber-physical systems with an increasing degree of automation, giving rise to Industry 4.0. When adopted in commercial/industrial contexts, this trend is revolutionising many aspects of our everyday life, including the way people access and receive...",
        "keywords": [
            "Smart Healthcare",
            "Industry 4.0",
            "Data fusion",
            "Complex Event Processing",
            "Distributed architecture",
            "Internet of Things",
            "Edge computing",
            "Cloud computing"
        ],
        "number_of_pages": "23",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0183-6.pdf",
        "abstract": "The Internet of Things (IoT ) facilitates creation of smart spaces by converting existing environments into sensor-rich data-centric cyber-physical systems with an increasing degree of automation, giving rise to Industry 4.0. When adopted in commercial/indus-trial contexts, this trend is revolutionising many aspects of our everyday life, including the way people access and receive healthcare services. As we move towards Health-care Industry 4.0, the underlying IoT systems of Smart Healthcare spaces are growing in size and complexity, making it important to ensure that extreme amounts of collected data are properly processed to provide valuable insights and decisions according to requirements in place. This paper focuses on the Smart Healthcare domain and addresses the issue of data fusion in the context of IoT networks, consisting of edge devices, network and communications units, and Cloud platforms. We propose a distributed hierarchical data fusion architecture, in which different data sources are combined at each level of the IoT taxonomy to produce timely and accurate results. This way, mission-critical decisions, as demonstrated by the presented Smart Health-care scenario, are taken with minimum time delay, as soon as necessary information is generated and collected. The proposed approach was implemented using the Com-plex Event Processing technology, which natively supports the hierarchical processing model and specifically focuses on handling streaming data ‘on the fly’—a key require-ment for storage-limited IoT devices and time-critical application domains. Initial experiments demonstrate that the proposed approach enables fine-grained decision taking at different data fusion levels and, as a result, improves the overall performance and reaction time of public healthcare services, thus promoting the adoption of the IoT technologies in Healthcare Industry 4.0."
    },
    {
        "url": "https://paperity.org/p/191804279/predicting-customers-gender-and-age-depending-on-mobile-phone-data",
        "title": "Predicting customer’s gender and age depending on mobile phone data",
        "authors": [
            "Ibrahim Mousa Al-Zuabi",
            " Assef Jafar",
            " Kadan Aljoum"
        ],
        "date_article": "02-2019",
        "short_description": "In the age of data driven solution, the customer demographic attributes, such as gender and age, play a core role that may enable companies to enhance the offers of their services and target the right customer in the right time and place. In the marketing campaign, the companies want to target the real user of the GSM (global system for mobile communications), not the line owner...",
        "keywords": [
            "Gender prediction",
            "Age prediction",
            "Customer behavior",
            "Machine learning",
            "Big data",
            "Classification",
            "CDR"
        ],
        "number_of_pages": "16",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0180-9.pdf",
        "abstract": "In the age of data driven solution, the customer demographic attributes, such as gender and age, play a core role that may enable companies to enhance the offers of their services and target the right customer in the right time and place. In the market‑ing campaign, the companies want to target the real user of the GSM (global system for mobile communications), not the line owner. Where sometimes they may not be the same. This work proposes a method that predicts users’ gender and age based on their behavior, services and contract information. We used call detail records (CDRs), customer relationship management (CRM) and billing information as a data source to analyze telecom customer behavior, and applied different types of machine learning algorithms to provide marketing campaigns with more accurate information about customer demographic attributes. This model is built using reliable data set of 18,000 users provided by SyriaTel Telecom Company, for training and testing. The model applied by using big data technology and achieved 85.6% accuracy in terms of user gender prediction and 65.5% of user age prediction. The main contribution of this work is the improvement in the accuracy in terms of user gender prediction and user age prediction based on mobile phone data and end‑to‑end solution that approaches customer data from multiple aspects in the telecom domain."
    },
    {
        "url": "https://paperity.org/p/191833032/application-of-variable-selection-and-dimension-reduction-on-predictors-of-mses",
        "title": "Application of variable selection and dimension reduction on predictors of MSE’s development",
        "authors": [
            "Habtamu Tilaye Wubeti"
        ],
        "date_article": "02-2019",
        "short_description": "Nature create variables using its character component, and variables are sharing characters from a vary small to relatively large scale. This results, variables to have from a vary different to a more similar character, and leads to have a relation ship. Literature suggested different relation measures based on the nature of variable and type of relation ship exist. Today, due to...",
        "keywords": [
            "Variable selection",
            "Dimension reduction",
            "CANOVA",
            "Stepwise elimination",
            "Lasso variable selection"
        ],
        "number_of_pages": "44",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-018-0153-4.pdf",
        "abstract": "Nature create variables using its character component, and variables are sharing characters from a vary small to relatively large scale. This results, variables to have from a vary different to a more similar character, and leads to have a relation ship. Litera-ture suggested different relation measures based on the nature of variable and type of relation ship exist. Today, due to having high variety of frequently produced large data size, currently suggested variable filtering and selection methods have gaps to full fill the need. This research desires to fill this gap by comparing literature suggested methods to finding out a better variable selection and dimension reduction methods. The result from regression analysis using all literature suggested factors shows that none of the predictors for development status of enterprise are significant, and only 10 predictors for number of employer in an enterprise are significant out of 81 fac-tors. Since, variable selection and dimension reduction methods are applied to find out predictors of a response by removing variable redundancy, and complexity of incorporating large number variable. Based on statistical power, for the results from variable selection methods, specially association and correlation methods showed that, CANOVA more efficiently detects non-linear or non-monotonic correlation between a continuous–continuous and a continuous-categorical variables. Spearman’s correlation coefficient more efficiently detects a monotonic correlation between a continuous with a continuous, and a continuous with a categorical variable. Pearson correlation coefficient more efficiently detects the linear correlation between continuous vari-ables. MIC efficiently detects non-linear or non-monotonic relation between continu-ous variables. Chi-square test of independence efficiently detects relation between a continuous with a continuous, and categorical with categorical variables, but the non linear or non monotonic relation between a continuous with a categorical are not well detected. On the other hand, the result from lasso and stepwise methods reveals that, the relation between the predictor and response due to interaction effect not detected by correlation and association methods are detected by stepwise variable selection method, and the multicollinearity is detected and removed by lasso method. Regressing the response variable “number of employer in an enterprise” based on vari-ables selected by lasso and stepwise method does bring greater model fitness (based on adjusted R-squared value) than variables selected by association and correlation methods. Similarly, regressing the response variable “development status of an enter-prise” based on variables selected by association and correlation methods does bring Open Access©  The  Author(s)  2019.  This  article  is  distributed  under  the  terms  of  the  Creative  Commons  Attribution  4.0  International  License  (http://creat iveco mmons .org/licen ses/by/4.0/),  which  permits  unrestricted  use,  distribution,  and  reproduction  in  any  medium,  provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made.METHODOLOGYWubetie J Big Data            (2019) 6:17  https://doi.org/10.1186/s40537-018-0153-4*Correspondence:   habtamu.tilaye@yahoo.com Statistics Department, College of Natural and Computational Science, University of Gondar, Gondar, Ethiopia\nPage 2 of 44Wubetie J Big Data            (2019) 6:17 12 significant variables, where none of variables are significant from variables selected by lasso and stepwise methods. As a result, 51 predictors for number of employment in an enterprise, and 40 predictors for development status of an enterprise are detected as significantly related variables. And, lasso and stepwise methods are preferred to select predictors of a continuous response variable “number of employers in an enter-prise”, and association and correlation methods are preferred to select predictors of a categorical response variable “development status of an enterprise”. Finally, the reduced regression models result reveals that, 20 predictors have causal relation with number of employment in an enterprise, and 12 predictors have causal relation with development status of an enterprise. On the other hand, based on model fitness, information lost, and number of significant factors, principal factor is preferred and applied in dimen-sion reduction for a categorical response variable “development status of an enterprise”, and factor score based regression is preferred and applied for a continuous response variable “number of employers in an enterprise”. However, the comparison of the results in variable selection and dimension reduction indicates that, variable selection methods gave more gain in model fitness than dimension reduction methods. Hence, the suggested variable selection methods are more preferred than dimension reduc-tion methods, and applied to find out predictors. In general, the suggested procedure for variable selection methods are recommended when small number of variables are studied, and the suggested dimension reduction methods are recommended for large number of variant variables (Big data case)."
    },
    {
        "url": "https://paperity.org/p/185678118/a-parallel-and-distributed-stochastic-gradient-descent-implementation-using-commodity",
        "title": "A parallel and distributed stochastic gradient descent implementation using commodity clusters",
        "authors": [
            "Robert K. L. Kennedy",
            " Taghi M. Khoshgoftaar",
            " Flavio Villanustr"
        ],
        "date_article": "02-2019",
        "short_description": "Deep Learning is an increasingly important subdomain of artificial intelligence, which benefits from training on Big Data. The size and complexity of the model combined with the size of the training dataset makes the training process very computationally and temporally expensive. Accelerating the training process of Deep Learning using cluster computers faces many challenges...",
        "keywords": [
            "Parallel stochastic gradient descent",
            "Parallel and distributed processing",
            "Deep learning",
            "Big data",
            "Neural network",
            "Cluster computer",
            "HPCC systems"
        ],
        "number_of_pages": "23",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0179-2.pdf",
        "abstract": "Deep Learning is an increasingly important subdomain of artificial intelligence, which benefits from training on Big Data. The size and complexity of the model combined with the size of the training dataset makes the training process very computationally and temporally expensive. Accelerating the training process of Deep Learning using cluster computers faces many challenges ranging from distributed optimizers to the large communication overhead specific to systems with off the shelf networking components. In this paper, we present a novel distributed and parallel implementation of stochastic gradient descent (SGD) on a distributed cluster of commodity computers. We use high-performance computing cluster (HPCC) systems as the underlying cluster environment for the implementation. We overview how the HPCC systems platform provides the environment for distributed and parallel Deep Learning, how it provides a facility to work with third party open source libraries such as TensorFlow, and detail our use of third-party libraries and HPCC functionality for implementation. We pro-vide experimental results that validate our work and show that our implementation can scale with respect to both dataset size and the number of compute nodes in the cluster."
    },
    {
        "url": "https://paperity.org/p/185706871/a-survey-on-data-storage-and-placement-methodologies-for-cloud-big-data-ecosystem",
        "title": "A survey on data storage and placement methodologies for Cloud-Big Data ecosystem",
        "authors": [
            "Somnath Mazumdar",
            " Daniel Seybold",
            " Kyriakos Kritikos"
        ],
        "date_article": "02-2019",
        "short_description": "Currently, the data to be explored and exploited by computing systems increases at an exponential rate. The massive amount of data or so-called “Big Data” put pressure on existing technologies for providing scalable, fast and efficient support. Recent applications and the current user support from multi-domain computing, assisted in migrating from data-centric to knowledge...",
        "keywords": [
            "Big Data",
            "Cloud",
            "Data models",
            "Data storage",
            "Placement"
        ],
        "number_of_pages": "37",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0178-3.pdf",
        "abstract": "Currently, the data to be explored and exploited by computing systems increases at an exponential rate. The massive amount of data or so-called “Big Data” put pressure on existing technologies for providing scalable, fast and efficient support. Recent applications and the current user support from multi-domain computing, assisted in migrating from data-centric to knowledge-centric computing. However, it remains a challenge to optimally store and place or migrate such huge data sets across data cent-ers (DCs). In particular, due to the frequent change of application and DC behaviour (i.e., resources or latencies), data access or usage patterns need to be analyzed as well. Primarily, the main objective is to find a better data storage location that improves the overall data placement cost as well as the application performance (such as through-put). In this survey paper, we are providing a state of the art overview of Cloud-centric Big Data placement together with the data storage methodologies. It is an attempt to highlight the actual correlation between these two in terms of better supporting Big Data management. Our focus is on management aspects which are seen under the prism of non-functional properties. In the end, the readers can appreciate the deep analysis of respective technologies related to the management of Big Data and be guided towards their selection in the context of satisfying their non-functional applica-tion requirements. Furthermore, challenges are supplied highlighting the current gaps in Big Data management marking down the way it needs to evolve in the near future."
    },
    {
        "url": "https://paperity.org/p/185735624/identifying-and-characterizing-the-effects-of-calendar-and-environmental-conditions-on",
        "title": "Identifying and characterizing the effects of calendar and environmental conditions on pediatric admissions in Shanghai",
        "authors": [
            "Guang-jun Yu",
            " Jian-lei Gu",
            " Wen-bin Cui"
        ],
        "date_article": "02-2019",
        "short_description": "BackgroundGlobal environmental pollution caused by human activities has become a threat to public health. Children are especially susceptible to adverse environmental conditions owing to their unique physiological and behavioral characteristics. A number of studies have demonstrated associations between the incidence of some childhood diseases and adverse environmental conditions...",
        "keywords": [
            "Pediatric care",
            "Retrospective analysis",
            "Hospital admissions",
            "Generalized additive model",
            "Environment",
            "Shanghai",
            "China"
        ],
        "number_of_pages": "15",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0171-x.pdf",
        "abstract": "Background:Global environmental pollution caused by human activities has become a threat to public health. Children are especially susceptible to adverse environmen‑tal conditions owing to their unique physiological and behavioral characteristics. A number of studies have demonstrated associations between the incidence of some childhood diseases and adverse environmental conditions. Shanghai is the largest and most important economic center in China. After rapid population expansion in recent decades, the shortage of pediatric medical resources is becoming a serious public health problem. This study aimed to identify and characterize the social and environ‑mental effect of adverse environmental conditions on overall pediatric admissions at hospitals in Shanghai, China.Methods:This was a multi‑center study spanning from January, 2013 to November, 2014. Daily pediatric admission data (~ 12,000 overall pediatric admissions/day) of three tertiary pediatric hospitals were collected from the large‑scale health information exchange network of Shanghai. We linked the admission data with local environmental data. A seasonal decomposition method was applied to a time‑trend analysis of the admission data; a generalized additive model was applied to model the association between environmental measurements and admissions data.Results:Admissions to outpatient and emergency departments were highly influ‑enced by calendar factors; however, these same factors showed opposite effects on different clinical departments. The effect of nitrogen dioxide was a 0.27% increase (95% confidence interval (CI) 0.23% to 0.32%) in outpatient admissions and 0.78% (95% CI 0.68% to 0.88%) increase in emergency admissions. Concentrations of fine particles ≤2.5 micrometers in diameter (PM2.5) and carbon monoxide (CO) showed multi‑fac‑eted effects on pediatric admissions. PM2.5 and CO concentrations were significantly associated with decreased current‑day outpatient admissions but also significantly associated with increased current‑day emergency admissions at all three hospitals.Conclusions:Based on the health information exchange network of Shanghai, we conducted a large‑scale, multi‑center retrospective study of the association between adverse environmental conditions and pediatric admissions. Our study contributes to environmental health research in children and may guide decision‑making regarding pediatric resource planning and policies.Open Access©  The  Author(s)  2019.  This  article  is  distributed  under  the  terms  of  the  Creative  Commons  Attribution  4.0  International  License  (http://creat iveco mmons .org/licen ses/by/4.0/),  which  permits  unrestricted  use,  distribution,  and  reproduction  in  any  medium,  provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made.RESEARCHYu et al. J Big Data            (2019) 6:14  https://doi.org/10.1186/s40537-019-0171-x*Correspondence:   yu_guangjun@hotmail.com 1 Department of Children’s Healthcare, Shanghai Children’s Hospital, Shanghai Jiao Tong University, 355 Luding Road, Putuo District, Shanghai 200062, ChinaFull list of author information is available at the end of the article\nPage 2 of 15Yu et al. J Big Data            (2019) 6:14 "
    },
    {
        "url": "https://paperity.org/p/185764377/analysis-of-diabetes-mellitus-for-early-prediction-using-optimal-features-selection",
        "title": "Analysis of diabetes mellitus for early prediction using optimal features selection",
        "authors": [
            "N. Sneha",
            " Tarun Gangi"
        ],
        "date_article": "02-2019",
        "short_description": "Diabetes is a chronic disease or group of metabolic disease where a person suffers from an extended level of blood glucose in the body, which is either the insulin production is inadequate, or because the body’s cells do not respond properly to insulin. The constant hyperglycemia of diabetes is related to long-haul harm, brokenness, and failure of various organs, particularly the...",
        "keywords": [
            "Data mining",
            "Big Data",
            "Diabetes",
            "Naive Bayesian",
            "SVM"
        ],
        "number_of_pages": "19",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0175-6.pdf",
        "abstract": "Diabetes is a chronic disease or group of metabolic disease where a person suffers from an extended level of blood glucose in the body, which is either the insulin production is inadequate, or because the body’s cells do not respond properly to insulin. The con-stant hyperglycemia of diabetes is related to long-haul harm, brokenness, and failure of various organs, particularly the eyes, kidneys, nerves, heart, and veins. The objective of this research is to make use of significant features, design a prediction algorithm using Machine learning and find the optimal classifier to give the closest result comparing to clinical outcomes. The proposed method aims to focus on selecting the attributes that ail in early detection of Diabetes Miletus using Predictive analysis. The result shows the decision tree algorithm and the Random forest has the highest specificity of 98.20% and 98.00%, respectively holds best for the analysis of diabetic data. Naïve Bayesian outcome states the best accuracy of 82.30%. The research also generalizes the selec-tion of optimal features from dataset to improve the classification accuracy."
    },
    {
        "url": "https://paperity.org/p/185592362/big-data-and-discrimination-perils-promises-and-solutions-a-systematic-review",
        "title": "Big Data and discrimination: perils, promises and solutions. A systematic review",
        "authors": [
            "Maddalena Favaretto",
            " Eva De Clercq",
            " Bernice Simone Elger"
        ],
        "date_article": "02-2019",
        "short_description": "BackgroundBig Data analytics such as credit scoring and predictive analytics offer numerous opportunities but also raise considerable concerns, among which the most pressing is the risk of discrimination. Although this issue has been examined before, a comprehensive study on this topic is still lacking. This literature review aims to identify studies on Big Data in relation to...",
        "keywords": [
            "Big Data",
            "Data analytics",
            "Unfair discrimination",
            "Disparity",
            "Inequality",
            "Ethics"
        ],
        "number_of_pages": "27",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0177-4.pdf",
        "abstract": "Background:Big Data analytics such as credit scoring and predictive analytics offer numerous opportunities but also raise considerable concerns, among which the most pressing is the risk of discrimination. Although this issue has been examined before, a comprehensive study on this topic is still lacking. This literature review aims to identify studies on Big Data in relation to discrimination in order to (1) understand the causes and consequences of discrimination in data mining, (2) identify barriers to fair data-mining and (3) explore potential solutions to this problem.Methods:Six databases were systematically searched (between 2010 and 2017): PsychINDEX, SocIndex, PhilPapers, Cinhal, Pubmed and Web of Science.Results:Most of the articles addressed the potential risk of discrimination of data mining technologies in numerous aspects of daily life (e.g. employment, marketing, credit scoring). The majority of the papers focused on instances of discrimination related to historically vulnerable categories, while others expressed the concern that scoring systems and predictive analytics might introduce new forms of discrimination in sectors like insurance and healthcare. Discriminatory consequences of data mining were mainly attributed to human bias and shortcomings of the law; therefore sug-gested solutions included comprehensive auditing strategies, implementation of data protection legislation and transparency enhancing strategies. Some publications also highlighted positive applications of Big Data technologies.Conclusion:This systematic review primarily highlights the need for additional empiri-cal research to assess how discriminatory practices are both voluntarily and acciden-tally emerging from the increasing use of data analytics in our daily life. Moreover, since the majority of papers focused on the negative discriminative consequences of Big Data, more research is needed on the potential positive uses of Big Data with regards to social disparity."
    },
    {
        "url": "https://paperity.org/p/185387724/data-mining-approach-for-predicting-the-daily-internet-data-traffic-of-a-smart-university",
        "title": "Data mining approach for predicting the daily Internet data traffic of a smart university",
        "authors": [
            "Aderibigbe Israel Adekitan",
            " Jeremiah Abolade",
            " Olamilekan Shobayo"
        ],
        "date_article": "02-2019",
        "short_description": "Internet traffic measurement and analysis generate dataset that are indicators of usage trends, and such dataset can be used for traffic prediction via various statistical analyses. In this study, an extensive analysis was carried out on the daily internet traffic data generated from January to December, 2017 in a smart university in Nigeria. The dataset analysed contains seven...",
        "keywords": [
            "Machine learning",
            "Data mining",
            "Nigerian university",
            "Internet data traffic",
            "Network operations monitoring",
            "Pattern recognition models"
        ],
        "number_of_pages": "23",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0176-5.pdf",
        "abstract": "Internet traffic measurement and analysis generate dataset that are indicators of usage trends, and such dataset can be used for traffic prediction via various statistical analy-ses. In this study, an extensive analysis was carried out on the daily internet traffic data generated from January to December, 2017 in a smart university in Nigeria. The dataset analysed contains seven key features: the month, the week, the day of the week, the daily IP traffic for the previous day, the average daily IP traffic for the two previous days, the traffic status classification (TSC) for the download and the TSC for the upload inter-net traffic data. The data mining analysis was performed using four learning algorithms: the Decision Tree, the Tree Ensemble, the Random Forest, and the Naïve Bayes Algo-rithm on KNIME (Konstanz Information Miner) data mining application and kNN, Neural Network, Random Forest, Naïve Bayes and CN2 Rule Inducer algorithms on the Orange platform. A comparative performance analysis for the models is presented using the confusion matrix, Cohen’s Kappa value, the accuracy of each model, Area under ROC Curve, etc. A minimum accuracy of 55.66% was observed for both the upload and the download IP data on the KNIME platform while minimum accuracies of 57.3% and 51.4% respectively were observed on the Orange platform."
    },
    {
        "url": "https://paperity.org/p/185358971/a-quadri-dimensional-approach-for-poor-performance-prioritization-in-mobile-networks",
        "title": "A quadri-dimensional approach for poor performance prioritization in mobile networks using Big Data",
        "authors": [
            "Maluambanzila Minerve Mampaka",
            " Mbuyu Sumbwanyamb"
        ],
        "date_article": "02-2019",
        "short_description": "The Management of mobile networks has become so complex due to a huge number of devices, technologies and services involved. Network optimization and incidents management in mobile networks determine the level of the quality of service provided by the communication service providers (CSPs). Generally, the down time of a system and the time taken to repair [mean time to repair...",
        "keywords": [
            "Big Data",
            "QoS",
            "QoE",
            "MTTR",
            "Root cause analysis",
            "SQM",
            "CEM",
            "Mobile networks"
        ],
        "number_of_pages": "15",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0173-8.pdf",
        "abstract": "The Management of mobile networks has become so complex due to a huge number of devices, technologies and services involved. Network optimization and incidents management in mobile networks determine the level of the quality of service provided by the communication service providers (CSPs). Generally, the down time of a system and the time taken to repair [mean time to repair (MTTR)] has a direct impact on the revenue, especially on the operational expenditure (OPEX). A fast root cause analysis (RCA) mechanism is therefore crucial to improve the efficiency of the operational team within the CSPs. This paper proposes a quadri-dimensional approach (i.e. services, subscribers, handsets and cells) to build a service quality management (SQM) tree in a Big Data platform. This is meant to speed up the root cause analysis and prioritize the elements impacting the performance of the network. Two algorithms have been pro-posed; the first one, to normalize the performance indicators and the second one to build the SQM tree by aggregating the performance indicators for different dimensions to allow ranking and detection of tree paths with the worst performance. Addition-ally, the proposed approach will allow CSPs to detect the mobile network dimensions causing network issues in a faster way and protect their revenue while improving the quality of the service delivered."
    },
    {
        "url": "https://paperity.org/p/185416477/how-to-better-find-a-perpetrator-in-a-haystack",
        "title": "How to (better) find a perpetrator in a haystack",
        "authors": [
            "Yair Neuman",
            " Yochai Cohen",
            " Yiftach Neuman"
        ],
        "date_article": "02-2019",
        "short_description": "In many real-world contexts, there is a pressing need to automatically screen for potential perpetrators, such as school shooters, whose prevalence in the population is extremely low. We first explain one possible obstacle in addressing this challenge, which is the confusion between “recognition” and “localization” during a search process. Next, we present a pragmatic screening...",
        "keywords": [
            "Homeland security",
            "Lone wolf perpetrators",
            "Terrorism",
            "Screening",
            "Needle in a haystack",
            "Bayes Factor",
            "Jaynes"
        ],
        "number_of_pages": "17",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0172-9.pdf",
        "abstract": "In many real-world contexts, there is a pressing need to automatically screen for potential perpetrators, such as school shooters, whose prevalence in the population is extremely low. We first explain one possible obstacle in addressing this challenge, which is the confusion between “recognition” and “localization” during a search process. Next, we present a pragmatic screening methodology to the problem along Jaynes Bayesian hypothesis testing procedure. According to this approach, we should first focus our efforts on reducing the size of the haystack rather than on the identifica-tion of the needle. The third and major methodological contribution of the paper is in proposing that we may reduce the size of the haystack through the identification and use of unique data cues we describe as “impostors’ cues”. An experiment performed on an artificial data set of 7000 texts, shows that when incorporating these cues in the hypothesis testing procedure, they significantly improve the automatic screening of objects characterized by an attribute of a low prevalence (i.e. a psychopathic signa-ture). The relevance of the proposed approach for Big Data and Homeland security is explained and discussed."
    },
    {
        "url": "https://paperity.org/p/185531489/the-impact-of-colleges-and-hospitals-to-local-real-estate-markets",
        "title": "The impact of colleges and hospitals to local real estate markets",
        "authors": [
            "Ryan Rivas",
            " Dinesh Patil",
            " Vagelis Hristidis"
        ],
        "date_article": "01-2019",
        "short_description": "This paper studies how the presence of universities and hospitals influences local home prices and rents. We analyze the data on ZIP code level and on the level of individual homes. Our ZIP code-level analysis uses median home price data from 13,105 ZIP codes over 21 years and rent data from 15,918 ZIP codes over 7 years to compare a ZIP code’s appreciation, volatility and...",
        "keywords": [
            "Home prices",
            "Rent",
            "Appreciation",
            "Volatility",
            "University proximity",
            "Hospital proximity"
        ],
        "number_of_pages": "24",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0174-7.pdf",
        "abstract": "This paper studies how the presence of universities and hospitals influences local home prices and rents. We analyze the data on ZIP code level and on the level of indi-vidual homes. Our ZIP code-level analysis uses median home price data from 13,105 ZIP codes over 21 years and rent data from 15,918 ZIP codes over 7 years to compare a ZIP code’s appreciation, volatility and vacancies to the size of a university or hospi-tal within that ZIP code. Our home-level analysis uses data from 2,786,895 homes for sale and 267,486 homes for rent to study the impact of the distance from the nearest university or hospital to individual home prices. While our results generally agree with our expectations that larger, closer institutions yield higher prices, we also find some interesting results that challenge these expectations, such as positive correlations between volatility and university/hospital size in some ZIP codes, a positive correlation between rent and distance from a hospital for some homes, and lower correlations of rent vs. distance from a university compared to price vs. distance."
    },
    {
        "url": "https://paperity.org/p/185502736/investigating-the-adoption-of-big-data-analytics-in-healthcare-the-moderating-role-of",
        "title": "Investigating the adoption of big data analytics in healthcare: the moderating role of resistance to change",
        "authors": [
            "Muhammad Shahbaz",
            " Changyuan Gao",
            " LiLi Zhai"
        ],
        "date_article": "01-2019",
        "short_description": "Big data analytics is gaining substantial attention due to its innovative contribution to decision making and strategic development across the healthcare field. Therefore, this study explored the adoption mechanism of big data analytics in healthcare organizations to inspect elements correlated to behavioral intention using the technology acceptance model and task-technology fit...",
        "keywords": [
            "Big data analytics",
            "Healthcare",
            "Trust"
        ],
        "number_of_pages": "20",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0170-y.pdf",
        "abstract": "Big data analytics is gaining substantial attention due to its innovative contribution to decision making and strategic development across the healthcare field. Therefore, this study explored the adoption mechanism of big data analytics in healthcare organiza-tions to inspect elements correlated to behavioral intention using the technology acceptance model and task-technology fit paradigm. Using a survey questionnaire, we analyzed 224 valid responses in AMOS v21 to test the hypotheses. Our results posit that the credentials of the technology acceptance model together with task-technology fit contribute substantially to the enhancement of behavioral intentions to use the big data analytics system in healthcare, ultimately leading towards actual use. Meanwhile, trust in and security of the information system also positively influenced the behavioral intention for use. Employee resistance to change is a key factor underlying failure of the innovative system in organizations and has been proven in this study to nega-tively moderate the relationship between intention to use and actual use of big data analytics in healthcare. Our results can be implemented by healthcare organizations to develop an understanding of the implementation of big data analytics and to promote psychological empowerment of employees to accept this innovative system."
    },
    {
        "url": "https://paperity.org/p/185473983/data-mining-combined-to-the-multicriteria-decision-analysis-for-the-improvement-of-road",
        "title": "Data mining combined to the multicriteria decision analysis for the improvement of road safety: case of France",
        "authors": [
            "Fatima Zahra El Mazouri",
            " Mohammed Chaouki Abounaima",
            " Khalid Zenkouar"
        ],
        "date_article": "01-2019",
        "short_description": "IntroductionThe problem studied in this paper is the road insecurity, which is manifested by the big number of injuries and deaths recorded annually around the world. These victims of road accidents worry the whole community, hence the duty and the need to find solutions for reducing the number of victims and material damage. The overall purpose of this case study is to treat the...",
        "keywords": [
            "Data mining",
            "Multicriteria decision aid",
            "Association rules",
            "Apriori algorithm",
            "ELECTRE II method",
            "Road safety"
        ],
        "number_of_pages": "30",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-018-0165-0.pdf",
        "abstract": "Introduction:  The problem studied in this paper is the road insecurity, which is mani-fested by the big number of injuries and deaths recorded annually around the world. These victims of road accidents worry the whole community, hence the duty and the need to find solutions for reducing the number of victims and material damage. The overall purpose of this case study is to treat the problem of corporeal accidents known in France.Case description:  The case study presented in this paper is intended to help decision-makers to find and understand the all significant relationships and correlations that exist between the conditions that led to these corporeal accidents. In fact, the two French ministries of interior and transport have jointly created a unique database on corporeal accidents, called BAAC “Accident Analysis Bulletin Corporal”, with the aim of allowing different exploitations of this database by different concerned administra-tions and research organizations. Our intervention consists to adopt a hybrid approach based on data mining techniques combined to the multicriteria decision methods. This approach allows to extract the most relevant association rules. The results thus obtained can be easily exploited by the decision-makers to choose the appropriate policies in the perspective of improving road safety.Discussion and evaluation:  The proposed approach ranks all association rules in order of importance for several quality measures of association rules. The alternatives at the top of the ranking are the results to retaining for the analysis. The approach is applied to the BAAC database of 2016, which led to the selection of three association rules. These association rules reveal that there are narrow correlations between the fol-lowing elements: Driver with Pedestrian, Normal Surface of Road with Normal Atmos-pheric Condition and the Pavement with Pedestrian. These correlations can be justified by excess speed and carelessness of the drivers.Conclusion:  The improvement of the road safety needs mainly to work more inten-sively on the behavioral side of the road users. For future work, it is planned to apply the proposed approach to the French traffic accident database, containing all the data on road accidents collected over several years. In addition, other measures will be used in the ranking of the association rules."
    },
    {
        "url": "https://paperity.org/p/185445230/knowledge-discovery-from-a-more-than-a-decade-studies-on-healthcare-big-data-systems-a",
        "title": "Knowledge discovery from a more than a decade studies on healthcare Big Data systems: a scientometrics study",
        "authors": [
            "Fatemeh Soleimani-Roozbahani",
            " Ali Rajabzadeh Ghatari",
            " Reza Radfar"
        ],
        "date_article": "01-2019",
        "short_description": "Annually, lots of research papers are published in scientific journals around the world. The knowledge of the status of research is a prerequisite for research planning and policy making. This type of knowledge could be gained through a scientometrics study on the published literature that analyzes research products in a scientific field. Always healthcare was a permanent concern...",
        "keywords": [
            "Big Data",
            "Healthcare",
            "Knowledge discovery",
            "Scientometrics study",
            "Naïve Bayes",
            "Published papers"
        ],
        "number_of_pages": "15",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-018-0167-y.pdf",
        "abstract": "Annually, lots of research papers are published in scientific journals around the world. The knowledge of the status of research is a prerequisite for research planning and policy making. This type of knowledge could be gained through a scientometrics study on the published literature that analyzes research products in a scientific field. Always healthcare was a permanent concern of researchers and also rapidly expanding field of Big Data analytics has started to play a pivotal role in the evolution of healthcare prac‑tices and research. It leads attracting attention from academia, industry and even gov‑ernments around the world to “Big data in Healthcare”. Therefore, this paper has done a meta‑analysis on published researches methodology in this field in the period of 2008–2018. Statistical finding shows the “Meta‑analysis and evidence” is the most used methodology in published papers. We applied data mining techniques for predicting using methodologies in the various databases to achieving knowledge discovery in the field. Naïve Bayes classifier in RapidMiner has been applied and results show eight main categories for words used in papers while “Developing methods to evaluate of care” averagely is the most intended using methodology for publishing papers and “Agent‑based modeling” in nature is most using methodology and could be better predicted."
    },
    {
        "url": "https://paperity.org/p/185588995/detecting-and-understanding-urban-changes-through-decomposing-the-numbers-of-visitors",
        "title": "Detecting and understanding urban changes through decomposing the numbers of visitors’ arrivals using human mobility data",
        "authors": [
            "Takashi Nicholas Maeda",
            " Narushige Shiode",
            " Chen Zhong"
        ],
        "date_article": "01-2019",
        "short_description": "In recent years, mobility data from smart cards, mobile phones and sensors have become increasingly available. However, they often lack some of the key information including the purposes of trips for each individual user. Information on trip purposes is crucial for projecting the future travel patterns as well as understanding the characteristics of each area of a city and how it...",
        "keywords": [
            "Change detection",
            "Human mobility",
            "Non-negative matrix factorization",
            "Public transportation"
        ],
        "number_of_pages": "25",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0168-5.pdf",
        "abstract": "In recent years, mobility data from smart cards, mobile phones and sensors have become increasingly available. However, they often lack some of the key information including the purposes of trips for each individual user. Information on trip purposes is crucial for projecting the future travel patterns as well as understanding the charac-teristics of each area of a city and how it is changing. This paper proposes a method called EAT-CD (Extraction of Activity Types and Change Detection). It estimates the volume of passengers by activity types (e.g. commuting, leisure) using non-negative matrix factorization and detects changes in the number of visitors for each activity (e.g. increase in shopping trips triggered by the development of a new commercial facility). Validity of EAT-CD is tested through empirical analysis using smart card data of public transportation in Western Japan. The results showed that EAT-CD is effective in deriving activity patterns, which showed strong correlation with travel survey data. The results also confirmed that EAT-CD detects changes in travel patterns (e.g. start and end of semesters) and land uses (e.g. establishment of new facilities)."
    },
    {
        "url": "https://paperity.org/p/185560242/predicting-referendum-results-in-the-big-data-era",
        "title": "Predicting referendum results in the Big Data Era",
        "authors": [
            "Amaryllis Mavragani",
            " Konstantinos P. Tsagarakis"
        ],
        "date_article": "01-2019",
        "short_description": "In addressing the challenge of Big Data Analytics, what has been of notable significance is the analysis of online search traffic data in order to analyze and predict human behavior. Over the last decade, since the establishment of the most popular such tool, Google Trends, the use of online data has been proven valuable in various research fields, including -but not limited to...",
        "keywords": [
            "Big data",
            "Elections",
            "Google Trends",
            "Internet behavior",
            "Nowcasting",
            "Online behavior",
            "Online queries",
            "Politics",
            "Prediction",
            "Referendum",
            "Voting"
        ],
        "number_of_pages": "20",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-018-0166-z.pdf",
        "abstract": "In addressing the challenge of Big Data Analytics, what has been of notable signifi-cance is the analysis of online search traffic data in order to analyze and predict human behavior. Over the last decade, since the establishment of the most popular such tool, Google Trends, the use of online data has been proven valuable in various research fields, including -but not limited to- medicine, economics, politics, the environment, and behavior. In the field of politics, given the inability of poll agencies to always well approximate voting intentions and results over the past years, what is imperative is to find new methods of predicting elections and referendum outcomes. This paper aims at presenting a methodology of predicting referendum results using Google Trends; a method applied and verified in six separate occasions: the 2014 Scottish Referendum, the 2015 Greek Referendum, the 2016 UK Referendum, the 2016 Hungarian Referen-dum, the 2016 Italian Referendum, and the 2017 Turkish Referendum. Said referendums were of importance for the respective country and the EU as well, and received wide international attention. Google Trends has been empirically verified to be a tool that can accurately measure behavioral changes as it takes into account the users’ revealed and not the stated preferences. Thus we argue that, in the time of intelligence excess, Google Trends can well address the analysis of social changes that the internet brings."
    },
    {
        "url": "https://paperity.org/p/185355604/large-scale-e-learning-recommender-system-based-on-spark-and-hadoop",
        "title": "Large-scale e-learning recommender system based on Spark and Hadoop",
        "authors": [
            "Karim Dahdouh",
            " Ahmed Dakkak",
            " Lahcen Oughdir"
        ],
        "date_article": "01-2019",
        "short_description": "The present work is a part of the ESTenLigne project which is the result of several years of experience for developing e-learning in Sidi Mohamed Ben Abdellah University through the implementation of open, online and adaptive learning environment. However, this platform faces many challenges, such as the increasing amount of data, the diversity of pedagogical resources and a...",
        "keywords": [
            "Big data",
            "Spark",
            "Hadoop",
            "E-learning",
            "Online learning",
            "Course recommender system",
            "MLlib methods",
            "Association rules",
            "Parallel FP-growth algorithm"
        ],
        "number_of_pages": "23",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0169-4.pdf",
        "abstract": "The present work is a part of the ESTenLigne project which is the result of several years of experience for developing e-learning in Sidi Mohamed Ben Abdellah University through the implementation of open, online and adaptive learning environment. How-ever, this platform faces many challenges, such as the increasing amount of data, the diversity of pedagogical resources and a large number of learners that makes harder to find what the learners are really looking for. Furthermore, most of the students in this platform are new graduates who have just come to integrate higher education and who need a system to help them to take the relevant courses that take into account the requirements and needs of each learner. In this article, we develop a distributed courses recommender system for the e-learning platform. It aims to discover relation-ships between student’s activities using association rules method in order to help the student to choose the most appropriate learning materials. We also focus on the analysis of past historical data of the courses enrollments or log data. The article dis-cusses particularly the frequent itemsets concept to determine the interesting rules in the transaction database. Then, we use the extracted rules to find the catalog of more suitable courses according to the learner’s behaviors and preferences. Next, we deploy our recommender system using big data technologies and techniques. Especially, we implement parallel FP-growth algorithm provided by Spark Framework and Hadoop ecosystem. The experimental results show the effectiveness and scalability of the pro-posed system. Finally, we evaluate the performance of Spark MLlib library compared to traditional machine learning tools including Weka and R."
    },
    {
        "url": "https://paperity.org/p/185384357/manufacturing-process-data-analysis-pipelines-a-requirements-analysis-and-survey",
        "title": "Manufacturing process data analysis pipelines: a requirements analysis and survey",
        "authors": [
            "Ahmed Ismail",
            " Hong-Linh Truong",
            " Wolfgang Kastner"
        ],
        "date_article": "01-2019",
        "short_description": "Smart manufacturing is strongly correlated with the digitization of all manufacturing activities. This increases the amount of data available to drive productivity and profit through data-driven decision making programs. The goal of this article is to assist data engineers in designing big data analysis pipelines for manufacturing process data. Thus, this paper characterizes the...",
        "keywords": [
            "Big data",
            "Smart manufacturing",
            "Industry 4.0",
            "Analysis pipelines",
            "Industrial Internet of Things",
            "Data-driven decision making",
            "High performance computing"
        ],
        "number_of_pages": "26",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-018-0162-3.pdf",
        "abstract": "Smart manufacturing is strongly correlated with the digitization of all manufacturing activities. This increases the amount of data available to drive productivity and profit through data‑driven decision making programs. The goal of this article is to assist data engineers in designing big data analysis pipelines for manufacturing process data. Thus, this paper characterizes the requirements for process data analysis pipelines and surveys existing platforms from academic literature. The results demonstrate a stronger focus on the storage and analysis phases of pipelines than on the ingestion, communi‑cation, and visualization stages. Results also show a tendency towards custom tools for ingestion and visualization, and relational data tools for storage and analysis. Tools for handling heterogeneous data are generally well‑represented throughout the pipeline. Finally, batch processing tools are more widely adopted than real‑time stream process‑ing frameworks, and most pipelines opt for a common script‑based data processing approach. Based on these results, recommendations are offered for each phase of the pipeline."
    },
    {
        "url": "https://paperity.org/p/181580091/the-affordance-of-virtual-reality-to-enable-the-sensory-representation-of-multi",
        "title": "The affordance of virtual reality to enable the sensory representation of multi-dimensional data for immersive analytics: from experience to insight",
        "authors": [
            "Jules Moloney",
            " Branka Spehar",
            " Anastasia Glob"
        ],
        "date_article": "12-2018",
        "short_description": "Using the theory of affordance from perceptual psychology and through discussion of literature within visual data mining and immersive analytics, a position for the multi-sensory representation of big data using virtual reality (VR) is developed. While it would seem counter intuitive, information-dense virtual environments are theoretically easier to process than simplified...",
        "keywords": [
            "Virtual reality",
            "Affordance",
            "Big data",
            "Multisensory representation",
            "Human interaction"
        ],
        "number_of_pages": "19",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-018-0158-z.pdf",
        "abstract": "Using the theory of affordance from perceptual psychology and through discussion of literature within visual data mining and immersive analytics, a position for the multi-sensory representation of big data using virtual reality (VR) is developed. While it would seem counter intuitive, information-dense virtual environments are theoretically easier to process than simplified graphic encoding—if there is alignment with human eco-logical perception of natural environments. Potentially, VR affords insight into patterns and anomalies through dynamic experience of data representations within interactive, kinaesthetic audio-visual virtual environments. To this end we articulate principles that can inform the development of VR applications for immersive analytics: a mimetic approach to data mapping that aligns spatial, aural and kinaesthetic attributes with abstractions of natural environments; layered with constructed features that comple-ment natural structures; the use of cross-modal sensory mapping; a focus on interme-diate levels of contrast; and the adaptation of naturally occurring distribution patterns for the granularity and distribution of data. While it appears problematic to directly translate visual data mining techniques to VR, the ecological approach to human perception discussed in this article provides a new framework for big data visualization researchers to consider."
    },
    {
        "url": "https://paperity.org/p/181608844/pairwise-document-similarity-measure-based-on-present-term-set",
        "title": "Pairwise document similarity measure based on present term set",
        "authors": [
            "Marzieh Oghbaie",
            " Morteza Mohammadi Zanjireh"
        ],
        "date_article": "12-2018",
        "short_description": "Measuring pairwise document similarity is an essential operation in various text mining tasks. Most of the similarity measures judge the similarity between two documents based on the term weights and the information content that two documents share in common. However, they are insufficient when there exist several documents with an identical degree of similarity to a particular...",
        "keywords": [
            "Similarity measure",
            "Distance metric",
            "Document clustering",
            "Document classification",
            "Near-duplicates detection",
            "Information retrieval"
        ],
        "number_of_pages": "23",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-018-0163-2.pdf",
        "abstract": "Measuring pairwise document similarity is an essential operation in various text mining tasks. Most of the similarity measures judge the similarity between two documents based on the term weights and the information content that two documents share in common. However, they are insufficient when there exist several documents with an identical degree of similarity to a particular document. This paper introduces a novel text document similarity measure based on the term weights and the number of terms appeared in at least one of the two documents. The effectiveness of our measure is evaluated on two real-world document collections for a variety of text mining tasks, such as text document classification, clustering, and near-duplicates detection. The performance of our measure is compared with that of some popular measures. The experimental results showed that our proposed similarity measure yields more accu-rate results."
    },
    {
        "url": "https://paperity.org/p/179675184/prediction-and-analysis-of-indonesia-presidential-election-from-twitter-using-sentiment",
        "title": "Prediction and analysis of Indonesia Presidential election from Twitter using sentiment analysis",
        "authors": [
            "Widodo Budiharto",
            " Meiliana Meilian"
        ],
        "date_article": "12-2018",
        "short_description": "Big data encompasses social networking websites including Twitter as popular micro-blogging social media platform for a political campaign. The explosive Twitter data as a respond of the political campaign can be used to predict the Presidential election as has been conducted to predict the political election in several countries such as US, UK, Spain, and French. The authors use...",
        "keywords": [
            "Sentiment analysis",
            "Twitter",
            "Presidential election",
            "Prediction"
        ],
        "number_of_pages": "10",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-018-0164-1.pdf",
        "abstract": "Big data encompasses social networking websites including Twitter as popular micro-blogging social media platform for a political campaign. The explosive Twitter data as a respond of the political campaign can be used to predict the Presidential election as has been conducted to predict the political election in several countries such as US, UK, Spain, and French. The authors use tweets from President Candidates of Indone-sia (Jokowi and Prabowo), and tweets from relevant hashtags for sentiment analysis gathered from March to July 2018 to predict Indonesian Presidential election result. The authors make an algorithm and method to count important data, top words and train the model and predict the polarity of the sentiment. The experimental result is produced by using R language and show that Jokowi leads the current election predic-tion. This prediction result is corresponding to four survey institutes in Indonesia that proved our method had produced reliable prediction results."
    },
    {
        "url": "https://paperity.org/p/179646431/a-new-effective-method-for-labeling-dynamic-xml-data",
        "title": "A new effective method for labeling dynamic XML data",
        "authors": [
            "Eynollah Khanjari",
            " Leila Gaeini"
        ],
        "date_article": "12-2018",
        "short_description": "Query processing based on labeling dynamic XML documents has gained more attention in the past several years. An efficient labeling scheme should provide small size labels keeping the simplicity of the exploited algorithm in order to avoid complex computations as well as retaining the readability of structural relationships between nodes. Moreover, for dynamic XML data...",
        "keywords": [
            "XML labeling scheme",
            "Dynamic labeling",
            "XML query processing",
            "XML updates"
        ],
        "number_of_pages": "17",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-018-0161-4.pdf",
        "abstract": "Query processing based on labeling dynamic XML documents has gained more attention in the past several years. An efficient labeling scheme should provide small size labels keeping the simplicity of the exploited algorithm in order to avoid complex computations as well as retaining the readability of structural relationships between nodes. Moreover, for dynamic XML data, relabeling the nodes in XML updates should be avoided. However, the existing schemes lack the capability of supporting all of these requirements. In this paper, we propose a new labeling scheme which assigns variable-length labels to nodes in dynamic XML documents. Our method employs the FibLSS encoding scheme that exploits the properties of the Fibonacci sequence to provide variable-length node labels of appropriate size. In XML updating process, we add a new section only in the new node’s label without relabeling the existing nodes while keeping the order of nodes as well as preserving the structural relationships. Our labeling method is scalable as it is not subject to overflow, and as the number of nodes to be labeled increases exponentially, the size of labels grows linearly, which makes it suitable for big datasets. It also has the best performance in computational processing costs compared to existing approaches. The results of the experiments confirm the advantages of our proposed method in comparison to state-of-the-art techniques."
    },
    {
        "url": "https://paperity.org/p/177683976/building-efficient-fuzzy-regression-trees-for-large-scale-and-high-dimensional-problems",
        "title": "Building efficient fuzzy regression trees for large scale and high dimensional problems",
        "authors": [
            "Javier Cózar",
            " Francesco Marcelloni",
            " José A. Gámez"
        ],
        "date_article": "12-2018",
        "short_description": "Regression trees (RTs) are simple, but powerful models, which have been widely used in the last decades in different scopes. Fuzzy RTs (FRTs) add fuzziness to RTs with the aim of dealing with uncertain environments. Most of the FRT learning approaches proposed in the literature aim to improve the accuracy, measured in terms of mean squared error, and often neglect to consider the...",
        "keywords": [
            "Fuzzy regression trees",
            "Big Data",
            "Fuzzy discretizer",
            "Apache Spark"
        ],
        "number_of_pages": "25",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-018-0159-y.pdf",
        "abstract": "Regression trees (RTs) are simple, but powerful models, which have been widely used in the last decades in different scopes. Fuzzy RTs (FRTs) add fuzziness to RTs with the aim of dealing with uncertain environments. Most of the FRT learning approaches proposed in the literature aim to improve the accuracy, measured in terms of mean squared error, and often neglect to consider the computation time and/or the memory requirements. In today’s application domains, which require the management of huge amounts of data, this carelessness can strongly limit their use. In this paper, we propose a distributed FRT (DFRT ) learning scheme for generating binary RTs from big datasets, that is based on the MapReduce paradigm. We have designed and implemented the scheme on the Apache Spark framework. We have used eight real-world and four synthetic datasets for evaluating its performance, in terms of mean squared error, computation time and scalability. As a baseline, we have compared the results with the distributed RT (DRT ) and the Distributed Random Forest (DRF) available in the Spark MLlib library. Results show that our DFRT scales similarly to DRT and better than DRF. Regarding the performance, DFRT generalizes much better than DRT and similarly to DRF."
    },
    {
        "url": "https://paperity.org/p/175823173/ann-based-short-term-traffic-flow-forecasting-in-undivided-two-lane-highway",
        "title": "ANN based short-term traffic flow forecasting in undivided two lane highway",
        "authors": [
            "Bharti Sharma",
            " Sachin Kumar",
            " Prayag Tiwari"
        ],
        "date_article": "12-2018",
        "short_description": "Short term traffic forecasting is one of the important fields of study in the transportation domain. Short term traffic forecasting is very useful to develop a more advanced transportation system to control traffic signals and avoid congestions. Several studies have made efforts for short term traffic flow forecasting for divided and undivided highways across the world. However...",
        "keywords": [
            "Traffic density",
            "Multilayer perceptron with back-propagation",
            "Multi-class traffic",
            "Statistical indices",
            "Traffic flow"
        ],
        "number_of_pages": "16",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-018-0157-0.pdf",
        "abstract": "Short term traffic forecasting is one of the important fields of study in the transporta-tion domain. Short term traffic forecasting is very useful to develop a more advanced transportation system to control traffic signals and avoid congestions. Several studies have made efforts for short term traffic flow forecasting for divided and undivided highways across the world. However, all these studies relied on the dataset which are greatly varied between countries due to the technology used for transportation data collection. India is a developing country in which efforts are being done to improve the transportation system to avoid congestion and travel time. Two-lane undivided high-ways with mixed traffic constitute a large portion of Indian road network. This study is an attempt to develop a short term traffic forecasting model using back propagation artificial neural network for two lane undivided highway with mixed traffic conditions in India. The results were compared with random forest, support vector machine, k-nearest neighbor classifier, regression tree and multiple regression models. It was found that back-propagation neural network performs better than other approaches and achieved an  R2 value 0.9962, which is a good score."
    },
    {
        "url": "https://paperity.org/p/175851926/a-method-of-trend-forecasting-for-financial-and-geopolitical-data-inferring-the-effects",
        "title": "A method of trend forecasting for financial and geopolitical data: inferring the effects of unknown exogenous variables",
        "authors": [
            "Lucas Cassiel Jacaruso"
        ],
        "date_article": "12-2018",
        "short_description": "This paper intends to contribute to the field of trend forecasting by proposing a new forecasting approach for stock market prices and geopolitical time series data of economic, financial and geopolitical importance. Designing models which account for every possible exogenous variable of relevance to a time series in question can often be an onerous and impractical task. Instead...",
        "keywords": [
            "Exogenous variables",
            "Endogenous variables",
            "Trend extrapolation forecasting",
            "Time series",
            "Stock closing prices",
            "Financial securities prices",
            "Trading volume",
            "Asylum seekers",
            "Refugees"
        ],
        "number_of_pages": "14",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-018-0160-5.pdf",
        "abstract": "This paper intends to contribute to the field of trend forecasting by proposing a new forecasting approach for stock market prices and geopolitical time series data of economic, financial and geopolitical importance. Designing models which account for every possible exogenous variable of relevance to a time series in question can often be an onerous and impractical task. Instead, this paper explores a new method which uses periods of decreased significance in the variable of foremost importance as a window of opportunity to observe the possible effects other variables may be having in a general way for the purpose of trend forecasting. When the latter variables are too unquantifiable to be accounted for in a model, having the ability to nonetheless dis-cern their overall influence can be useful for anticipating trend changes. The proposed method was used in conjunction with the existing method of exponential smoothing to generate forecasts. It was also applied alone and contrasted with the results of expo-nential smoothing when used separately. This paper specifically addresses the ability of the newly proposed method to forecast the upwards/downwards extrapolation of the weekly trend for 9 weeks on stock closing prices for five companies of interest (Apple Inc, Amazon.com Inc, General Electric Company, Intel Corporation, and Alcoa Corpora-tion). It was also applied to forecasting the annual trend for 9 years of Afghan asylum seeker data. These differing areas were chosen in order to demonstrate applications in finance as well as international relations. The empirical results and 95% confidence intervals indicate a clear advantage when the newly proposed method is used both in conjunction with exponential smoothing and on its own."
    },
    {
        "url": "https://paperity.org/p/175880679/automatic-schema-suggestion-model-for-nosql-document-stores-databases",
        "title": "Automatic schema suggestion model for NoSQL document-stores databases",
        "authors": [
            "Abdullahi Abubakar Imam",
            " Shuib Basri",
            " Rohiza Ahmad"
        ],
        "date_article": "12-2018",
        "short_description": "New generation databases also called NoSQL (Not only SQL) databases are highly scalable, flexible, and low-latent. These types of databases emerge as a result of the rigidity shown by traditional databases to handle today’s data which is voluminous, highly diversified and generated at a very high rate. With NoSQL, problems such as database expansion difficulties, low query...",
        "keywords": [
            "NoSQL databases",
            "Big data",
            "Database modeling",
            "Schema suggestion",
            "Document-model databases",
            "Data security"
        ],
        "number_of_pages": "17",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-018-0156-1.pdf",
        "abstract": "New generation databases also called NoSQL (Not only SQL) databases are highly scal‑able, flexible, and low‑latent. These types of databases emerge as a result of the rigidity shown by traditional databases to handle today’s data which is voluminous, highly diversified and generated at a very high rate. With NoSQL, problems such as database expansion difficulties, low query performance and low storage capacity are addressed. However, the inherent complexity of contemporary datasets coupled with program‑mers’ low NoSQL modeling competence are increasingly making database modeling and design vastly challenging, especially when parameters like consistency, availability and scalability are to be balanced in accordance with system requirements. As such, a schema suggestion model for NoSQL databases is posed to address this balancing issue. The proposed model aims to abstractly suggest schemas at the initial stage of system development based on user defined system requirements and CRUD (Create, Read, Update and Delete) operations among others. This is achieved through the adap‑tation of exploratory and experimental approaches of research. Also, few mathemati‑cal formulas are introduced to calculate clusters availability during entity mappings. A comparison was conducted between the schema produced using the proposed model and the one without. Results obtained shows substantial improvement in the areas of security and read–write query performance."
    },
    {
        "url": "https://paperity.org/p/162814958/the-hiperwall-tiled-display-wall-system-for-big-data-research",
        "title": "The Hiperwall tiled-display wall system for Big-Data research",
        "authors": [
            "Muhammad Saleem",
            " Hugo E. Valle",
            " Stephen Brown"
        ],
        "date_article": "10-2018",
        "short_description": "In the era of Big Data, with the increasing use of large-scale data-driven applications, visualization of very large high-resolution images and extracting useful information (searching for specific targets or rare signal events) from these images can pose challenges to the current display wall technologies. At Bellarmine University, we have set up an Advanced Visualization and...",
        "keywords": [
            "Hiperwall",
            "High-resolution displays",
            "Information visualization",
            "Collaborative visualization",
            "Big data visualization",
            "Visual analytics"
        ],
        "number_of_pages": "45",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-018-0150-7.pdf",
        "abstract": "In the era of Big Data, with the increasing use of large-scale data-driven applications, visualization of very large high-resolution images and extracting useful information (searching for specific targets or rare signal events) from these images can pose chal-lenges to the current display wall technologies. At Bellarmine University, we have set up an Advanced Visualization and Computational Lab using a state-of-the-art next generation display wall technology, called Hiperwall (Highly Interactive Parallelized Dis-play Wall). The 16 ft ×4.5 ft Hiperwall visualization system has a total resolution of 16.5 Megapixels (MP) which consists of eight display-tiles that are arranged in a 4×2 tile configuration. Using Hiperwall, we can perform interactive visual data analytics of large images by conducting comparative views of multiple large images in Astronomy and multiple event displays in experimental High Energy Physics. Users can display a single large image across all the display-tiles, or view many different images simultaneously on multiple display-tiles. Hiperwall enables simultaneous visualization of multiple high resolution images and its contents on the entire display wall without loss of clarity and resolution. Hiperwall’s middleware also allows researchers in geographically diverse locations to collaborate on large scientific experiments. In this paper we will provide a description of a new generation of display wall setup at Bellarmine University that is based on the Hiperwall technology, which is a robust visualization system for Big Data research."
    },
    {
        "url": "https://paperity.org/p/162843711/chabok-a-map-reduce-based-method-to-solve-data-warehouse-problems",
        "title": "Chabok: a Map-Reduce based method to solve data warehouse problems",
        "authors": [
            "Mohammadhossein Barkhordari",
            " Mahdi Niamanesh"
        ],
        "date_article": "10-2018",
        "short_description": "Currently, immense quantities of data cannot be managed by traditional database management systems. Instead, they must be managed by big data solutions using shared nothing architectures. Data warehouse systems are systems that address very large amounts of information. The most prominent data warehouse model is star schema, which consists of a fact table and some number of...",
        "keywords": [
            "Big data",
            "MapReduce",
            "Data warehouse",
            "Data locality"
        ],
        "number_of_pages": "25",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-018-0144-5.pdf",
        "abstract": "Currently, immense quantities of data cannot be managed by traditional database management systems. Instead, they must be managed by big data solutions using shared nothing architectures. Data warehouse systems are systems that address very large amounts of information. The most prominent data warehouse model is star schema, which consists of a fact table and some number of dimension tables. It is nec-essary to join the facts and dimensions for query executions on the data warehouse. In shared nothing architecture, all of the required information is not placed on a single node so it is necessary to retrieve information from other nodes, which causes network congestion and low speeds of query execution. To avoid this problem and achieve maximum parallelism, dimensions can be replicated over nodes if they are not too large. However, if there are dimensions with data volumes greater than the capacity of a node or dimensions where the data volume summation exceeds node capacity, the query execution is confronted with serious problems. In big data problems, the amount of data is immense, and thus replicating immense data cannot be considered an appropriate method. In this paper, we propose a method called Chabok, which uses two-phased Map-Reduce to solve the data warehouse problem. In this method, aggregation is performed completely on Mappers, and intermediate results are sent to the Reducer. Chabok does not need data replication for join omission. The proposed method was implemented on Hadoop, and TPC-DS queries were executed for bench-marking. The query execution time on Chabok surpassed prominent big data products for data warehousing."
    },
    {
        "url": "https://paperity.org/p/162872464/tsim-a-system-for-discovering-similar-users-on-twitter",
        "title": "TSim: a system for discovering similar users on Twitter",
        "authors": [
            "Hind AlMahmoud",
            " Shurug AlKhalif"
        ],
        "date_article": "10-2018",
        "short_description": "This paper presents a framework for discovering similar users on Twitter that can be used in profiling users for social, recruitment and security reasons. The framework contains a novel formula that calculates the similarity between users on Twitter by using seven different signals (features). The signals are followings and followers, mention, retweet, favorite, common hashtag...",
        "keywords": [
            "Twitter",
            "MapReduce",
            "Similarity on social media",
            "Big data"
        ],
        "number_of_pages": "20",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-018-0147-2.pdf",
        "abstract": "This paper presents a framework for discovering similar users on Twitter that can be used in profiling users for social, recruitment and security reasons. The framework contains a novel formula that calculates the similarity between users on Twitter by using seven different signals (features). The signals are followings and followers, men-tion, retweet, favorite, common hashtag, common interests, and profile similarity. The proposed framework is scalable and can handle big data because it is implemented using the MapReduce paradigm. It is also adjustable since the weight and contribution of each signal in calculating the final similarity score is determined by the user based on their needs. The accuracy of the system was evaluated through human judges and by comparing the system’s results against Twitter’s Who To Follow service. The results show moderately accurate results."
    },
    {
        "url": "https://paperity.org/p/162929970/experimenting-sensitivity-based-anonymization-framework-in-apache-spark",
        "title": "Experimenting sensitivity-based anonymization framework in apache spark",
        "authors": [
            "Mohammed Al-Zobbi",
            " Seyed Shahrestani",
            " Chun Ruan"
        ],
        "date_article": "10-2018",
        "short_description": "One of the biggest concerns of big data and analytics is privacy. We believe the forthcoming frameworks and theories will establish several solutions for the privacy protection. One of the known solutions is the k-anonymity that was introduced for traditional data. Recently, two major frameworks leveraged big data processing and applications; these are MapReduce and Spark. Spark...",
        "keywords": [
            "Spark",
            "Anonymization",
            "Big data",
            "k-Anonymity",
            "MapReduce",
            "Sensitivity",
            "SQL spark"
        ],
        "number_of_pages": "26",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-018-0149-0.pdf",
        "abstract": "One of the biggest concerns of big data and analytics is privacy. We believe the forthcoming frameworks and theories will establish several solutions for the privacy protection. One of the known solutions is the k‑anonymity that was introduced for traditional data. Recently, two major frameworks leveraged big data processing and applications; these are MapReduce and Spark. Spark data processing has been attract‑ing more attention due to its crucial impacts on a wide range of big data applications. One of the predominant big data applications is data analytics and anonymization. We previously proposed an anonymization method for implementing k‑anonymity in MapReduce processing framework. In this paper, we investigate Spark performance in processing data anonymization. Spark is a fast processing framework that was imple‑mented in several applications such as: SQL, multimedia, and data stream. Our focus is the SQL Spark, which is adequate for big data anonymization. Since Spark operates in‑memory, we need to observe its limitations, speed, and fault tolerance on data size increase, and to compare MapReduce to Spark in processing anonymity. Spark intro‑duces an abstraction called resilient distributed datasets, which reads and serializes a collection of objects partitioned across a set of machines. Developers claim that Spark can outperform MapReduce by 10 times in iterative machine learning jobs. Our experi‑ments in this paper compare between MapReduce and Spark. The overall results show a better performance for Spark’s processing time in anonymity operations. However, in some limited cases, we prefer to implement the old MapReduce framework, when the cluster resources are limited and the network is non‑congested."
    },
    {
        "url": "https://paperity.org/p/162901217/joint-index-vector-a-novel-assessment-measure-for-stratified-medicine-in-patients-with",
        "title": "Joint index vector: a novel assessment measure for stratified medicine in patients with rheumatoid arthritis",
        "authors": [
            "Susumu Nishiyama",
            " Tetsuji Sawada",
            " Jinju Nishino"
        ],
        "date_article": "10-2018",
        "short_description": "ObjectiveTo predict the next-year status in patients with rheumatoid arthritis using big data.MethodsJoint index (JI) of upper/large (UL), upper/small (US), lower/large (LL), and lower/small (LS) was calculated as the sum of tender and swollen joint counts divided by the number of evaluable joints in each region of interest. Joint index vector V (x, y, z) was defined as x = JIUL...",
        "keywords": [
            "Data management",
            "Decision analysis",
            "Rheumatoid arthritis",
            "Stratified medicine"
        ],
        "number_of_pages": "15",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-018-0148-1.pdf",
        "abstract": "Objective:  To predict the next-year status in patients with rheumatoid arthritis using big data.Methods:  Joint index (JI) of upper/large (UL), upper/small (US), lower/large (LL), and lower/small (LS) was calculated as the sum of tender and swollen joint counts divided by the number of evaluable joints in each region of interest. Joint index vector V (x, y, z) was defined as x= JIUL+ JIUS, y = JILL+ JILS, and z= JIUL+ JILL− JIUS− JILS. Low disease activity was defined as |Vxy| (=√x2+ y2) ≤ 0.1. Patients with |Vxy| > 0.1 were further classified into three groups: evenly affected (EVN): |z|≤ 0.2, small joint dominant (SML): z < − 0.2, and large joint dominant (LAR): z > 0.2. To predict the next-year V (x, y, z) of each patient, a transformation matrix was computed from the mean vectors of the EVN, SML, and LAR groups and their translation vectors.Results:  |Vxy| was correlated with Simplified Disease Activity Index (SDAI) (r= 0.82).  Z of mean vector increased as the disability index of the Health Assessment Question-naire (HAQ-DI) and the Steinbrocker class worsened. The LAR group had the worst HAQ-DI and the second highest SDAI after those in the SML group. Positive predictive value and likelihood ratio in predicting the LAR group were 58.7% and 5.9, respectively. Likelihood ratio was greater with treatment, at 7.2, 7.4, and 8.6 when targeted patients were treated with methotrexate, biologics, and both drugs, respectively.Conclusions:  Patients with high disease activity and poor functional state were pre-dicted with high probability using joint index vectors."
    },
    {
        "url": "https://paperity.org/p/162958723/evaluation-of-high-level-query-languages-based-on-mapreduce-in-big-data",
        "title": "Evaluation of high-level query languages based on MapReduce in Big Data",
        "authors": [
            "Marouane Birjali",
            " Abderrahim Beni-Hssane",
            " Mohammed Erritali"
        ],
        "date_article": "10-2018",
        "short_description": "MapReduce (MR) is a criterion of Big Data processing model with parallel and distributed large datasets. This model knows difficult problems related to low-level and batch nature of MR that gives rise to an abstraction layer on the top of MR. Therefore; several High-Level MapReduce Query Languages built on the top of MR provide more abstract query languages and extend the MR...",
        "keywords": [
            "High Level MapReduce Query Languages",
            "JAQL",
            "Big SQL",
            "Hive",
            "Pig",
            "Hadoop",
            "Big Data",
            "Performance comparison"
        ],
        "number_of_pages": "21",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-018-0146-3.pdf",
        "abstract": "MapReduce (MR) is a criterion of Big Data processing model with parallel and dis‑tributed large datasets. This model knows difficult problems related to low‑level and batch nature of MR that gives rise to an abstraction layer on the top of MR. Therefore; several High‑Level MapReduce Query Languages built on the top of MR provide more abstract query languages and extend the MR programming model. These High‑Level MapReduce Query Languages remove the burden of MR programming away from the developers and make a soft migration of existing competences with SQL skills to Big Data. This paper investigates the very used—common High‑Level MapReduce Query Languages built directly on the top of MR that translate queries into executable native MR jobs. It evaluates the performance of the four presented High‑Level MapReduce Query Languages: JAQL, Hive, Big SQL and Pig, with regards to their insightful perspec‑tives and ease of programming. The baseline metrics reported are increasing input size, scale‑out number of nodes and controlling number of reducers. The experimental results study the technical advantages and limitations of each High‑Level MapReduce Query Languages. Finally, the paper provides a summary for developers to choose the High‑Level MapReduce Query Languages which fulfill their needs and interests."
    },
    {
        "url": "https://paperity.org/p/162987476/an-intuitionistic-fuzzy-diagnosis-analytics-for-stroke-disease",
        "title": "An intuitionistic fuzzy diagnosis analytics for stroke disease",
        "authors": [
            "Taufik Djatna",
            " Medria Kusuma Dewi Hardhienata",
            " Anis Fitri Nur Masruriyah"
        ],
        "date_article": "10-2018",
        "short_description": "One of the challenges in diagnosing stroke disease is the lack of useful analysis tool to identify critical stroke data that contains hidden relationships and trends from a vast amount of data. In order to address this problem, we proposed Intuitionistic Fuzzy Based Decision Tree in order to diagnosis the different types of stroke disease. The approach is implemented by mapping...",
        "keywords": [
            "Data mining",
            "Diagnostics analytics",
            "Health analytics",
            "Intuitionistic fuzzy sets",
            "Intuitionistic fuzzy decision tree",
            "Stroke"
        ],
        "number_of_pages": "14",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-018-0142-7.pdf",
        "abstract": "One of the challenges in diagnosing stroke disease is the lack of useful analysis tool to identify critical stroke data that contains hidden relationships and trends from a vast amount of data. In order to address this problem, we proposed Intuitionistic Fuzzy Based Decision Tree in order to diagnosis the different types of stroke disease. The approach is implemented by mapping observation data into Intuitionistic Fuzzy Set. These results lead to a compound of a membership function, non-membership function, and a hesitation degree for each record. The result of Intuitionistic Fuzzy is cal-culated using Hamming Distance as main requirement for Intuitionistic Fuzzy Entropy. The Hamming Distance calculate the difference between values on the same variable. Main advantage of this approach is that we can find out variables effected on the stroke disease using information gain derived from Intutionistics Entropy. Furthermore, the Intuitionistic Fuzzy based Decision Tree are able to provide plenty of information to stakeholders regarding the hidden facts of established rules and utilize linguistic terms to accommodate unclearness, ambiguity, and hesitation in human perception. The results of Intuitionistic Fuzzy Entropy determine the root and node in the formation of the decision tree model based on the information gain of variables in the data. In this study, simulation results show that the approach successfully determine 20 variables that directly influence stroke. These variables are used to classify the types of stroke. Furthermore, results show that the approach has resulted in 90.59% in classifying stroke disease. Results of the study also demonstrates that the approach produces the best diagnosis performance compared to the other two models according to the accuracy of classification from the type of stroke disease."
    },
    {
        "url": "https://paperity.org/p/160753449/intrusion-detection-model-using-machine-learning-algorithm-on-big-data-environment",
        "title": "Intrusion detection model using machine learning algorithm on Big Data environment",
        "authors": [
            "Suad Mohammed Othman",
            " Fadl Mutaher Ba-Alwi",
            " Nabeel T. Alsohyb"
        ],
        "date_article": "09-2018",
        "short_description": "Recently, the huge amounts of data and its incremental increase have changed the importance of information security and data analysis systems for Big Data. Intrusion detection system (IDS) is a system that monitors and analyzes data to detect any intrusion in the system or network. High volume, variety and high speed of data generated in the network have made the data analysis...",
        "keywords": [
            "Intrusion detection",
            "Big Data",
            "Apache Spark",
            "Support vector machine (SVM)",
            "ChiSqSelector"
        ],
        "number_of_pages": "12",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-018-0145-4.pdf",
        "abstract": "Recently, the huge amounts of data and its incremental increase have changed the importance of information security and data analysis systems for Big Data. Intrusion detection system (IDS) is a system that monitors and analyzes data to detect any intru‑sion in the system or network. High volume, variety and high speed of data generated in the network have made the data analysis process to detect attacks by traditional techniques very difficult. Big Data techniques are used in IDS to deal with Big Data for accurate and efficient data analysis process. This paper introduced Spark‑Chi‑SVM model for intrusion detection. In this model, we have used ChiSqSelector for feature selection, and built an intrusion detection model by using support vector machine (SVM) classifier on Apache Spark Big Data platform. We used KDD99 to train and test the model. In the experiment, we introduced a comparison between Chi‑SVM classifier and Chi‑Logistic Regression classifier. The results of the experiment showed that Spark‑Chi‑SVM model has high performance, reduces the training time and is efficient for Big Data."
    },
    {
        "url": "https://paperity.org/p/158934156/privacy-preservation-techniques-in-big-data-analytics-a-survey",
        "title": "Privacy preservation techniques in big data analytics: a survey",
        "authors": [
            "P. Ram Mohan Rao",
            " S. Murali Krishna",
            " A. P. Siva Kumar"
        ],
        "date_article": "09-2018",
        "short_description": "Incredible amounts of data is being generated by various organizations like hospitals, banks, e-commerce, retail and supply chain, etc. by virtue of digital technology. Not only humans but machines also contribute to data in the form of closed circuit television streaming, web site logs, etc. Tons of data is generated every minute by social media and smart phones. The voluminous...",
        "keywords": [
            "Data",
            "Data analytics",
            "Privacy threats",
            "Privacy preservation"
        ],
        "number_of_pages": "12",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-018-0141-8.pdf",
        "abstract": "Incredible amounts of data is being generated by various organizations like hospitals, banks, e-commerce, retail and supply chain, etc. by virtue of digital technology. Not only humans but machines also contribute to data in the form of closed circuit televi-sion streaming, web site logs, etc. Tons of data is generated every minute by social media and smart phones. The voluminous data generated from the various sources can be processed and analyzed to support decision making. However data analytics is prone to privacy violations. One of the applications of data analytics is recommen-dation systems which is widely used by ecommerce sites like Amazon, Flip kart for suggesting products to customers based on their buying habits leading to inference attacks. Although data analytics is useful in decision making, it will lead to serious privacy concerns. Hence privacy preserving data analytics became very important. This paper examines various privacy threats, privacy preservation techniques and models with their limitations, also proposes a data lake based modernistic privacy preservation technique to handle privacy preservation in unstructured data."
    },
    {
        "url": "https://paperity.org/p/158962909/step-away-from-stepwise",
        "title": "Step away from stepwise",
        "authors": [
            "Gary Smith"
        ],
        "date_article": "09-2018",
        "short_description": "BackgroundStepwise regression is a popular data-mining tool that uses statistical significance to select the explanatory variables to be used in a multiple-regression model.FindingsA fundamental problem with stepwise regression is that some real explanatory variables that have causal effects on the dependent variable may happen to not be statistically significant, while nuisance...",
        "keywords": [
            "Stepwise regression",
            "Data mining",
            "Big Data"
        ],
        "number_of_pages": "12",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-018-0143-6.pdf",
        "abstract": "Background:  Stepwise regression is a popular data-mining tool that uses statistical significance to select the explanatory variables to be used in a multiple-regression model.Findings:  A fundamental problem with stepwise regression is that some real explana-tory variables that have causal effects on the dependent variable may happen to not be statistically significant, while nuisance variables may be coincidentally significant. As a result, the model may fit the data well in-sample, but do poorly out-of-sample.Conclusion:  Many Big-Data researchers believe that, the larger the number of possible explanatory variables, the more useful is stepwise regression for selecting explanatory variables. The reality is that stepwise regression is less effective the larger the number of potential explanatory variables. Stepwise regression does not solve the Big-Data problem of too many explanatory variables. Big Data exacerbates the failings of stepwise regression."
    },
    {
        "url": "https://paperity.org/p/156017044/big-data-fraud-detection-using-multiple-medicare-data-sources",
        "title": "Big Data fraud detection using multiple medicare data sources",
        "authors": [
            "Matthew Herland",
            " Taghi M. Khoshgoftaar",
            " Richard A. Bauder"
        ],
        "date_article": "09-2018",
        "short_description": "In the United States, advances in technology and medical sciences continue to improve the general well-being of the population. With this continued progress, programs such as Medicare are needed to help manage the high costs associated with quality healthcare. Unfortunately, there are individuals who commit fraud for nefarious reasons and personal gain, limiting Medicare’s...",
        "keywords": [
            "Big Data",
            "U.S. Medicare",
            "LEIE",
            "Fraud detection"
        ],
        "number_of_pages": "21",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-018-0138-3.pdf",
        "abstract": "In the United States, advances in technology and medical sciences continue to improve the general well-being of the population. With this continued progress, programs such as Medicare are needed to help manage the high costs associated with quality healthcare. Unfortunately, there are individuals who commit fraud for nefari-ous reasons and personal gain, limiting Medicare’s ability to effectively provide for the healthcare needs of the elderly and other qualifying people. To minimize fraudulent activities, the Centers for Medicare and Medicaid Services (CMS) released a num-ber of “Big Data” datasets for different parts of the Medicare program. In this paper, we focus on the detection of Medicare fraud using the following CMS datasets: (1) Medicare Provider Utilization and Payment Data: Physician and Other Supplier (Part B), (2) Medicare Provider Utilization and Payment Data: Part D Prescriber (Part D), and (3) Medicare Provider Utilization and Payment Data: Referring Durable Medical Equipment, Prosthetics, Orthotics and Supplies (DMEPOS). Additionally, we create a fourth dataset which is a combination of the three primary datasets. We discuss data processing for all four datasets and the mapping of real-world provider fraud labels using the List of Excluded Individuals and Entities (LEIE) from the Office of the Inspector General. Our exploratory analysis on Medicare fraud detection involves building and assessing three learners on each dataset. Based on the Area under the Receiver Operating Characteris-tic (ROC) Curve performance metric, our results show that the Combined dataset with the Logistic Regression (LR) learner yielded the best overall score at 0.816, closely fol-lowed by the Part B dataset with LR at 0.805. Overall, the Combined and Part B datasets produced the best fraud detection performance with no statistical difference between these datasets, over all the learners. Therefore, based on our results and the assumption that there is no way to know within which part of Medicare a physician will commit fraud, we suggest using the Combined dataset for detecting fraudulent behavior when a physician has submitted payments through any or all Medicare parts evaluated in our study."
    },
    {
        "url": "https://paperity.org/p/138528580/a-mapreduce-based-adjoint-method-for-preventing-brain-disease",
        "title": "A MapReduce-based Adjoint method for preventing brain disease",
        "authors": [
            "Manal Zettam",
            " Jalal Laassiri",
            " Nourddine Enney"
        ],
        "date_article": "08-2018",
        "short_description": "In this paper, we present a statistical model performed on the basis of a patient dataset. This model predicts efficiently the brain disease risk. Multiple regression was used to build the statistical model. The least squares estimation problem usually used to estimate the parameters of regression model is solved via parallelized algebraic Adjoint method. As the parallelized...",
        "keywords": [
            "Brain disease",
            "Adjoint method",
            "Multiple regression",
            "MapReduce"
        ],
        "number_of_pages": "17",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-018-0136-5.pdf",
        "abstract": "In this paper, we present a statistical model performed on the basis of a patient data-set. This model predicts efficiently the brain disease risk. Multiple regression was used to build the statistical model. The least squares estimation problem usually used to estimate the parameters of regression model is solved via parallelized algebraic Adjoint method. As the parallelized algebraic Adjoint method is not the only Mapreduce-based method used to solve the least square problem, experimentations were carried out to classify the Adjoint method amongst the other methods. The calculated job comple-tion time shows the competitive trait of the Mapreduce-based Adjoint method."
    },
    {
        "url": "https://paperity.org/p/117390114/kavosh-an-effective-map-reduce-based-association-rule-mining-method",
        "title": "Kavosh: an effective Map-Reduce-based association rule mining method",
        "authors": [
            "Mohammadhossein Barkhordari",
            " Mahdi Niamanesh"
        ],
        "date_article": "07-2018",
        "short_description": "The immense amount of data generated on a daily basis by various devices and systems necessitates a change in data analysis methods. As an important part of analytics, data mining methods require a paradigm shift to solve problems because the old methods cannot manage massive data. Association rule mining is a data mining algorithm used to solve various domain problems. Because...",
        "keywords": [
            "Big data",
            "Data mining",
            "Map-Reduce",
            "Association rules"
        ],
        "number_of_pages": "22",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-018-0129-4.pdf",
        "abstract": "The immense amount of data generated on a daily basis by various devices and sys-tems necessitates a change in data analysis methods. As an important part of analyt-ics, data mining methods require a paradigm shift to solve problems because the old methods cannot manage massive data. Association rule mining is a data mining algo-rithm used to solve various domain problems. Because of the immense volume of data, one-node solutions are no longer useful, and it is necessary to solve problems by using a distributed and shared-nothing architecture such as Map-Reduce. However, when association rule mining is transferred to these architectures, new problems appear. The main problems are lack of data locality and iteration support and process skewness. In this paper, a method is proposed that solves these problems. Kavosh converts data into a unified format that helps nodes perform their tasks independently without the need to exchange data with other nodes. In addition, the proposed method compresses input data to facilitate data management. Another advantage is the lack of process skewness because it is possible to allocate a predefined amount of data to each node. Kavosh omits iterations required for finding frequent itemsets by changing the Map-Reduce architecture. The proposed method is implemented using Hadoop, and the results are compared with open-source products in terms of three aspects: execution time, load balancing and data compression. The results show that Kavosh outperforms other methods in these aspects."
    },
    {
        "url": "https://paperity.org/p/117418867/dimpl-a-distributed-in-memory-drone-flight-path-builder-system",
        "title": "DIMPL: a distributed in-memory drone flight path builder system",
        "authors": [
            "Manu Shukla",
            " Zhiqian Chen",
            " Chang-Tien Lu"
        ],
        "date_article": "07-2018",
        "short_description": "Drones are increasingly being used to perform risky and labor intensive aerial tasks cheaply and safely. To ensure operating costs are low and flights autonomous, their flight plans must be pre-built. In existing techniques drone flight paths are not automatically pre-calculated based on drone capabilities and terrain information. Instead, they focus on adaptive shortest paths...",
        "keywords": [
            "Distributed system",
            "In-memory distribution",
            "Autonomous drones",
            "Flight path of drones"
        ],
        "number_of_pages": "29",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-018-0134-7.pdf",
        "abstract": "Drones are increasingly being used to perform risky and labor intensive aerial tasks cheaply and safely. To ensure operating costs are low and flights autonomous, their flight plans must be pre‑built. In existing techniques drone flight paths are not auto‑matically pre‑calculated based on drone capabilities and terrain information. Instead, they focus on adaptive shortest paths, manually determined paths, navigation through camera, images and/or GPS for guidance and genetic or geometric algorithms to guide the drone during flight, all of which makes flight navigation complex and risky. In this paper we present details of an automated flight plan builder DIMPL that pre‑builds flight plans for drones tasked with surveying a large area to take photographs of electric poles to identify ones with hazardous vegetation overgrowth. The flight plans are built for subregions allowing the drones to navigate autonomously. DIMPL employs a distributed in‑memory paradigm to process subregions in parallel and build flight paths in a highly efficient manner. Experiments performed with network and elevation datasets validated the efficiency of DIMPL in building optimal flight plans for a fleet of different types of drones and demonstrated the tremendous performance improve‑ments possible using the distributed in‑memory paradigm."
    },
    {
        "url": "https://paperity.org/p/117185476/anomaly-behaviour-detection-based-on-the-meta-morisita-index-for-large-scale-spatio",
        "title": "Anomaly behaviour detection based on the meta-Morisita index for large scale spatio-temporal data set",
        "authors": [
            "Zhao Yang",
            " Nathalie Japkowicz"
        ],
        "date_article": "07-2018",
        "short_description": "In this paper, we propose a framework for processing and analysing large-scale spatio-temporal data that uses a battery of machine learning methods based on a meta-data representation of point patterns. Existing spatio-temporal analysis methods do not include a specific mechanism for analysing meta-data (point pattern information). In this work, we extend a spatial point pattern...",
        "keywords": [
            "Spatio-temporal data",
            "Point pattern",
            "Data mining",
            "Unsupervised learning",
            "Morisita index"
        ],
        "number_of_pages": "28",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-018-0133-8.pdf",
        "abstract": "In this paper, we propose a framework for processing and analysing large-scale spatio-temporal data that uses a battery of machine learning methods based on a meta-data representation of point patterns. Existing spatio-temporal analysis methods do not include a specific mechanism for analysing meta-data (point pattern information). In this work, we extend a spatial point pattern analysis method (the Morisita index) with meta-data analysis, which includes anomaly behaviour detection and unsupervised learning to support spatio-temporal data analysis and demonstrate its practical use. The resulting framework is robust and has the capability to detect anomalies among large-scale spatio-temporal data using meta-data based on point pattern analysis. It returns visualized reports to end users."
    },
    {
        "url": "https://paperity.org/p/107649061/privacy-preserving-data-publishing-based-on-sensitivity-in-context-of-big-data-using-hive",
        "title": "Privacy preserving data publishing based on sensitivity in context of Big Data using Hive",
        "authors": [
            "P. Srinivasa Rao",
            " S. Satyanarayan"
        ],
        "date_article": "07-2018",
        "short_description": "Privacy preserving data publication is the main concern in present days, because the data being published through internet has been increasing day by day. This huge amount of data was named as Big Data by its size. This project deals with the privacy preservation in context of big data using a data warehousing solution called hive. We implemented nearest similarity based...",
        "keywords": [
            "Sensitivity",
            "Sensitive level",
            "Clustering",
            "PPDP",
            "Bottom-up generalization",
            "Big Data"
        ],
        "number_of_pages": "20",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-018-0130-y.pdf",
        "abstract": "Privacy preserving data publication is the main concern in present days, because the data being published through internet has been increasing day by day. This huge amount of data was named as Big Data by its size. This project deals with the privacy preservation in context of big data using a data warehousing solution called hive. We implemented nearest similarity based clustering (NSB) with Bottom-up generalization to achieve (v,l)-anonymity which deals with the sensitivity vulnerabilities and ensures the individual privacy. We also calculate the sensitivity levels by simple comparison method using the index values, by classifying the different levels of sensitivity. The experiments were carried out on the hive environment to verify the efficiency of algo-rithms with big data. This framework also supports the execution of existing algorithms without any changes. The model in the article outperforms than existing models."
    },
    {
        "url": "https://paperity.org/p/98967323/a-non-parametric-maximum-for-number-of-selected-features-objective-optima-for-fdr-and",
        "title": "A non-parametric maximum for number of selected features: objective optima for FDR and significance threshold with application to ordinal survey analysis",
        "authors": [
            "Amir Hassan Ghaseminejad Tafreshi"
        ],
        "date_article": "05-2018",
        "short_description": "This paper identifies a criterion for choosing an optimum set of selected features, or rejected null hypotheses, in high-dimensional data analysis. The method is designed for dimension reduction with multiple hypothesis testing used in filtering process of big data, and in exploratory research, to identify significant associations among many predictor variables and few outcomes...",
        "keywords": [
            "High-dimensional data analysis",
            "Dimension reduction",
            "Feature selection",
            "Multiple hypothesis testing",
            "False discovery rate",
            "Optimum significance threshold",
            "Maximum for reasonable number of rejected hypotheses",
            "Big data analysis"
        ],
        "number_of_pages": "19",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-018-0128-5.pdf",
        "abstract": "This paper identifies a criterion for choosing an optimum set of selected features, or rejected null hypotheses, in high-dimensional data analysis. The method is designed for dimension reduction with multiple hypothesis testing used in filtering process of big data, and in exploratory research, to identify significant associations among many predictor variables and few outcomes. The novelty of the proposed method is that the selected p-value threshold will be insensitive to dependency within features, and between features and outcome. The method neither requires predetermined thresh-olds for level of significance, nor uses presumed thresholds for false discovery rate. Using the presented method, the optimum p-value for powerful yet parsimonious model is chosen, then for every set of rejected hypotheses, the researcher can also report traditional measures of statistical accuracy such as the expected number of false positives, and false discovery rate. The upper limit for number of rejected hypotheses (or selected features) is determined by finding the maximum difference between expected true hypotheses and expected false hypotheses among all possible sets of rejected hypotheses. Then, many methods of choosing an optimum number of selected features such as piecewise regression are used to form a parsimonious model. The paper reports the results of implementation of proposed methods in a novel example of non-parametric analysis of high-dimensional ordinal survey data."
    },
    {
        "url": "https://paperity.org/p/98966529/cross-domain-graph-based-similarity-measurement-of-workflows",
        "title": "Cross-domain graph based similarity measurement of workflows",
        "authors": [
            "Tahereh Koohi-Var",
            " Morteza Zahedi"
        ],
        "date_article": "05-2018",
        "short_description": "The aim of this article is to analyze search and retrieval of workflows. It represents workflows relatedness based on transfer learning. Workflows from different domains (e.g. scientific or business) have similarities and, more important, differences between themselves. Some concepts and solutions developed in one domain may be readily applicable to the other. This paper proposes...",
        "keywords": [
            "Scientific workflows",
            "Business process",
            "Big Data",
            "Transfer learning"
        ],
        "number_of_pages": "16",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-018-0127-6.pdf",
        "abstract": "The aim of this article is to analyze search and retrieval of workflows. It represents workflows relatedness based on transfer learning. Workflows from different domains (e.g. scientific or business) have similarities and, more important, differences between themselves. Some concepts and solutions developed in one domain may be readily applicable to the other. This paper proposes a cross‑domain concept extraction by similarity measurement and has a new research effort at the intersection of workflow domains. It deals with the huge amount of structured and unstructured data (Big Data) that is a demanding task when working on real‑life event logs. The proposed method in this paper gives a general solution in the sense that it can be coupled to any Process Aware Information System."
    },
    {
        "url": "https://paperity.org/p/98742507/forecasting-aids-prevalence-in-the-united-states-using-online-search-traffic-data",
        "title": "Forecasting AIDS prevalence in the United States using online search traffic data",
        "authors": [
            "Amaryllis Mavragani",
            " Gabriela Ocho"
        ],
        "date_article": "05-2018",
        "short_description": "Over the past decade and with the increasing use of the Internet, the assessment of health issues using online search traffic data has become an integral part of Health Informatics. Internet data in general and from Google Trends in particular have been shown to be valid and valuable in predictions, forecastings, and nowcastings; and in detecting, tracking, and monitoring...",
        "keywords": [
            "AIDS",
            "Big data",
            "Forecasting",
            "Google Trends",
            "HIV",
            "Internet",
            "Online behavior"
        ],
        "number_of_pages": "21",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-018-0126-7.pdf",
        "abstract": "Over the past decade and with the increasing use of the Internet, the assessment of health issues using online search traffic data has become an integral part of Health Informatics. Internet data in general and from Google Trends in particular have been shown to be valid and valuable in predictions, forecastings, and nowcastings; and in detecting, tracking, and monitoring diseases’ outbreaks and epidemics. Empirical relationships have been shown to exist between Google Trends’ data and official data in several health topics, with the science of infodemiology using the vast amount of information available online for the assessment of public health and policy matters. The aim of this study is to provide a method of forecasting AIDS prevalence in the US using online search traffic data from Google Trends on AIDS related terms. The results at first show that significant correlations between Google Trends’ data and official health data on AIDS prevalence (2004–2015) exist in several States, while the estimated forecasting models for AIDS prevalence show that official health data and Google Trends data on AIDS follow a logarithmic relationship. Overall, the results of this study support previous work on the subject suggesting that Google data are valid and valuable for the analysis and forecasting of human behavior towards health topics, and could further assist with Health Assessment in the US and in other countries and regions with valid available official health data."
    },
    {
        "url": "https://paperity.org/p/98478567/the-mapreduce-based-approach-to-improve-the-shortest-path-computation-in-large-scale-road",
        "title": "The MapReduce-based approach to improve the shortest path computation in large-scale road networks: the case of A* algorithm",
        "authors": [
            "Wilfried Yves Hamilton Adoni",
            " Tarik Nahhal",
            " Brahim Aghezzaf"
        ],
        "date_article": "05-2018",
        "short_description": "This paper deals with an efficient parallel and distributed framework for intensive computation with A* algorithm based on MapReduce concept. The A* algorithm is one of the most popular graph traversal algorithm used in route guidance. It requires exponential time computation and very costly hardware to compute the shortest path on large-scale networks. Thus, it is necessary to...",
        "keywords": [
            "Path-finding",
            "Large-scale network",
            "A* algorithm",
            "Big Data",
            "Hadoop",
            "MapReduce",
            "HDFS",
            "Parallel and distributed computing"
        ],
        "number_of_pages": "24",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-018-0125-8.pdf",
        "abstract": "This paper deals with an efficient parallel and distributed framework for intensive com-putation with A* algorithm based on MapReduce concept. The A* algorithm is one of the most popular graph traversal algorithm used in route guidance. It requires expo-nential time computation and very costly hardware to compute the shortest path on large-scale networks. Thus, it is necessary to reduce the time complexity while exploit-ing a low cost commodity hardwares. To cope with this situation, we propose a novel approach that reduces the A* algorithm into a set of Map and Reduce tasks for running the path computation on Hadoop MapReduce framework. An application on real road networks illustrates the feasibility and reliability of the proposed framework. The experi-ments performed on a 6-node Hadoop cluster proves that the proposed approach outperforms A* algorithm and achieves significant gain in terms of computation time."
    },
    {
        "url": "https://paperity.org/p/98073445/differential-privacy-its-technological-prescriptive-using-big-data",
        "title": "Differential privacy: its technological prescriptive using big data",
        "authors": [
            "Priyank Jain",
            " Manasi Gyanchandani",
            " Nilay Khar"
        ],
        "date_article": "04-2018",
        "short_description": "Data is being produced in large amounts and in rapid pace which is diverse in quality, hence, the term big data used. Now, big data has started to influence modern day life in almost every sphere, be it business, education or healthcare. Data being a part and parcel of everyday life, privacy has become a topic requiring emphasis. Privacy can be defined as the capacity of a person...",
        "keywords": [
            "Differential privacy",
            "Big data",
            "Big data privacy",
            "Airavat",
            "PINQ",
            "Geo-indistinguishability",
            "GUPT",
            "Privacy budget",
            "Sensitivity",
            "Laplace",
            "Exponential"
        ],
        "number_of_pages": "24",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-018-0124-9.pdf",
        "abstract": "Data is being produced in large amounts and in rapid pace which is diverse in qual-ity, hence, the term big data used. Now, big data has started to influence modern day life in almost every sphere, be it business, education or healthcare. Data being a part and parcel of everyday life, privacy has become a topic requiring emphasis. Privacy can be defined as the capacity of a person or group to seclude themselves or infor-mation about themselves, and thereby express them selectively. Privacy in big data can be achieved through various means but here the focus is on differential privacy. Differential privacy is one such field with one of the strongest mathematical guaran-tee and with a large scope of future development. Along these lines, in this paper, the fundamental ideas of sensitivity and privacy budget in differential privacy, the noise mechanisms utilized as a part of differential privacy, the composition properties, the ways through which it can be achieved and the developments in this field till date has been presented. The research gap and future directions have also been mentioned as part of this paper."
    },
    {
        "url": "https://paperity.org/p/85955949/semlinker-automating-big-data-integration-for-casual-users",
        "title": "SemLinker: automating big data integration for casual users",
        "authors": [
            "Hassan Alrehamy",
            " Coral Walker"
        ],
        "date_article": "03-2018",
        "short_description": "A data integration approach combines data from different sources and builds a unified view for the users. Big data integration inherently is a complex task, and the existing approaches are either potentially limited or invariably rely on manual inputs and interposition from experts or skilled users. SemLinker, an ontology-based data integration system, is part of a metadata...",
        "keywords": [
            "Data integration",
            "Big data",
            "Data lake",
            "Modeling",
            "Schema evolution",
            "Schema mapping",
            "Metadata management"
        ],
        "number_of_pages": "26",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-018-0123-x.pdf",
        "abstract": "A data integration approach combines data from different sources and builds a unified view for the users. Big data integration inherently is a complex task, and the existing approaches are either potentially limited or invariably rely on manual inputs and inter-position from experts or skilled users. SemLinker, an ontology-based data integration system, is part of a metadata management framework for personal data lake (PDL), a personal store-everything architecture. PDL is for casual and unskilled users, therefore SemLinker adopts an automated data integration workflow to minimize manual input requirements. To support the flat architecture of a lake, SemLinker builds and maintains a schema metadata level without involving any physical transformation of data during integration, preserving the data in their native formats while, at the same time, allow-ing them to be queried and analyzed. Scalability, heterogeneity, and schema evolu-tion are big data integration challenges that are addressed by SemLinker. Large and real-world datasets of substantial heterogeneities are used in evaluating SemLinker. The results demonstrate and confirm the integration efficiency and robustness of Sem-Linker, especially regarding its capability in the automatic handling of data heteroge-neities and schema evolutions."
    },
    {
        "url": "https://paperity.org/p/85722558/efficiency-of-random-swap-clustering",
        "title": "Efficiency of random swap clustering",
        "authors": [
            "Pasi Fränti"
        ],
        "date_article": "03-2018",
        "short_description": "Random swap algorithm aims at solving clustering by a sequence of prototype swaps, and by fine-tuning their exact location by k-means. This randomized search strategy is simple to implement and efficient. It reaches good quality clustering relatively fast, and if iterated longer, it finds the correct clustering with high probability. In this paper, we analyze the expected number...",
        "keywords": [
            "Clustering",
            "Random swap",
            "K-means",
            "Local search",
            "Efficiency"
        ],
        "number_of_pages": "29",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-018-0122-y.pdf",
        "abstract": "Random swap algorithm aims at solving clustering by a sequence of prototype swaps, and by fine-tuning their exact location by k-means. This randomized search strategy is simple to implement and efficient. It reaches good quality clustering relatively fast, and if iterated longer, it finds the correct clustering with high probability. In this paper, we analyze the expected number of iterations needed to find the correct clustering. Using this result, we derive the expected time complexity of the random swap algorithm. The main results are that the expected time complexity has (1) linear dependency on the number of data vectors, (2) quadratic dependency on the number of clusters, and (3) inverse dependency on the size of neighborhood. Experiments also show that the algorithm is clearly more efficient than k-means and almost never get stuck in inferior local minimum."
    },
    {
        "url": "https://paperity.org/p/85751311/a-novel-adaptable-approach-for-sentiment-analysis-on-big-social-data",
        "title": "A novel adaptable approach for sentiment analysis on big social data",
        "authors": [
            "Imane El Alaoui",
            " Youssef Gahi",
            " Rochdi Messoussi"
        ],
        "date_article": "03-2018",
        "short_description": "Gathering public opinion by analyzing big social data has attracted wide attention due to its interactive and real time nature. For this, recent studies have relied on both social media and sentiment analysis in order to accompany big events by tracking people’s behavior. In this paper, we propose an adaptable sentiment analysis approach that analyzes social media posts and...",
        "keywords": [
            "-"
        ],
        "number_of_pages": "18",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-018-0120-0.pdf",
        "abstract": "Gathering public opinion by analyzing big social data has attracted wide attention due to its interactive and real time nature. For this, recent studies have relied on both social media and sentiment analysis in order to accompany big events by tracking people’s behavior. In this paper, we propose an adaptable sentiment analysis approach that analyzes social media posts and extracts user’s opinion in real-time. The proposed approach consists of first constructing a dynamic dictionary of words’ polarity based on a selected set of hashtags related to a given topic, then, classifying the tweets under several classes by introducing new features that strongly fine-tune the polarity degree of a post. To validate our approach, we classified the tweets related to the 2016 US election. The results of prototype tests have performed a good accuracy in detecting positive and negative classes and their sub-classes.Open Access©  The  Author(s)  2018.  This  article  is  distributed  under  the  terms  of  the  Creative  Commons  Attribution  4.0  International  License  (http://creativecommons.org/licenses/by/4.0/),  which  permits  unrestricted  use,  distribution,  and  reproduction  in  any  medium,  provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made.METHODOLOGYEl Alaouiet al. J Big Data  (2018) 5:12  https://doi.org/10.1186/s40537-018-0120-0*Correspondence:   imane.el.alaoui@uit.ac.ma 1 Laboratoire des Systèmes de Télécommunications et Ingénierie de la Décision, University of Ibn Tofail, Kenitra, MoroccoFull list of author information is available at the end of the article\nPage 2 of 18El Alaoui et al. J Big Data  (2018) 5:12 •Machine  learning  (ML)  involves  building  models  from  labeled  training  dataset  (instances of texts or sentences) in order to determine the orientation of a document. Studies that used this type of methods have been carried out on a specific topic.These two analysis methods have been widely used on big social data to gather pub-lic opinion in order to asssess internauts satisfaction of a subject (services, products, events, topics or persons) in several domains including politics [3], marketing [4] and health  [7].  However,  the  results  are  varying,  sometimes  concluding  with  a  reason-able  degree  of  accuracy  and  sometimes  are  not.  The  failure  is  generally  due  to  the  opinion  mining  challenges  such  as  the  semantic  orientation  of  a  word  which  could  change depending on the context. In this paper, we aim to tackle semantic analysis by introducing a novel adaptable approach that relies on social media posts and big data architecture  to  analyze  internauts’  behaviors  and  feelings  toward  a  subject  in  real-time. The proposed approach is based on three stages as shown in Fig. 1.In  order  to  validate  our  proposed  approach,  we  built  a  prototype  and  conducted  a  study on analyzing the 2016 US election related tweets to find out which candidate is the favorite.The  remainder  of  this  paper  is  organized  as  follows.  In  the  second  section,  work  related  to  analyze  social  media  data  and  its  correlation  with  trends  are  explored.  In  the  third  section,  we  highlight  the  theoretical  basis  on  textual  analysis.  Section  four  presents  an  overview  of  the  proposed  method.  The  experimental  methodology  and  Fig. 1Stages of the model"
    },
    {
        "url": "https://paperity.org/p/85780064/concept-and-benchmark-results-for-big-data-energy-forecasting-based-on-apache-spark",
        "title": "Concept and benchmark results for Big Data energy forecasting based on Apache Spark",
        "authors": [
            "Jorge Ángel González Ordiano",
            " Andreas Bartschat",
            " Nicole Ludwig"
        ],
        "date_article": "03-2018",
        "short_description": "The present article describes a concept for the creation and application of energy forecasting models in a distributed environment. Additionally, a benchmark comparing the time required for the training and application of data-driven forecasting models on a single computer and a computing cluster is presented. This comparison is based on a simulated dataset and both R and Apache...",
        "keywords": [
            "Big Data",
            "Forecasting",
            "Energy",
            "Data-driven",
            "EnergyLab 2.0"
        ],
        "number_of_pages": "11",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-018-0119-6.pdf",
        "abstract": "The present article describes a concept for the creation and application of energy forecasting models in a distributed environment. Additionally, a benchmark comparing the time required for the training and application of data-driven forecasting models on a single computer and a computing cluster is presented. This comparison is based on a simulated dataset and both R and Apache Spark are used. Furthermore, the obtained results show certain points in which the utilization of distributed computing based on Spark may be advantageous."
    },
    {
        "url": "https://paperity.org/p/85808817/graphzip-a-clique-based-sparse-graph-compression-method",
        "title": "GraphZIP: a clique-based sparse graph compression method",
        "authors": [
            "Ryan A. Rossi",
            " Rong Zhou"
        ],
        "date_article": "03-2018",
        "short_description": "Massive graphs are ubiquitous and at the heart of many real-world problems and applications ranging from the World Wide Web to social networks. As a result, techniques for compressing graphs have become increasingly important and remains a challenging and unsolved problem. In this work, we propose a graph compression and encoding framework called GraphZIP based on the observation...",
        "keywords": [
            "Graph compression",
            "Large cliques",
            "Graph encoding",
            "Sparse graphs",
            "Graph algorithms"
        ],
        "number_of_pages": "14",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-018-0121-z.pdf",
        "abstract": "Massive graphs are ubiquitous and at the heart of many real-world problems and applications ranging from the World Wide Web to social networks. As a result, tech-niques for compressing graphs have become increasingly important and remains a challenging and unsolved problem. In this work, we propose a graph compression and encoding framework called GraphZIP based on the observation that real-world graphs often form many cliques of a large size. Using this as a foundation, the proposed tech-nique decomposes a graph into a set of large cliques, which is then used to compress and represent the graph succinctly. In particular, disk-resident and in-memory graph encodings are proposed and shown to be effective with three important benefits. First, it reduces the space needed to store the graph on disk (or other permanent storage device) and in-memory. Second, GraphZIP reduces IO traffic involved in using the graph. Third, it reduces the amount of work involved in running an algorithm on the graph. The experiments demonstrate the scalability, flexibility, and effectiveness of the clique-based compression techniques using a collection of networks from various domains."
    },
    {
        "url": "https://paperity.org/p/85866323/who-is-behind-the-wheel-driver-identification-and-fingerprinting",
        "title": "Who is behind the wheel? Driver identification and fingerprinting",
        "authors": [
            "Saad Ezzini",
            " Ismail Berrada",
            " Mounir Ghogho"
        ],
        "date_article": "02-2018",
        "short_description": "In the last decade, significant advances have been made in sensing and communication technologies. Such progress led to a considerable growth in the development and use of intelligent transportation systems. Characterizing driving styles of drivers using in-vehicle sensor data is an interesting research problem and an essential real-world requirement for automotive industries. A...",
        "keywords": [
            "Driver fingerprinting",
            "Driver identification",
            "Driver verification",
            "Machine learning"
        ],
        "number_of_pages": "15",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-018-0118-7.pdf",
        "abstract": "In the last decade, significant advances have been made in sensing and communica-tion technologies. Such progress led to a considerable growth in the development and use of intelligent transportation systems. Characterizing driving styles of drivers using in-vehicle sensor data is an interesting research problem and an essential real-world requirement for automotive industries. A good representation of driving features can be extremely valuable for anti-theft, auto insurance, autonomous driving, and many other application scenarios. This paper addresses the problem of driver identification using real driving datasets consisting of measurements taken from in-vehicle sensors. The paper investigates the minimum learning and classification times that are required to achieve a desired identification performance. Further, feature selection is carried out to extract the most relevant features for driver identification. Finally, in addition to driv-ing pattern related features, driver related features (e.g., heart-rate) are shown to further improve the identification performance."
    },
    {
        "url": "https://paperity.org/p/85837570/streamaligner-a-streaming-based-sequence-aligner-on-apache-spark",
        "title": "StreamAligner: a streaming based sequence aligner on Apache Spark",
        "authors": [
            "Sanjay Rathee",
            " Arti Kashyap"
        ],
        "date_article": "02-2018",
        "short_description": "Next-Generation Sequencing technologies are generating a huge amount of genetic data that need to be mapped and analyzed. Single machine sequence alignment tools are becoming incapable or inefficient in keeping track of the same. Therefore, distributed computing platforms based on MapReduce paradigm, which uses thousands of commodity machines to process and analyze huge datasets...",
        "keywords": [
            "Sequence alignment",
            "Apache Spark",
            "Hadoop",
            "Distributed computing frameworks"
        ],
        "number_of_pages": "18",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-018-0114-y.pdf",
        "abstract": "Next-Generation Sequencing technologies are generating a huge amount of genetic data that need to be mapped and analyzed. Single machine sequence alignment tools are becoming incapable or inefficient in keeping track of the same. Therefore, distrib-uted computing platforms based on MapReduce paradigm, which uses thousands of commodity machines to process and analyze huge datasets, are emerging as the best solution for growing genomics data. A lot of MapReduce-based sequence alignment tools like CloudBurst, CloudAligner, Halvade, and SparkBWA are proposed by various researchers in recent few years. These sequence aligners are very fast and efficient. These sequence aligners are capable of aligning billions of reads (stored as fasta or fastq files) on reference genome in few minutes. In the current era of fastly growing technology, analyzing huge genome data fast is not enough. We need to analyze data in real time to automate alignment process. Therefore, we propose a MapReduce-based sequence alignment tool StreamAligner which is implemented on Spark stream-ing engine. StreamAligner can align stream of reads on reference genome in real time. Therefore, it can be used to automate sequencing and alignment process. It uses suffix array index for read alignment which is generated using distributed index generation algorithm. Due to distributed index generation algorithm, index generation time is very less. It needs to upload index only once when StreamAligner is launched. After that index stays in Spark memory and can be used for an unlimited times without reload-ing. Whereas, current state-of-the-art sequence aligner either generate (hash index based) or load (sorted index based) index for every task. Hence, StreamAligner reduces time to generate or load index for every task. A working and tested implementation of streamAligner is available on GitHub for download and use. We tested the effective-ness, efficiency, and scalability of our aligner for various standard and real-life datasets."
    },
    {
        "url": "https://paperity.org/p/85895076/bayesian-count-regression-analysis-for-determinants-of-antenatal-care-service-visits",
        "title": "Bayesian count regression analysis for determinants of antenatal care service visits among pregnant women in Amhara regional state, Ethiopia",
        "authors": [
            "Mekuanint Simeneh Workie",
            " Ayenew Molla Lakew"
        ],
        "date_article": "02-2018",
        "short_description": "",
        "keywords": [
            "Bayesian approach",
            "Classical approach",
            "ANC",
            "Amhara region",
            "MCMC",
            "Posterior distribution",
            "Prior density"
        ],
        "number_of_pages": "23",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-018-0117-8.pdf",
        "abstract": "Background:  Complications of pregnancy and childbirth are a leading cause of maternal morbidities and mortalities in developing countries. World Health Organi-zation (WHO) estimates that over 500,000 women and girls die each year from the complications. Despite proven interventions that could prevent death or disability during pregnancy and childbirth, maternal mortality remains a major burden in many developing countries, including Ethiopia. This study aimed to assess the status of ante-natal care utilization and modeling Bayesian Count Regression model for the determi-nants of utilization of antenatal care services visits among pregnant women in Amhara regional state.Methods:  It was a community based analytical cross-sectional study, conducted in Amhara region among women in the reproductive age group (age 15–49). The analysis was based on data from women who had at least one birth during the 5 years preced-ing the survey. The source of data was the 2014 Ethiopia Demographic and Health Survey which was accessed from Central Statistical Agency. Bayesian analytic approach was applied to model the mixture data structure inherent in zero-inflated count data by using the zero-inflated Poisson model.Results:  About 37% (95% CI 0.32, 0.42) of the pregnant mothers were not received antenatal care services during their pregnancy and about 23% of them were visited at least four times. From Bayesian zero inflated Poisson regression it was found that rural pregnant women (OR= 1.13; HPD CI 1.12, 1.44), women who can read and write (OR= 0.54; HPD CI 0.40, 0.72), middle Wealth index (OR= 0.60; HPD CI 0.46, 0.78) and media exposures (OR= 0.72; HPD: 0.56, 0.92) were statistically associated with no ANC visits.Conclusions:  About three-fourth pregnant mothers were not receive adequate number of visits recommended by the World Health Organization. Mother’s education, media exposure, residence and wealth index were significant predictors of ANC service utilization. This research suggests that to reduce the inadequate number of ANC visits in Amhara region, attention should be given to women with low educational status and rural women.Open Access©  The  Author(s)  2018.  This  article  is  distributed  under  the  terms  of  the  Creative  Commons  Attribution  4.0  International  License  (http://creativecommons.org/licenses/by/4.0/),  which  permits  unrestricted  use,  distribution,  and  reproduction  in  any  medium,  provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made.RESEARCHWorkie and Lakew  J Big Data  (2018) 5:7  https://doi.org/10.1186/s40537-018-0117-8*Correspondence:   mekuanintsimeneh@gmail.com 1 Mathematical and Statistical Modeling, Debre Markos University, Debre Markos, EthiopiaFull list of author information is available at the end of the article\nPage 2 of 23Workie and Lakew  J Big Data  (2018) 5:7 "
    },
    {
        "url": "https://paperity.org/p/85923829/adaptive-miner-an-efficient-distributed-association-rule-mining-algorithm-on-spark",
        "title": "Adaptive-Miner: an efficient distributed association rule mining algorithm on Spark",
        "authors": [
            "Sanjay Rathee",
            " Arti Kashyap"
        ],
        "date_article": "02-2018",
        "short_description": "Extraction of valuable data from extensive datasets is a standout amongst the most vital exploration issues. Association rule mining is one of the highly used methods for this purpose. Finding possible associations between items in large transaction based datasets (finding frequent itemsets) is most crucial part of the association rule mining task. Many single-machine based...",
        "keywords": [
            "Association rule mining",
            "Apache Spark",
            "Hadoop",
            "Distributed computing frameworks"
        ],
        "number_of_pages": "17",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-018-0112-0.pdf",
        "abstract": "Extraction of valuable data from extensive datasets is a standout amongst the most vital exploration issues. Association rule mining is one of the highly used methods for this purpose. Finding possible associations between items in large transaction based datasets (finding frequent itemsets) is most crucial part of the association rule min-ing task. Many single-machine based association rule mining algorithms exist but the massive amount of data available these days is above the capacity of a single machine based algorithm. Therefore, to meet the demands of this ever-growing enormous data, there is a need for distributed association rule mining algorithm which can run on mul-tiple machines. For these types of parallel/distributed applications, MapReduce is one of the best fault-tolerant frameworks. Hadoop is one of the most popular open-source software frameworks with MapReduce based approach for distributed storage and processing of large datasets using standalone clusters built from commodity hardware. But heavy disk I/O operation at each iteration of a highly iterative algorithm like Apriori makes Hadoop inefficient. A number of MapReduce based platforms are being devel-oped for parallel computing in recent years. Among them, a platform, namely, Spark have attracted a lot of attention because of its inbuilt support to distributed computa-tions. Therefore, we implemented a distributed association rule mining algorithm on Spark named as Adaptive-Miner which uses adaptive approach for finding frequent patterns with higher accuracy and efficiency. Adaptive-Miner uses an adaptive strategy based on the partial processing of datasets. Adaptive-Miner makes execution plans before every iteration and goes with the best suitable plan to minimize time and space complexity. Adpative-Miner is a dynamic association rule mining algorithm which change its approach based on the nature of dataset. Therefore, it is different and better than state-of-the-art static association rule mining algorithms. We conduct in-depth experiments to gain insight into the effectiveness, efficiency, and scalability of the Adaptive-Miner algorithm on Spark. Available: https://github.com/sanjaysinghrathi/Adaptive-Miner"
    },
    {
        "url": "https://paperity.org/p/85952582/a-new-approach-to-the-space-time-analysis-of-big-data-application-to-subway-traffic-data",
        "title": "A new approach to the space–time analysis of big data: application to subway traffic data in Seoul",
        "authors": [
            "Kwang-Yul Kim",
            " Chae-Young Lim",
            " Eunice J. Kim"
        ],
        "date_article": "02-2018",
        "short_description": "A prevalent type of big data is in the form of space–time measurements. Cyclostationary empirical orthogonal function (CSEOF) analysis is introduced as an efficient and valuable technique to interpret space–time structure of variability in a big dataset. CSEOF analysis is demonstrated to be a powerful tool in understanding the space–time structure of variability, when data...",
        "keywords": [
            "CSEOF analysis",
            "Space–time analysis",
            "Big data analysis"
        ],
        "number_of_pages": "18",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-018-0116-9.pdf",
        "abstract": "A prevalent type of big data is in the form of space–time measurements. Cyclostation‑ary empirical orthogonal function (CSEOF) analysis is introduced as an efficient and valuable technique to interpret space–time structure of variability in a big dataset. CSEOF analysis is demonstrated to be a powerful tool in understanding the space–time structure of variability, when data exhibits periodic statistics in time. As an example, CSEOF analysis is applied to the hourly passenger traffic on Subway Line #2 of Seoul, South Korea during the period of 2010–2017. The first mode represents the weekly cycle of subway passengers and captures the majority (~ 97%) of the total variability. The corresponding loading vector exhibits a typical weekly pattern of subway passen‑gers as a function of time and the locations of subway stations. The associated principal component time series shows that there are two occasions of significant reduction in the amplitude of the weekly activity in each year; these reductions are associated with two major holidays—lunar New Year and Fall Festival (called Chuseok in Korea). The second and third modes represent daily contrasts in a week and are associated with taking extra days off before or after holidays. The fourth mode exhibits an interesting upward trend, which represents a general decrease in the number of subway passen‑gers during weekdays except for Wednesday and an increase over the weekends."
    },
    {
        "url": "https://paperity.org/p/85747944/big-data-deep-learning-for-financial-sentiment-analysis",
        "title": "Big Data: Deep Learning for financial sentiment analysis",
        "authors": [
            "Sahar Sohangir",
            " Dingding Wang",
            " Anna Pomeranets"
        ],
        "date_article": "01-2018",
        "short_description": "Deep Learning and Big Data analytics are two focal points of data science. Deep Learning models have achieved remarkable results in speech recognition and computer vision in recent years. Big Data is important for organizations that need to collect a huge amount of data like a social network and one of the greatest assets to use Deep Learning is analyzing a massive amount of data...",
        "keywords": [
            "Deep Learning",
            "Big Data",
            "Sentiment analysis",
            "Information retrieval"
        ],
        "number_of_pages": "25",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-017-0111-6.pdf",
        "abstract": "Deep Learning and Big Data analytics are two focal points of data science. Deep Learn-ing models have achieved remarkable results in speech recognition and computer vision in recent years. Big Data is important for organizations that need to collect a huge amount of data like a social network and one of the greatest assets to use Deep Learning is analyzing a massive amount of data (Big Data). This advantage makes Deep Learning as a valuable tool for Big Data. Deep Learning can be used to extract incred-ible information that buried in a Big Data. The modern stock market is an example of these social networks. They are a popular place to increase wealth and generate income, but the fundamental problem of when to buy or sell shares, or which stocks to buy has not been solved. It is very common among investors to have professional financial advisors, but what is the best resource to support the decisions these peo-ple make? Investment banks such as Goldman Sachs, Lehman Brothers, and Salomon Brothers dominated the world of financial advice for more than a decade. However, via the popularity of the Internet and financial social networks such as StockTwits and SeekingAlpha, investors around the world have new opportunity to gather and share their experiences. Individual experts can predict the movement of the stock market in financial social networks with the reasonable accuracy, but what is the sentiment of a mass group of these expert authors towards various stocks? In this paper, we seek to determine if Deep Learning models can be adapted to improve the performance of sentiment analysis for StockTwits. We applied several neural network models such as long short-term memory, doc2vec, and convolutional neural networks, to stock market opinions posted in StockTwits. Our results show that Deep Learning model can be used effectively for financial sentiment analysis and a convolutional neural network is the best model to predict sentiment of authors in StockTwits dataset."
    },
    {
        "url": "https://paperity.org/p/85776697/investigating-important-urban-characteristics-in-the-formation-of-urban-heat-islands-a",
        "title": "Investigating important urban characteristics in the formation of urban heat islands: a machine learning approach",
        "authors": [
            "Sanglim Yoo"
        ],
        "date_article": "01-2018",
        "short_description": "Despite the urban heat islands phenomenon has long been recognized as a major urban environmental problem, it was not until recently that this urban phenomenon gained attention from the discipline of urban planning. To integrate the findings of the urban heat islands research into the planning practice, the relationship between land surface temperatures and urban physical and...",
        "keywords": [
            "Urban heat island effect",
            "Biophysical vulnerability",
            "Socioeconomic vulnerability",
            "Machine learning",
            "Random forest",
            "Variable selection"
        ],
        "number_of_pages": "24",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-018-0113-z.pdf",
        "abstract": "Despite the urban heat islands phenomenon has long been recognized as a major urban environmental problem, it was not until recently that this urban phenomenon gained attention from the discipline of urban planning. To integrate the findings of the urban heat islands research into the planning practice, the relationship between land surface temperatures and urban physical and socioeconomic characteristics should be addressed at the planning relevant spatial scale, a land parcel. Using a parcel as a unit of analysis, this study proposed to use a machine learning approach to identify impor-tant variables in the formation of urban heat islands in Indianapolis, Indiana. Applying random forest method to planning zones, this study identified planning zone specific urban physical and socioeconomic characteristics that are important for the interpreta-tion of urban heat islands phenomenon of Indianapolis, Indiana. The main contribution of this study is twofold: to integrate urban physical and socioeconomic characteristics into a land parcel for the better interpretation of the result of urban heat islands study into planning practice and to apply machine learning approach to identify highly determinant variables in the formation of urban heat islands."
    },
    {
        "url": "https://paperity.org/p/85477273/big-healthcare-data-preserving-security-and-privacy",
        "title": "Big healthcare data: preserving security and privacy",
        "authors": [
            "Karim Abouelmehdi",
            " Abderrahim Beni-Hessane",
            " Hayat Khaloufi"
        ],
        "date_article": "01-2018",
        "short_description": "Big data has fundamentally changed the way organizations manage, analyze and leverage data in any industry. One of the most promising fields where big data can be applied to make a change is healthcare. Big healthcare data has considerable potential to improve patient outcomes, predict outbreaks of epidemics, gain valuable insights, avoid preventable diseases, reduce the cost of...",
        "keywords": [
            "Security and privacy",
            "Big healthcare data",
            "Security lifecycle",
            "Anonymization",
            "Encryption"
        ],
        "number_of_pages": "18",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-017-0110-7.pdf",
        "abstract": "Big data has fundamentally changed the way organizations manage, analyze and leverage data in any industry. One of the most promising fields where big data can be applied to make a change is healthcare. Big healthcare data has considerable potential to improve patient outcomes, predict outbreaks of epidemics, gain valuable insights, avoid preventable diseases, reduce the cost of healthcare delivery and improve the quality of life in general. However, deciding on the allowable uses of data while pre‑serving security and patient’s right to privacy is a difficult task. Big data, no matter how useful for the advancement of medical science and vital to the success of all healthcare organizations, can only be used if security and privacy issues are addressed. To ensure a secure and trustworthy big data environment, it is essential to identify the limita‑tions of existing solutions and envision directions for future research. In this paper, we have surveyed the state‑of‑the‑art security and privacy challenges in big data as applied to healthcare industry, assessed how security and privacy issues occur in case of big healthcare data and discussed ways in which they may be addressed. We mainly focused on the recently proposed methods based on anonymization and encryption, compared their strengths and limitations, and envisioned future research directions."
    },
    {
        "url": "https://paperity.org/p/85564153/scaling-associative-classification-for-very-large-datasets",
        "title": "Scaling associative classification for very large datasets",
        "authors": [
            "Luca Venturini",
            " Elena Baralis",
            " Paolo Garz"
        ],
        "date_article": "12-2017",
        "short_description": "Supervised learning algorithms are nowadays successfully scaling up to datasets that are very large in volume, leveraging the potential of in-memory cluster-computing Big Data frameworks. Still, massive datasets with a number of large-domain categorical features are a difficult challenge for any classifier. Most off-the-shelf solutions cannot cope with this problem. In this work...",
        "keywords": [
            "Apache Spark",
            "Associative classification",
            "Big Data",
            "Machine learning"
        ],
        "number_of_pages": "24",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-017-0107-2.pdf",
        "abstract": "Supervised learning algorithms are nowadays successfully scaling up to datasets that are very large in volume, leveraging the potential of in-memory cluster-computing Big Data frameworks. Still, massive datasets with a number of large-domain categorical features are a difficult challenge for any classifier. Most off-the-shelf solutions cannot cope with this problem. In this work we introduce DAC, a Distributed Associative Classi-fier. DAC exploits ensemble learning to distribute the training of an associative classi-fier among parallel workers and improve the final quality of the model. Furthermore, it adopts several novel techniques to reach high scalability without sacrificing quality, among which a preventive pruning of classification rules in the extraction phase based on Gini impurity. We ran experiments on Apache Spark, on a real large-scale dataset with more than 4 billion records and 800 million distinct categories. The results showed that DAC improves on a state-of-the-art solution in both prediction quality and execu-tion time. Since the generated model is human-readable, it can not only classify new records, but also allow understanding both the logic behind the prediction and the properties of the model, becoming a useful aid for decision makers."
    },
    {
        "url": "https://paperity.org/p/85536981/improved-classification-of-large-imbalanced-data-sets-using-rationalized-technique",
        "title": "Improved classification of large imbalanced data sets using rationalized technique: Updated Class Purity Maximization Over_Sampling Technique (UCPMOT)",
        "authors": [
            "Sachin S. Patil",
            " Shefali P. Sonavan"
        ],
        "date_article": "12-2017",
        "short_description": "The huge variety of NoSQL Big Data has tossed a need for new pathways to store, process and analyze it. The quantum of data created is inconceivable along with a mixed breath of unknown veracity and creative visualization. The new trials of frameworks help to find substantial unidentified values from massive data sets. They have added an exceptional dimension to the pre...",
        "keywords": [
            "Big Data",
            "Imbalanced data sets",
            "Multi-class",
            "Over_Sampling Techniques",
            "Lowest versus highest"
        ],
        "number_of_pages": "32",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-017-0108-1.pdf",
        "abstract": "The huge variety of NoSQL Big Data has tossed a need for new pathways to store, pro-cess and analyze it. The quantum of data created is inconceivable along with a mixed breath of unknown veracity and creative visualization. The new trials of frameworks help to find substantial unidentified values from massive data sets. They have added an exceptional dimension to the pre-processing and contextual conversion of the data sets for needful analysis. In addition, handling of ambitious imbalanced data sets has acknowledged an intimation of alarm. Traditional classifiers are unable to discourse the precise need of grouping for such data sets. Over_sampling of the minority classes help to improve the performance. Updated Class Purity Maximization Over_Sampling Technique (UCPMOT ) is a rationalized technique proposed to handle imbalanced data sets using exclusive safe-level based synthetic sample creation. It addresses the multi-class problem in alignment to a newly induced method namely lowest versus highest. The projected technique experiments with several data sets from the UCI repository. The underlying bed of mapreduce environment encompasses the distributed process-ing approach on Apache Hadoop framework. Several classifiers help to authorize the classification results using parameters like F-measure and AUC values. The experimental conclusions quote the dominance of UCPMOT over the benchmarking techniques."
    },
    {
        "url": "https://paperity.org/p/85500485/missing-data-management-and-statistical-measurement-of-socio-economic-status-application",
        "title": "Missing data management and statistical measurement of socio-economic status: application of big data",
        "authors": [
            "Habtamu Tilaye Wubeti"
        ],
        "date_article": "12-2017",
        "short_description": "Socio-economic status measurement is an ongoing problem where different suggested measurements are given by researchers. This work investigates a socio-economic status measurement derived from natural correlations of variables which can better and meaningfully cluster African countries for the level of status. The researcher used 48 African countries socio-economic yearly time...",
        "keywords": [
            "African countries",
            "socio-economic development",
            "missing data management",
            "Principal component analysis",
            "Factor analysis and cluster analysis"
        ],
        "number_of_pages": "44",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-017-0099-y.pdf",
        "abstract": "Socio-economic status measurement is an ongoing problem where different sug-gested measurements are given by researchers. This work investigates a socio-eco-nomic status measurement derived from natural correlations of variables which can better and meaningfully cluster African countries for the level of status. The researcher used 48 African countries socio-economic yearly time series data from 1993 to 2013 of IMF 2013 data set for data management (i.e, 2737 variables for 21 years), however, the analysis is reasonably done based on recent 14 years time series data. In data manage-ment, missing values are treated (imputed) by using regression estimates, Lagrange interpolation, linear interpolation and linear spline interpolation based on the appropri-ate method which best fits for the trend of data with minimum error at each time level. From principal component and factor analysis of average time series data, 7 principal factors contributed by 84 variables which explain 70% of the variation in the data set are suggested as a socio-economic status measuring components and as a result the considered clustering methods (K-mean Method, Average linkage method, Ward’s method and Bootstrap Ward’s method) are agreed on six clusters of countries, those are statistically significant at 95%, where as three countries each where suggested as outlier-countries made an individual cluster."
    },
    {
        "url": "https://paperity.org/p/85471732/a-clustering-algorithm-for-multivariate-data-streams-with-correlated-components",
        "title": "A clustering algorithm for multivariate data streams with correlated components",
        "authors": [
            "Giacomo Aletti",
            " Alessandra Micheletti"
        ],
        "date_article": "12-2017",
        "short_description": "Common clustering algorithms require multiple scans of all the data to achieve convergence, and this is prohibitive when large databases, with data arriving in streams, must be processed. Some algorithms to extend the popular K-means method to the analysis of streaming data are present in literature since 1998 (Bradley et al. in Scaling clustering algorithms to large databases...",
        "keywords": [
            "Big data",
            "Data streams",
            "Clustering",
            "Mahalanobis distance"
        ],
        "number_of_pages": "20",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-017-0109-0.pdf",
        "abstract": "Common clustering algorithms require multiple scans of all the data to achieve conver-gence, and this is prohibitive when large databases, with data arriving in streams, must be processed. Some algorithms to extend the popular K-means method to the analysis of streaming data are present in literature since 1998 (Bradley et al. in Scaling clustering algorithms to large databases. In: KDD. p. 9–15, 1998; O’Callaghan et al. in Streaming-data algorithms for high-quality clustering. In: Proceedings of IEEE international confer-ence on data engineering. p. 685, 2001), based on the memorization and recursive update of a small number of summary statistics, but they either don’t take into account the specific variability of the clusters, or assume that the random vectors which are processed and grouped have uncorrelated components. Unfortunately this is not the case in many practical situations. We here propose a new algorithm to process data streams, with data having correlated components and coming from clusters with different covariance matrices. Such covariance matrices are estimated via an optimal double shrinkage method, which provides positive definite estimates even in presence of a few data points, or of data having components with small variance. This is needed to invert the matrices and compute the Mahalanobis distances that we use for the data assignment to the clusters. We also estimate the total number of clusters from the data."
    },
    {
        "url": "https://paperity.org/p/85499320/a-computing-platform-for-pairs-trading-online-implementation-via-a-blended-kalman-hmm",
        "title": "A computing platform for pairs-trading online implementation via a blended Kalman-HMM filtering approach",
        "authors": [
            "Anton Tenyakov",
            " Rogemar Mamon"
        ],
        "date_article": "12-2017",
        "short_description": "This paper addresses the problem of designing an efficient platform for pairs-trading implementation in real time. Capturing the stylised features of a spread process, i.e., the evolution of the differential between the returns from a pair of stocks, exhibiting a heavy-tailed mean-reverting process is also dealt with. Likewise, the optimal recovery of time-varying parameters in a...",
        "keywords": [
            "algorithm fusion",
            "investment",
            "financial signal processing",
            "change of measure",
            "Ornstein–Uhlenbeck process"
        ],
        "number_of_pages": "20",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-017-0106-3.pdf",
        "abstract": "This paper addresses the problem of designing an efficient platform for pairs-trading implementation in real time. Capturing the stylised features of a spread process, i.e., the evolution of the differential between the returns from a pair of stocks, exhibiting a heavy-tailed mean-reverting process is also dealt with. Likewise, the optimal recovery of time-varying parameters in a return-spread model is tackled. It is important to solve such issues in an integrated manner to carry out the execution of trading strategies in a dynamic market environment. The Kalman and hidden Markov model (HMM) multi-regime dynamic filtering approaches are fused together to provide a powerful method for pairs-trading actualisation. Practitioners’ considerations are taken into account in the way the new filtering method is automated. The synthesis of the HMM’s expectation–maximisation algorithm and Kalman filtering procedure gives rise to a set of self-updat-ing optimal parameter estimates. The method put forward in this paper is a hybridisa-tion of signal-processing algorithms. It highlights the critical role and beneficial utility of data fusion methods. Its appropriateness and novelty support the advancements of accurate predictive analytics involving big financial data sets. The algorithm’s perfor-mance is tested on historical return spread between Coca-Cola and Pepsi Inc.’s equities. Through a back-testing trade, a hypothetical trader might earn a non-zero profit under the assumption of no transaction costs and bid-ask spreads. The method’s success is illustrated by a trading simulation. The findings from this work show that there is high potential to gain when the transaction fees are low, and an investor is able to benefit from the proposed interplay of the two filtering methods."
    },
    {
        "url": "https://paperity.org/p/85653668/improving-mapreduce-privacy-by-implementing-multi-dimensional-sensitivity-based",
        "title": "Improving MapReduce privacy by implementing multi-dimensional sensitivity-based anonymization",
        "authors": [
            "Mohammed Al-Zobbi",
            " Seyed Shahrestani",
            " Chun Ruan"
        ],
        "date_article": "12-2017",
        "short_description": "Big data is predominantly associated with data retrieval, storage, and analytics. Data analytics is prone to privacy violations and data disclosures, which can be partly attributed to the multi-user characteristics of big data environments. Adversaries may link data to external resources, try to access confidential data, or deduce private information from the large number of data...",
        "keywords": [
            "Anonymization",
            "Big data",
            "Data privacy",
            "Granular access",
            "Hadoop",
            "MapReduce"
        ],
        "number_of_pages": "23",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-017-0104-5.pdf",
        "abstract": "Big data is predominantly associated with data retrieval, storage, and analytics. Data analytics is prone to privacy violations and data disclosures, which can be partly attrib‑uted to the multi‑user characteristics of big data environments. Adversaries may link data to external resources, try to access confidential data, or deduce private informa‑tion from the large number of data pieces that they can obtain. Data anonymization can address some of these concerns by providing tools to mask and can help with con‑cealing the vulnerable data. Currently available anonymization methods, however, are not capable of accommodating the big data scalability, granularity, and performance in efficient manners. In this paper, we introduce a novel framework that implements SQL‑like Hadoop ecosystems, incorporating Pig Latin with the additional splitting of data. The splitting reduces data masking and increases the information gained from the anonymized data. Our solution provides a fine‑grained masking and conceal‑ment, which is based on access level privileges of the user. We also introduce a simple classification technique that can accurately measure the anonymization extent in any anonymized data. The results of testing this classification technique and the proposed sensitivity‑based anonymization method using different samples will also be discussed. These results show the significant benefits of the proposed approach, particularly regarding reduced information loss associated with the anonymization processes."
    },
    {
        "url": "https://paperity.org/p/85592906/some-dimension-reduction-strategies-for-the-analysis-of-survey-data",
        "title": "Some dimension reduction strategies for the analysis of survey data",
        "authors": [
            "Jiaying Weng",
            " Derek S. Young"
        ],
        "date_article": "12-2017",
        "short_description": "In the era of big data, researchers interested in developing statistical models are challenged with how to achieve parsimony. Usually, some sort of dimension reduction strategy is employed. Classic strategies are often in the form of traditional inference procedures, such as hypothesis testing; however, the increase in computing capabilities has led to the development of more...",
        "keywords": [
            "Big data",
            "Central mean subspace",
            "Flexible models",
            "Official statistics",
            "Principal component analysis",
            "Sufficient dimension reduction"
        ],
        "number_of_pages": "19",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-017-0103-6.pdf",
        "abstract": "In the era of big data, researchers interested in developing statistical models are chal-lenged with how to achieve parsimony. Usually, some sort of dimension reduction strategy is employed. Classic strategies are often in the form of traditional inference procedures, such as hypothesis testing; however, the increase in computing capabili-ties has led to the development of more sophisticated methods. In particular, sufficient dimension reduction has emerged as an area of broad and current interest. While these types of dimension reduction strategies have been employed for numerous data problems, they are scantly discussed in the context of analyzing survey data. This paper provides an overview of some classic and modern dimension reduction methods, followed by a discussion of how to use the transformed variables in the context of analyzing survey data. We highlight some of these methods with an analysis of health insurance coverage using the US Census Bureau’s 2015 Planning Database."
    },
    {
        "url": "https://paperity.org/p/85406736/mining-and-prioritization-of-association-rules-for-big-data-multi-criteria-decision",
        "title": "Mining and prioritization of association rules for big data: multi-criteria decision analysis approach",
        "authors": [
            "Addi Ait-Mlouk",
            " Tarik Agouti",
            " Fatima Gharnati"
        ],
        "date_article": "11-2017",
        "short_description": "Data mining techniques and extracting patterns from large datasets play a vital role in knowledge discovery. Most of the decision makers encounter a large number of decision rules resulted from association rules mining. Moreover, the volume of datasets brings a new challenge to extract patterns such as the cost of computing and inefficiency to achieve the relevant rules. To...",
        "keywords": [
            "Data mining",
            "Association rules",
            "PFP-growth",
            "Big data",
            "Apache Spark",
            "Road accident",
            "Multi-criteria decision analysis"
        ],
        "number_of_pages": "21",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-017-0105-4.pdf",
        "abstract": "Data mining techniques and extracting patterns from large datasets play a vital role in knowledge discovery. Most of the decision makers encounter a large number of decision rules resulted from association rules mining. Moreover, the volume of data‑sets brings a new challenge to extract patterns such as the cost of computing and inefficiency to achieve the relevant rules. To overcome these challenges, this paper aims to build a learning model based on FP‑growth and Apache Spark framework to process and to extract relevant association rules. We also integrate the multi‑criteria decision analysis to prioritize the extracted rules by taking into account the decision makers subjective judgment. We believe that this approach would be a useful model to follow, particularly for decision makers who are suffering from conflicts between extracted rules, and difficulties of building only the most interesting rules. Experimental results on road accidents analysis show that the proposed approach can be efficiently achieved more association rules with a higher accuracy rate and improve the response time of the proposed algorithm. The results make clear that the proposed approach performs well and can provide useful information that could help the decision makers to improve road safety."
    },
    {
        "url": "https://paperity.org/p/85362015/hcudablast-an-implementation-of-blast-on-hadoop-and-cuda",
        "title": "HCudaBLAST: an implementation of BLAST on Hadoop and Cuda",
        "authors": [
            "Nilay Khare",
            " Alind Khare",
            " Farhan Khan"
        ],
        "date_article": "11-2017",
        "short_description": "The world of DNA sequencing has not only been a difficult field since it was first worked upon, but it is also growing at an exponential rate. The amount of data involved in DNA searching is huge, thereby normal tools or algorithms are not suitable to handle this degree of data processing. BLAST is a tool given by National Center for Biotechnology Information (NCBI) to compare...",
        "keywords": [
            "DNA Searching",
            "BLAST",
            "CUDA",
            "Hadoop"
        ],
        "number_of_pages": "8",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-017-0102-7.pdf",
        "abstract": "The world of DNA sequencing has not only been a difficult field since it was first worked upon, but it is also growing at an exponential rate. The amount of data involved in DNA searching is huge, thereby normal tools or algorithms are not suit-able to handle this degree of data processing. BLAST is a tool given by National Center for Biotechnology Information (NCBI) to compare nucleotide or protein sequences to sequence databases and calculate the statistical significance of matches. Many variants of BLAST such as blastn, blastp, blastx, etc. are used to search for nucleotides, proteins, nucleotides-to-proteins sequences respectively. GPU-BLAST and HBLAST have already been proposed to handle the vast amount of data involved in searching DNA sequenc-ing and they also speedup the searching process. In this article, we propose a new model for searching DNA sequences—HCudaBLAST. It involves CUDA processing and Hadoop combined for efficient searching. The results recorded after implementing HCudaBLAST are shown. This solution combines the multi-core parallelism of GPGPUs and the scalability feature provided by the Hadoop framework."
    },
    {
        "url": "https://paperity.org/p/85438180/understanding-deep-learning-via-backtracking-and-deconvolution",
        "title": "Understanding deep learning via backtracking and deconvolution",
        "authors": [
            "Xing Fang"
        ],
        "date_article": "11-2017",
        "short_description": "Convolutional neural networks are widely adopted for solving problems in image classification. In this work, we aim to gain a better understanding of deep learning through exploring the miss-classified cases in facial and emotion recognitions. Particularly, we propose the backtracking algorithm in order to track down the activated pixels among the last layer of feature maps. We...",
        "keywords": [
            "Deep learning",
            "Convolutional neural networks",
            "Backtracking",
            "Deconvolution"
        ],
        "number_of_pages": "14",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-017-0101-8.pdf",
        "abstract": "Convolutional neural networks are widely adopted for solving problems in image clas-sification. In this work, we aim to gain a better understanding of deep learning through exploring the miss-classified cases in facial and emotion recognitions. Particularly, we propose the backtracking algorithm in order to track down the activated pixels among the last layer of feature maps. We then are able to visualize the facial features that lead to the miss-classifications, by applying the feature tracking algorithm. A comparative analysis of the activated pixels reveals that for the facial recognition, the activations of the common pixels are decisive for the result of classification; for the emotion recogni-tion, the activations of the unique pixels indeed determine the result of classification."
    },
    {
        "url": "https://paperity.org/p/85394312/the-core-enabling-technologies-of-big-data-analytics-and-context-aware-computing-for",
        "title": "The core enabling technologies of big data analytics and context-aware computing for smart sustainable cities: a review and synthesis",
        "authors": [
            "Simon Elias Bibri",
            " John Krogsti"
        ],
        "date_article": "11-2017",
        "short_description": "Data sensing, information processing, and networking technologies are being fast embedded into the very fabric of the contemporary city to enable the use of innovative solutions to overcome the challenges of sustainability and urbanization. This has been boosted by the new digital transition in ICT. Driving such transition predominantly are big data analytics and context-aware...",
        "keywords": [
            "Smart sustainable cities",
            "Urban sustainability",
            "Big data analytics",
            "Context-aware computing",
            "Sensors",
            "Models",
            "Data processing",
            "Cloud computing",
            "Middleware",
            "Big data and context-aware applications"
        ],
        "number_of_pages": "50",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-017-0091-6.pdf",
        "abstract": "Data sensing, information processing, and networking technologies are being fast embedded into the very fabric of the contemporary city to enable the use of inno-vative solutions to overcome the challenges of sustainability and urbanization. This has been boosted by the new digital transition in ICT. Driving such transition pre-dominantly are big data analytics and context-aware computing and their increasing amalgamation within a number of urban domains, especially as their functionality involve more or less the same core enabling technologies, namely sensing devices, cloud computing infrastructures, data processing platforms, middleware architectures, and wireless networks. Topical studies tend to only pass reference to such technologies or to largely focus on one particular technology as part of big data and context-aware ecosystems in the realm of smart cities. Moreover, empirical research on the topic, with some exceptions, is generally limited to case studies without the use of any common conceptual frameworks. In addition, relatively little attention has been given to the integration of big data analytics and context-aware computing as advanced forms of ICT in the context of smart sustainable cities. This endeavor is a first attempt to address these two major strands of ICT of the new wave of computing in relation to the infor-mational landscape of smart sustainable cities. Therefore, the purpose of this study is to review and synthesize the relevant literature with the objective of identifying and dis-tilling the core enabling technologies of big data analytics and context-aware comput-ing as ecosystems in relevance to smart sustainable cities, as well as to illustrate the key computational and analytical techniques and processes associated with the function-ing of such ecosystems. In doing so, we develop, elucidate, and evaluate the most rel-evant frameworks pertaining to big data analytics and context-aware computing in the context of smart sustainable cities, bringing together research directed at a more con-ceptual, analytical, and overarching level to stimulate new ways of investigating their role in advancing urban sustainability. In terms of originality, a review and synthesis of the technical literature has not been undertaken to date in the urban literature, and in doing so, we provide a basis for urban researchers to draw on a set of conceptual frameworks in future research. The proposed frameworks, which can be replicated and tested in empirical research, will add additional depth and rigor to studies in the field. In addition to reviewing the important works, we highlight important applications as well as challenges and open issues. We argue that big data analytics and context-aware Open Access©  The  Author(s)  2017.  This  article  is  distributed  under  the  terms  of  the  Creative  Commons  Attribution  4.0  International  License  (http://creativecommons.org/licenses/by/4.0/),  which  permits  unrestricted  use,  distribution,  and  reproduction  in  any  medium,  provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made.SURVEY PAPERBibri and Krogstie J Big Data  (2017) 4:38 DOI 10.1186/s40537‑017‑0091‑6*Correspondence:   simoe@ntnu.no 1 Department of Computer and Information Science and Department of Urban Planning and Design, NTNU Norwegian University of Science and Technology, Sem Saelands veie 9, 7491 Trondheim, NorwayFull list of author information is available at the end of the article\nPage 2 of 50Bibri and Krogstie J Big Data  (2017) 4:38 computing are prerequisite technologies for the functioning of smart sustainable cit-ies of the future, as their effects reinforce one another as to their efforts for bringing a whole new dimension to the operating and organizing processes of urban life in terms of employing a wide variety of big data and context-aware applications for advancing sustainability."
    },
    {
        "url": "https://paperity.org/p/85365559/an-algorithm-for-identification-of-natural-disaster-affected-area",
        "title": "An algorithm for identification of natural disaster affected area",
        "authors": [
            "M. V. Sangameswar",
            " M. Nagabhushana Rao",
            " S. Satyanarayan"
        ],
        "date_article": "11-2017",
        "short_description": "An important source of information presently is social media, which reports any major event including natural disasters. Social media also includes conversational data. As a result, the volume of data on social media has an enormous increase. During the time of natural disaster like floods, tsunami, earthquake, landslide, etc., people require information in those situations, so...",
        "keywords": [
            "Twitter",
            "Twitter API",
            "R-Studio",
            "Natural disasters",
            "Real-time systems",
            "Media",
            "Geoparsing",
            "Event detection",
            "Data analytics",
            "Sentiment analysis"
        ],
        "number_of_pages": "11",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-017-0096-1.pdf",
        "abstract": "An important source of information presently is social media, which reports any major event including natural disasters. Social media also includes conversational data. As a result, the volume of data on social media has an enormous increase. During the time of natural disaster like floods, tsunami, earthquake, landslide, etc., people require information in those situations, so that relief operations like help, medical facilities can save many lives (Bifet et al. in J Mach Learn Res Proc Track 17:5–11, 2011). An attempt is made in this article on Geoparsing which will identify the places of disaster on a Map. Geoparsing is a process of converting free text description of locations into the geographical identifier in an unambiguous manner with the help of longitude and latitude. With the help of geographical coordinates, it can be mapped and entered into geographical information system. A real-time, reliable at robust twitter messages which are the source of the information can handle a large amount of data. After collecting tweets at the real time we can parse them for the disaster situation and its location. This information will help to identify the exact location of the event. For knowing infor-mation on the natural disaster, tweets are extracted from twitter to R-Studio environ-ment. First the extracted tweets from twitter are parsed using R about “Natural Disas-ter”. Later we parsed the tweets and store in CSV format in R database. For all posted data tweets are calculated and stored in a file. Later visual analysis is performed for the data store using R Statistical Software. Further, it is useful to assess the severity of the natural disaster. Sentiment analysis (Rahmath in IJAIEM 3(5):1–3, 2014) of user tweets is useful for decision making (Rao et al. in Int J Comput Sci Inf Technol 6(3):2923–7, 2015)."
    },
    {
        "url": "https://paperity.org/p/85077193/a-framework-for-the-estimation-and-reduction-of-hospital-readmission-penalties-using",
        "title": "A framework for the estimation and reduction of hospital readmission penalties using predictive analytics",
        "authors": [
            "Christopher Baechle",
            " Ankur Agarw"
        ],
        "date_article": "11-2017",
        "short_description": "BackgroundRecent US legislation imposes financial penalties on hospitals with excessive patient readmissions. Predictive analytics for hospital readmissions have seen an increase in research due to the passage of this legislation. However, many current systems ignore the formulas used by the Centers for Medicare and Medicaid Services for imposing penalties. This research expands...",
        "keywords": [
            "Scientific algorithms of big data",
            "Big data applications",
            "Big data tools",
            "Natural language processing",
            "Naïve Bayes classification"
        ],
        "number_of_pages": "15",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-017-0098-z.pdf",
        "abstract": "Background:  Recent US legislation imposes financial penalties on hospitals with excessive patient readmissions. Predictive analytics for hospital readmissions have seen an increase in research due to the passage of this legislation. However, many current systems ignore the formulas used by the Centers for Medicare and Medicaid Services for imposing penalties. This research expands upon current methodologies and directly incorporates federal penalization formulas when selecting patients for which to dedi-cate resources.Methods:  Hospital discharge summaries are structured using clinical natural language processing techniques. Naïve Bayes classifiers are then used to assign a probability of readmission to each patient. Hospital Readmission Reductions Program formulas and probability of readmission are applied using four readmission scenarios to estimate the cost of readmission. The highest cost patients are identified and readmission mitigation efforts are attempted.Results:  The results show that the average penalty savings over currently employed binary classification to be 51.93%. Binary classification is also shown to select more patients than necessary for readmission intervention. Additionally, intervening in only high-risk patients saved an average of 90.07% compared to providing all patients with costly aftercare.Conclusion:  Focusing resources toward the potentially most expensive patients offers considerably better results than unfocused efforts. Utilizing direct calculation to esti-mate readmission costs has shown to be a more efficient use of resources than current readmission reduction methods."
    },
    {
        "url": "https://paperity.org/p/84961706/mining-and-visualising-contradictory-data",
        "title": "Mining and visualising contradictory data",
        "authors": [
            "Honour Chika Nwagwu",
            " George Okereke",
            " Chukwuemeka Nwobodo"
        ],
        "date_article": "10-2017",
        "short_description": "Big datasets are often stored in flat files and can contain contradictory data. Contradictory data undermines the soundness of the information from a noisy dataset. Traditional tools such as pie chart and bar chart are overwhelmed when used to visually identify contradictory data in multidimensional attribute-values of a big dataset. This work explains the importance of...",
        "keywords": [
            "ConTra",
            "Comma separated values",
            "Dataset",
            "Contradictions",
            "Contradictory data",
            "Mutual exclusion values"
        ],
        "number_of_pages": "11",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-017-0100-9.pdf",
        "abstract": "Big datasets are often stored in flat files and can contain contradictory data. Contradic-tory data undermines the soundness of the information from a noisy dataset. Tradi-tional tools such as pie chart and bar chart are overwhelmed when used to visually identify contradictory data in multidimensional attribute-values of a big dataset. This work explains the importance of identifying contradictions in a noisy dataset. It also examines how contradictory data in a large and noisy dataset can be mined and visu-ally analysed. The authors developed ‘ConTra’, an open source application which applies mutual exclusion rule in identifying contradictory data, existing in comma separated values (CSV ) dataset. ConTra’s capability to enable the identification of contradictory data in different sizes of datasets is examined. The results show that ConTra can pro-cess large dataset when hosted in servers with fast processors. It is also shown in this work that ConTra is 100% accurate in identifying contradictory data of objects whose attribute values do not conform to the mutual exclusion rule of a dataset in CSV format. Different approaches through which ConTra can mine and identify contradictory data are also presented."
    },
    {
        "url": "https://paperity.org/p/84965295/learning-topic-description-from-clustering-of-trusted-user-roles-and-event-models",
        "title": "Learning topic description from clustering of trusted user roles and event models characterizing distributed provenance networks: a reinforcement learning approach",
        "authors": [
            "Sanjoy Kumar Mukherjee",
            " Sivaji Bandyopadhyay"
        ],
        "date_article": "10-2017",
        "short_description": "This paper proposes a reinforcement learning based message transfer model for transferring news report messages through a selected path in a trusted provenance network with the objective of maximizing the reward values based on trust or importance based and network congestion or utility based cost measures. The reward values are calculated along a dynamically defined policy path...",
        "keywords": [
            "Computational trust",
            "Reinforcement learning",
            "Q Learning",
            "Policy path",
            "Reward",
            "Provenance",
            "Bayesian model",
            "Materialized views",
            "Network congestion",
            "Database roles"
        ],
        "number_of_pages": "34",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-017-0097-0.pdf",
        "abstract": "This paper proposes a reinforcement learning based message transfer model for transferring news report messages through a selected path in a trusted provenance network with the objective of maximizing the reward values based on trust or impor-tance based and network congestion or utility based cost measures. The reward values are calculated along a dynamically defined policy path connecting start topic or event node to a goal topic or event or issue nodes for incrementally defined time windows for a given network congestion situation. A hierarchy of agents of trusted roles is used to accomplish the sub-goals associated with sub-story or subtopic in the provenance structure where an agent role may assume the semantic role of the associated sub-topic. The twitted news story thread or plan of events is defined in this work from the starting topic or event node to the goal topic or event node for incrementally defined intervals of time. The graphs are clustered into subtopic and these sub-goals or sub topic nodes of a topic node at every level of granularity are associated with cluster of news reports which describe activities associated with sub-goal or sub-topic events. Such cluster of nodes may also represent drilled down sequence of sub-events describ-ing a sub-topic or sub-goal node. The policy path in a topic or story graph model is defined by applying reinforcement learning principles on dynamically defined event models associated with evolution of topic definition observed from incrementally acquired samples of input training data spanning multiple time windows. We provide a methodology for unifying similar provenance graph models for adapting and averag-ing the policy path classifiers associated with individual models to produce a reduced set of unified models derived during training. A minimum set cover of classifiers is identified for the models and a clustering procedure of the models is suggested based on these classifiers. Other database clustering methods have also been suggested as alternatives for clustering these models. A collection of unified models are identified from the models identified within a cluster and the policy path classifiers associated with these models provide the story or topic descriptions destined to goal topic or event nodes characterizing these models within a cluster."
    },
    {
        "url": "https://paperity.org/p/85031730/using-deep-learning-for-short-text-understanding",
        "title": "Using deep learning for short text understanding",
        "authors": [
            "Justin Zhan",
            " Binay Dah"
        ],
        "date_article": "10-2017",
        "short_description": "Classifying short texts to one category or clustering semantically related texts is challenging, and the importance of both is growing due to the rise of microblogging platforms, digital news feeds, and the like. We can accomplish this classifying and clustering with the help of a deep neural network which produces compact binary representations of a short text, and can assign...",
        "keywords": [
            "Short text classification",
            "Semantic enrichment",
            "Deep neural network"
        ],
        "number_of_pages": "15",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-017-0095-2.pdf",
        "abstract": "Classifying short texts to one category or clustering semantically related texts is chal-lenging, and the importance of both is growing due to the rise of microblogging platforms, digital news feeds, and the like. We can accomplish this classifying and clustering with the help of a deep neural network which produces compact binary rep-resentations of a short text, and can assign the same category to texts that have similar binary representations. But problems arise when there is little contextual information on the short texts, which makes it difficult for the deep neural network to produce similar binary codes for semantically related texts. We propose to address this issue using semantic enrichment. This is accomplished by taking the nouns, and verbs used in the short texts and generating the concepts and co-occurring words with the help of those terms. The nouns are used to generate concepts within the given short text, whereas the verbs are used to prune the ambiguous context (if any) present in the text. The enriched text then goes through a deep neural network to produce a prediction label for that short text representing it’s category."
    },
    {
        "url": "https://paperity.org/p/85126651/adaptive-memory-based-single-distribution-resampling-for-particle-filter",
        "title": "Adaptive memory-based single distribution resampling for particle filter",
        "authors": [
            "Wan Mohd Yaakob Wan Bejuri",
            " Mohd Murtadha Mohamad",
            " Raja Zahilah Raja Mohd Radzi"
        ],
        "date_article": "10-2017",
        "short_description": "The restrictions that are related to using single distribution resampling for some specific computing devices’ memory gives developers several difficulties as a result of the increased effort and time needed for the development of a particle filter. Thus, one needs a new sequential resampling algorithm that is flexible enough to allow it to be used with various computing devices...",
        "keywords": [
            "Particle filter",
            "Resampling",
            "Sequential implementation",
            "Memory consumption"
        ],
        "number_of_pages": "22",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-017-0094-3.pdf",
        "abstract": "The restrictions that are related to using single distribution resampling for some specific computing devices’ memory gives developers several difficulties as a result of the increased effort and time needed for the development of a particle filter. Thus, one needs a new sequential resampling algorithm that is flexible enough to allow it to be used with various computing devices. Therefore, this paper formulated a new single distribution resampling called the adaptive memory size-based single distribution resa-mpling (AMSSDR). This resampling method integrates traditional variation resampling and traditional resampling in one architecture. The algorithm changes the resampling algorithm using the memory in a computing device. This helps the developer formu-late a particle filter without over considering the computing devices’ memory utilisa-tion during the development of different particle filters. At the start of the operational process, it uses the AMSSDR selector to choose an appropriate resampling algorithm (for example, rounding copy resampling or systematic resampling), based on the cur-rent computing devices’ physical memory. If one chooses systematic resampling, the resampling will sample every particle for every cycle. On the other hand, if it chooses the rounding copy resampling, the resampling will sample more than one of each cycle’s particle. This illustrates that the method (AMSSDR) being proposed is capable of switching resampling algorithms based on various physical memory requirements. The aim of the authors is to extend this research in the future by applying their proposed method in various emerging applications such as real-time locator systems or medical applications."
    },
    {
        "url": "https://paperity.org/p/85179150/dimensionality-reduction-and-class-prediction-algorithm-with-application-to-microarray",
        "title": "Dimensionality reduction and class prediction algorithm with application to microarray Big Data",
        "authors": [
            "Fadoua Badaoui",
            " Amine Amar",
            " Laila Ait Hassou"
        ],
        "date_article": "10-2017",
        "short_description": "The recent technology development in the concern of microarray experiments has provided many new potentialities in terms of simultaneous measurement. But new challenges have arisen from these massive quantities of information qualified as Big Data. The challenge consists to extract the main information containing the sense from the data. To this end researchers are using various...",
        "keywords": [
            "Linear discriminant analysis",
            "Tumor classification",
            "Basis vectors",
            "Kendall rank correlation",
            "Cancer"
        ],
        "number_of_pages": "11",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-017-0093-4.pdf",
        "abstract": "The recent technology development in the concern of microarray experiments has provided many new potentialities in terms of simultaneous measurement. But new challenges have arisen from these massive quantities of information qualified as Big Data. The challenge consists to extract the main information containing the sense from the data. To this end researchers are using various techniques as “hierarchical clustering”, “mutual information” and “self-organizing maps” to name a few. However, the management and analysis of the millions resulting dataset haven’t yet reached a satisfactory level, and there is no clear consensus about the best method/methods revealing patterns of gene expression. Thus, many efforts are required to strengthen the methodologies for optimal analysis of Big Data. In this paper, we propose a new processing approach which is structured on feature extraction and selection. The feature extraction, is based on correlation and rank analysis and leads to a reduction of the number of variables. The feature selection, consists in eliminating redundant or irrelevant variables, using some adapted techniques of discriminant analysis. Our approach is tested on three type of cancer gene expression microarray and compared with concurrent other approaches. It performs well, in terms of prediction results, com-putation and processing time."
    },
    {
        "url": "https://paperity.org/p/84820285/a-novel-hybrid-model-based-on-hodrick-prescott-filter-and-support-vector-regression",
        "title": "A novel hybrid model based on Hodrick–Prescott filter and support vector regression algorithm for optimizing stock market price prediction",
        "authors": [
            "Meryem Ouahilal",
            " Mohammed El Mohajir",
            " Mohamed Chahhou"
        ],
        "date_article": "10-2017",
        "short_description": "Predicting stock market price is considered as a challenging task of financial time series analysis, which is of great interest to stock investors, stock traders and applied researchers. Many machine learning techniques have been used in this area to predict the stock market price, including regression algorithms which can be useful tools to provide good performance of financial...",
        "keywords": [
            "Stock price prediction",
            "Financial time series forecasting",
            "Business analytics",
            "Support vector regression",
            "Noise filtering techniques",
            "Hodrick–Prescott filter",
            "Decision support"
        ],
        "number_of_pages": "22",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-017-0092-5.pdf",
        "abstract": "Predicting stock market price is considered as a challenging task of financial time series analysis, which is of great interest to stock investors, stock traders and applied research-ers. Many machine learning techniques have been used in this area to predict the stock market price, including regression algorithms which can be useful tools to provide good performance of financial time series prediction. Support Vector Regression is one of the most powerful algorithms in machine learning. There have been countless suc-cesses in utilizing SVR algorithm for stock market prediction. In this paper, we propose a novel hybrid approach based on machine learning and filtering techniques. Our proposed approach combines Support Vector Regression and Hodrick–Prescott filter in order to optimize the prediction of stock price. To assess the performance of this proposed approach, we have conducted several experiments using real world datasets. The principle objective of this paper is to demonstrate the improvement in predictive performance of stock market and verify the works of our proposed model in compari-son with other optimized models. The experimental results confirm that the proposed algorithm constitutes a powerful model for predicting stock market prices."
    },
    {
        "url": "https://paperity.org/p/84902508/a-bibliometric-approach-to-tracking-big-data-research-trends",
        "title": "A bibliometric approach to tracking big data research trends",
        "authors": [
            "Ali Kalantari",
            " Amirrudin Kamsin",
            " Halim Shukri Kamaruddin"
        ],
        "date_article": "10-2017",
        "short_description": "The explosive growing number of data from mobile devices, social media, Internet of Things and other applications has highlighted the emergence of big data. This paper aims to determine the worldwide research trends on the field of big data and its most relevant research areas. A bibliometric approach was performed to analyse a total of 6572 papers including 28 highly cited...",
        "keywords": [
            "Big data",
            "Research trends",
            "Highly cited papers",
            "Citation analysis"
        ],
        "number_of_pages": "18",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-017-0088-1.pdf",
        "abstract": "The explosive growing number of data from mobile devices, social media, Internet of Things and other applications has highlighted the emergence of big data. This paper aims to determine the worldwide research trends on the field of big data and its most relevant research areas. A bibliometric approach was performed to analyse a total of 6572 papers including 28 highly cited papers and only papers that were published in the Web of ScienceTM Core Collection database from 1980 to 19 March 2015 were selected. The results were refined by all relevant Web of Science categories to com-puter science, and then the bibliometric information for all the papers was obtained. Microsoft Excel version 2013 was used for analyzing the general concentration, disper-sion and movement of the pool of data from the papers. The t test and ANOVA were used to prove the hypothesis statistically and characterize the relationship among the variables. A comprehensive analysis of the publication trends is provided by document type and language, year of publication, contribution of countries, analysis of journals, analysis of research areas, analysis of web of science categories, analysis of authors, analysis of author keyword and keyword plus. In addition, the novelty of this study is that it provides a formula from multi-regression analysis for citation analysis based on the number of authors, number of pages and number of references."
    },
    {
        "url": "https://paperity.org/p/84608667/a-survey-on-heterogeneous-transfer-learning",
        "title": "A survey on heterogeneous transfer learning",
        "authors": [
            "Oscar Day",
            " Taghi M. Khoshgoftaar"
        ],
        "date_article": "09-2017",
        "short_description": "Transfer learning has been demonstrated to be effective for many real-world applications as it exploits knowledge present in labeled training data from a source domain to enhance a model’s performance in a target domain, which has little or no labeled target training data. Utilizing a labeled source, or auxiliary, domain for aiding a target task can greatly reduce the cost and...",
        "keywords": [
            "Transfer learning",
            "Heterogeneous transfer learning",
            "Knowledge transfer",
            "Supervised learning",
            "Semisupervised learning",
            "Unsupervised learning"
        ],
        "number_of_pages": "42",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-017-0089-0.pdf",
        "abstract": "Transfer learning has been demonstrated to be effective for many real-world applica-tions as it exploits knowledge present in labeled training data from a source domain to enhance a model’s performance in a target domain, which has little or no labeled target training data. Utilizing a labeled source, or auxiliary, domain for aiding a target task can greatly reduce the cost and effort of collecting sufficient training labels to cre-ate an effective model in the new target distribution. Currently, most transfer learning methods assume the source and target domains consist of the same feature spaces which greatly limits their applications. This is because it may be difficult to collect auxiliary labeled source domain data that shares the same feature space as the target domain. Recently, heterogeneous transfer learning methods have been developed to address such limitations. This, in effect, expands the application of transfer learning to many other real-world tasks such as cross-language text categorization, text-to-image classification, and many others. Heterogeneous transfer learning is characterized by the source and target domains having differing feature spaces, but may also be combined with other issues such as differing data distributions and label spaces. These can present significant challenges, as one must develop a method to bridge the feature spaces, data distributions, and other gaps which may be present in these cross-domain learning tasks. This paper contributes a comprehensive survey and analysis of current methods designed for performing heterogeneous transfer learning tasks to provide an updated, centralized outlook into current methodologies."
    },
    {
        "url": "https://paperity.org/p/84495531/clustering-categorical-data-based-on-the-relational-analysis-approach-and-mapreduce",
        "title": "Clustering categorical data based on the relational analysis approach and MapReduce",
        "authors": [
            "Yasmine Lamari",
            " Said Chah Slaoui"
        ],
        "date_article": "09-2017",
        "short_description": "The traditional methods of clustering are unable to cope with the exploding volume of data that the world is currently facing. As a solution to this problem, the research is intensified in the direction of parallel clustering methods. Although there is a variety of parallel programming models, the MapReduce paradigm is considered as the most prominent model for problems of large...",
        "keywords": [
            "Categorical data",
            "Clustering",
            "MapReduce",
            "Relational analysis approach"
        ],
        "number_of_pages": "16",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-017-0090-7.pdf",
        "abstract": "The traditional methods of clustering are unable to cope with the exploding volume of data that the world is currently facing. As a solution to this problem, the research is intensified in the direction of parallel clustering methods. Although there is a variety of parallel programming models, the MapReduce paradigm is considered as the most prominent model for problems of large scale data processing of which the clustering. This paper introduces a new parallel design of a recently appeared heuristic for hard clustering using the MapReduce programming model. In this heuristic, clustering is performed by efficiently partitioning categorical large data sets according to the rela-tional analysis approach. The proposed design, called PMR-Transitive, is a single-scan and parameter-free heuristic which determines the number of clusters automatically. The experimental results on real-life and synthetic data sets demonstrate that PMR-Transitive produces good quality results."
    },
    {
        "url": "https://paperity.org/p/81306417/clustering-large-datasets-using-k-means-modified-inter-and-intra-clustering-km-i2c-in",
        "title": "Clustering large datasets using K-means modified inter and intra clustering (KM-I2C) in Hadoop",
        "authors": [
            "Chowdam Sreedhar",
            " Nagulapally Kasiviswanath",
            " Pakanti Chenna Reddy"
        ],
        "date_article": "09-2017",
        "short_description": "Big data has become popular for processing, storing and managing massive volumes of data. The clustering of datasets has become a challenging issue in the field of big data analytics. The K-means algorithm is best suited for finding similarities between entities based on distance measures with small datasets. Existing clustering algorithms require scalable solutions to manage...",
        "keywords": [
            "Clustering",
            "Big data",
            "Hadoop",
            "MapReduce",
            "Scalability"
        ],
        "number_of_pages": "19",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-017-0087-2.pdf",
        "abstract": "Big data has become popular for processing, storing and managing massive volumes of data. The clustering of datasets has become a challenging issue in the field of big data analytics. The K-means algorithm is best suited for finding similarities between entities based on distance measures with small datasets. Existing clustering algo-rithms require scalable solutions to manage large datasets. This study presents two approaches to the clustering of large datasets using MapReduce. The first approach, K-Means Hadoop MapReduce (KM-HMR), focuses on the MapReduce implementation of standard K-means. The second approach enhances the quality of clusters to produce clusters with maximum intra-cluster and minimum inter-cluster distances for large datasets. The results of the proposed approaches show significant improvements in the efficiency of clustering in terms of execution times. Experiments conducted on stand-ard K-means and proposed solutions show that the KM-I2C approach is both effective and efficient."
    },
    {
        "url": "https://paperity.org/p/81072672/survey-on-clinical-prediction-models-for-diabetes-prediction",
        "title": "Survey on clinical prediction models for diabetes prediction",
        "authors": [
            "N. Jayanthi",
            " B. Vijaya Babu",
            " N. Sambasiva Rao"
        ],
        "date_article": "08-2017",
        "short_description": "Predictive analytics has gained a lot of reputation in the emerging technology Big data. Predictive analytics is an advanced form of analytics. Predictive analytics goes beyond data mining. A huge amount of medical data is available today regarding the disease, their symptoms, reasons for illness, and their effects on health. But this data is not analysed properly to predict or...",
        "keywords": [
            "Predictive analytics",
            "Diabetes",
            "Clinical prediction models",
            "Traditional model",
            "Hybrid model",
            "Machine learning"
        ],
        "number_of_pages": "15",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-017-0082-7.pdf",
        "abstract": "Predictive analytics has gained a lot of reputation in the emerging technology Big data. Predictive analytics is an advanced form of analytics. Predictive analytics goes beyond data mining. A huge amount of medical data is available today regarding the disease, their symptoms, reasons for illness, and their effects on health. But this data is not analysed properly to predict or to study a disease. The aim of this paper is to give a detailed version of predictive models from base to state-of-art, describing various types of predictive models, steps to develop a predictive model, their applications in health care in a broader way and particularly in diabetes."
    },
    {
        "url": "https://paperity.org/p/80394234/improved-sqrt-cosine-similarity-measurement",
        "title": "Improved sqrt-cosine similarity measurement",
        "authors": [
            "Sahar Sohangir",
            " Dingding Wang"
        ],
        "date_article": "07-2017",
        "short_description": "Text similarity measurement aims to find the commonality existing among text documents, which is fundamental to most information extraction, information retrieval, and text mining problems. Cosine similarity based on Euclidean distance is currently one of the most widely used similarity measurements. However, Euclidean distance is generally not an effective metric for dealing...",
        "keywords": [
            "Similarity measure",
            "Information retrieval",
            "Hellinger distance"
        ],
        "number_of_pages": "13",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-017-0083-6.pdf",
        "abstract": "Text similarity measurement aims to find the commonality existing among text docu-ments, which is fundamental to most information extraction, information retrieval, and text mining problems. Cosine similarity based on Euclidean distance is currently one of the most widely used similarity measurements. However, Euclidean distance is generally not an effective metric for dealing with probabilities, which are often used in text analytics. In this paper, we propose a new similarity measure based on sqrt-cosine similarity. We apply the proposed improved sqrt-cosine similarity to a variety of document-understanding tasks, such as text classification, clustering, and query search. Comprehensive experiments are then conducted to evaluate our new similarity meas-urement in comparison to existing methods. These experimental results show that our proposed method is indeed effective."
    },
    {
        "url": "https://paperity.org/p/80001321/iihadoop-an-asynchronous-distributed-framework-for-incremental-iterative-computations",
        "title": "iiHadoop: an asynchronous distributed framework for incremental iterative computations",
        "authors": [
            "Afaf G. Bin Saadon",
            " Hoda M. O. Mokhtar"
        ],
        "date_article": "07-2017",
        "short_description": "It is true that data is never static; it keeps growing and changing over time. New data is added and old data can either be modified or deleted. This incremental nature of data motivates the development of new systems to perform large-scale data computations incrementally. MapReduce was recently introduced to provide an efficient approach for handling large-scale data...",
        "keywords": [
            "Big data",
            "Distributed systems",
            "Hadoop framework",
            "Iterative processing",
            "Incremental computation"
        ],
        "number_of_pages": "30",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-017-0086-3.pdf",
        "abstract": "It is true that data is never static; it keeps growing and changing over time. New data is added and old data can either be modified or deleted. This incremental nature of data motivates the development of new systems to perform large-scale data computations incrementally. MapReduce was recently introduced to provide an efficient approach for handling large-scale data computations. Nevertheless, it turned to be inefficient in sup-porting the processing of small incremental data. While many previous systems have extended MapReduce to perform iterative or incremental computations, these systems are still inefficient and too expensive to perform large-scale iterative computations on changing data. In this paper, we present a new system called iiHadoop, an extension of Hadoop framework, optimized for incremental iterative computations. iiHadoop accelerates program execution by performing the incremental computations on the small fraction of data that is affected by changes rather than the whole data. In addi-tion, iiHadoop improves the performance by executing iterations asynchronously, and employing locality-aware scheduling for the map and reduce tasks taking into account the incremental and iterative behavior. An evaluation for the proposed iiHadoop framework is presented using examples of iterative algorithms, and the results showed significant performance improvements over comparable existing frameworks."
    },
    {
        "url": "https://paperity.org/p/80080527/large-scale-distributed-l-bfgs",
        "title": "Large-scale distributed L-BFGS",
        "authors": [
            "Maryam M. Najafabadi",
            " Taghi M. Khoshgoftaar",
            " Flavio Villanustr"
        ],
        "date_article": "07-2017",
        "short_description": "With the increasing demand for examining and extracting patterns from massive amounts of data, it is critical to be able to train large models to fulfill the needs that recent advances in the machine learning area create. L-BFGS (Limited-memory Broyden Fletcher Goldfarb Shanno) is a numeric optimization method that has been effectively used for parameter estimation to train...",
        "keywords": [
            "Large-scale L-BFGS implementation",
            "Parallel and distributed processing",
            "HPCC systems"
        ],
        "number_of_pages": "17",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-017-0084-5.pdf",
        "abstract": "With the increasing demand for examining and extracting patterns from massive amounts of data, it is critical to be able to train large models to fulfill the needs that recent advances in the machine learning area create. L-BFGS (Limited-memory Broyden Fletcher Goldfarb Shanno) is a numeric optimization method that has been effec-tively used for parameter estimation to train various machine learning models. As the number of parameters increase, implementing this algorithm on one single machine can be insufficient, due to the limited number of computational resources available. In this paper, we present a parallelized implementation of the L-BFGS algorithm on a distributed system which includes a cluster of commodity computing machines. We use open source HPCC Systems (High-Performance Computing Cluster) platform as the underlying distributed system to implement the L-BFGS algorithm. We initially provide an overview of the HPCC Systems framework and how it allows for the parallel and dis-tributed computations important for Big Data analytics and, subsequently, we explain our implementation of the L-BFGS algorithm on this platform. Our experimental results show that our large-scale implementation of the L-BFGS algorithm can easily scale from training models with millions of parameters to models with billions of parameters by simply increasing the number of commodity computational nodes."
    },
    {
        "url": "https://paperity.org/p/80051774/horizontally-scalable-probabilistic-generalized-suffix-tree-pgst-based-route-prediction",
        "title": "Horizontally scalable probabilistic generalized suffix tree (PGST) based route prediction using map data and GPS traces",
        "authors": [
            "Vishnu Shankar Tiwari",
            " Arti Ary"
        ],
        "date_article": "07-2017",
        "short_description": "Route prediction is an essential requirement for many intelligent transport systems (ITS) services like VANETS, traffic congestion estimation, resource prediction in grid computing etc. This work focuses on building an end-to-end horizontally scalable route prediction application based on statistical modeling of user travel data. Probabilistic suffix tree (PST) is one of widely...",
        "keywords": [
            "Route prediction",
            "Suffix tree",
            "Big Data",
            "Map reduce",
            "HDFS"
        ],
        "number_of_pages": "23",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-017-0085-4.pdf",
        "abstract": "Route prediction is an essential requirement for many intelligent transport systems (ITS) services like VANETS, traffic congestion estimation, resource prediction in grid computing etc. This work focuses on building an end-to-end horizontally scalable route prediction application based on statistical modeling of user travel data. Probabil-istic suffix tree (PST ) is one of widely used sequence indexing technique which serves a model for prediction. The probabilistic generalized suffix tree (PGST ) is a variant of PST and is essentially a suffix tree built from a huge number of smaller sequences. We construct generalized suffix tree model from a large number of trips completed by the users. User trip raw GPS traces is mapped to the digitized road network by parallelizing map matching technique leveraging map reduce framework. PGST construction from the huge volume of data by processing sequentially is a bottleneck in the practical realization. Most of the existing works focused on time-space tradeoffs on a single machine. Proposed technique solves this problem by a two-step process which is intui-tive to execute in the map-reduce framework. In the first step, computes all the suffixes along with their frequency of occurrences and in the second step, builds probabilistic generalized suffix tree. The probabilistic aspect of the tree is also taken care so that it can be used as a model for prediction application. Dataset used are road network spatial data and GPS traces of users. Experiments carried out on real datasets available in public domain."
    },
    {
        "url": "https://paperity.org/p/80078561/big-data-analytics-does-organizational-factor-matters-impact-technology-acceptance",
        "title": "Big data analytics: does organizational factor matters impact technology acceptance?",
        "authors": [
            "Vitor Brock",
            " Habib Ullah Khan"
        ],
        "date_article": "07-2017",
        "short_description": "Ever since the emergence of big data concept, researchers have started applying the concept to various fields and tried to assess the level of acceptance of it with renown models like technology acceptance model (TAM) and it variations. In this regard, this paper tries to look at the factors that associated with the usage of big data analytics, by synchronizing TAM with...",
        "keywords": [
            "Big data",
            "Technology acceptance model (TAM)",
            "Organizational learning capabilities (OLC)",
            "Structural equation model (SEM)",
            "Mediation effect"
        ],
        "number_of_pages": "28",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-017-0081-8.pdf",
        "abstract": "Ever since the emergence of big data concept, researchers have started applying the concept to various fields and tried to assess the level of acceptance of it with renown models like technology acceptance model (TAM) and it variations. In this regard, this paper tries to look at the factors that associated with the usage of big data analytics, by synchronizing TAM with organizational learning capabilities (OLC) framework. These models are applied on the construct, intended usage of big data and also the media-tion effect of the OLC constructs is assessed. The data for the study is collected from the students pertaining to information technology disciplines at University of Liverpool, online programme. Though, invitation to participate e-mails are sent to 1035 students, only 359 members responded back with filled questionnaires. This study uses structural equation modelling and multivariate regression using ordinary least squares estimation to test the proposed hypotheses using the latest statistical software R. It is proved from the analysis that compared to other models, model 4 (which is constructed by using the constructs of OLC and TAM frameworks) is able to explain 44% variation in the usage pattern of big data. In addition to this, the mediation test performed revealed that the interaction between OLC dimensions and TAM dimensions on intended usage of big data has no mediation effect. Thus, this work provided inputs to the research community to look into the relation between the constructs of OLC framework and the selection of big data technology."
    },
    {
        "url": "https://paperity.org/p/80169529/analysis-of-agriculture-data-using-data-mining-techniques-application-of-big-data",
        "title": "Analysis of agriculture data using data mining techniques: application of big data",
        "authors": [
            "Jharna Majumdar",
            " Sneha Naraseeyappa",
            " Shilpa Ankalaki"
        ],
        "date_article": "07-2017",
        "short_description": "In agriculture sector where farmers and agribusinesses have to make innumerable decisions every day and intricate complexities involves the various factors influencing them. An essential issue for agricultural planning intention is the accurate yield estimation for the numerous crops involved in the planning. Data mining techniques are necessary approach for accomplishing...",
        "keywords": [
            "Big Data",
            "PAM",
            "CLARA and DBSCAN"
        ],
        "number_of_pages": "15",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-017-0077-4.pdf",
        "abstract": "In agriculture sector where farmers and agribusinesses have to make innumerable decisions every day and intricate complexities involves the various factors influencing them. An essential issue for agricultural planning intention is the accurate yield estima-tion for the numerous crops involved in the planning. Data mining techniques are necessary approach for accomplishing practical and effective solutions for this prob-lem. Agriculture has been an obvious target for big data. Environmental conditions, variability in soil, input levels, combinations and commodity prices have made it all the more relevant for farmers to use information and get help to make critical farming decisions. This paper focuses on the analysis of the agriculture data and finding optimal parameters to maximize the crop production using data mining techniques like PAM, CLARA, DBSCAN and Multiple Linear Regression. Mining the large amount of existing crop, soil and climatic data, and analysing new, non-experimental data optimizes the production and makes agriculture more resilient to climatic change."
    },
    {
        "url": "https://paperity.org/p/80092688/theory-driven-or-process-driven-prediction-epistemological-challenges-of-big-data",
        "title": "Theory-driven or process-driven prediction? Epistemological challenges of big data analytics",
        "authors": [
            "Ahmed Elragal",
            " Ralf Klischewski"
        ],
        "date_article": "06-2017",
        "short_description": "Most scientists are accustomed to make predictions based on consolidated and accepted theories pertaining to the domain of prediction. However, nowadays big data analytics (BDA) is able to deliver predictions based on executing a sequence of data processing while seemingly abstaining from being theoretically informed about the subject matter. This paper discusses how to deal with...",
        "keywords": [
            "Big data analytics",
            "Epistemological challenges",
            "Information systems theories",
            "Predictive research"
        ],
        "number_of_pages": "20",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-017-0079-2.pdf",
        "abstract": "Most scientists are accustomed to make predictions based on consolidated and accepted theories pertaining to the domain of prediction. However, nowadays big data analytics (BDA) is able to deliver predictions based on executing a sequence of data processing while seemingly abstaining from being theoretically informed about the subject matter. This paper discusses how to deal with the shift from theory-driven to process-driven prediction through analyzing the BDA steps and identifying the epistemological challenges and various needs of theoretically informing BDA through-out data acquisition, preprocessing, analysis, and interpretation. We suggest a theory-driven guidance for the BDA process including acquisition, pre-processing, analytics and interpretation. That is, we propose—in association with these BDA process steps—a lightweight theory-driven approach in order to safeguard the analytics process from epistemological pitfalls. This study may serve as a guideline for researchers and practi-tioners to consider while conducting future big data analytics."
    },
    {
        "url": "https://paperity.org/p/80147153/host-managed-contention-avoidance-storage-solutions-for-big-data",
        "title": "Host managed contention avoidance storage solutions for Big Data",
        "authors": [
            "Pratik Mishra",
            " Arun K. Somani"
        ],
        "date_article": "06-2017",
        "short_description": "The performance gap between compute and storage is fairly considerable. This results in a mismatch between the application needs from storage and what storage can deliver. The full potential of storage devices cannot be harnessed till all layers of I/O hierarchy function efficiently. Despite advanced optimizations applied across various layers along the odyssey of data access...",
        "keywords": [
            "Multi-tier",
            "Hard disk drives",
            "Solid state drives",
            "MapReduce",
            "Hadoop",
            "Hdfs",
            "Contention avoidance",
            "Big Data",
            "Storage",
            "Block I/O layer",
            "I/O scheduler"
        ],
        "number_of_pages": "42",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-017-0080-9.pdf",
        "abstract": "The performance gap between compute and storage is fairly considerable. This results in a mismatch between the application needs from storage and what storage can deliver. The full potential of storage devices cannot be harnessed till all layers of I/O hierarchy function efficiently. Despite advanced optimizations applied across various layers along the odyssey of data access, the I/O stack still remains volatile. The prob-lems associated due to the inefficiencies in data management get amplified in Big Data shared resource environments. The Linux OS (host) block layer is the most critical part of the I/O hierarchy, as it orchestrates the I/O requests from different applica-tions to the underlying storage. Unfortunately, despite it’s significance, the block layer, essentially the block I/O scheduler, hasn’t evolved to meet the needs of Big Data. We have designed and developed two contention avoidance storage solutions, collectively known as “BID: Bulk I/O Dispatch” in the Linux block layer specifically to suit multi-tenant, multi-tasking shared Big Data environments. Hard disk drives (HDDs) form the backbone of data center storage. The data access time in HDDs is majorly governed by disk arm movements, which usually occurs when data is not accessed sequentially. Big Data applications exhibit evident sequentiality but due to the contentions amongst other I/O submitting applications, the I/O accesses get multiplexed which leads to higher disk arm movements. BID schemes aim to exploit the inherent I/O sequentiality of Big Data applications to improve the overall I/O completion time by reducing the avoidable disk arm movements. In the first part, we propose a dynamically adaptable block I/O scheduling scheme BID-HDD for disk based storage. BID-HDD tries to recreate the sequentiality in I/O access in order to provide performance isolation to each I/O submitting process. Through trace driven simulation based experiments with cloud emulating MapReduce benchmarks, we show the effectiveness of BID-HDD which results in 28–52% lesser time for all I/O requests than the best performing Linux disk schedulers. In the second part, we propose a hybrid scheme BID-Hybrid to exploit SCM’s (SSDs) superior random performance to further avoid contentions at disk based storage. BID-Hybrid is able to efficiently offload non-bulky interruptions from HDD request queue to SSD queue using BID-HDD for disk request processing and multi-q FIFO architecture for SSD. This results in performance gain of 6–23% for MapReduce workloads when compared to BID-HDD and 33–54% over best performing Linux scheduling scheme. BID schemes as a whole is aimed to avoid contentions for disk based storage I/Os following system constraints without compromising SLAs."
    },
    {
        "url": "https://paperity.org/p/80186489/deriv-distributed-brand-perception-tracking-framework",
        "title": "DERIV: distributed brand perception tracking framework",
        "authors": [
            "Manu Shukla",
            " Raimundo Dos Santos",
            " Andrew Fong"
        ],
        "date_article": "06-2017",
        "short_description": "Determining user’s perception of a brand in short periods of time has become crucial for business. Distilling brand perception directly from people’s comments in social media has promise. Current techniques for determining brand perception, such as surveys of handpicked users by mail, in person, phone or online, are time consuming and increasingly inadequate. The DERIV system...",
        "keywords": [
            "Big data",
            "Distributed learning",
            "Brand perception",
            "In-memory distribution"
        ],
        "number_of_pages": "23",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-017-0078-3.pdf",
        "abstract": "Determining user’s perception of a brand in short periods of time has become crucial for business. Distilling brand perception directly from people’s comments in social media has promise. Current techniques for determining brand perception, such as sur‑veys of handpicked users by mail, in person, phone or online, are time consuming and increasingly inadequate. The DERIV system distills storylines from open data represent‑ing direct consumer voice into a brand perception. The framework summarizes percep‑tion of a brand in comparison to peer brands with in‑memory distributed algorithms utilizing supervised machine learning techniques. Experiments performed with open data and models built with storylines of known peer brands show the technique as highly scalable and accurate in capturing brand perception from vast amounts of social data compared to sentiment analysis."
    },
    {
        "url": "https://paperity.org/p/79928648/identification-of-top-k-nodes-in-large-networks-using-katz-centrality",
        "title": "Identification of top-K nodes in large networks using Katz centrality",
        "authors": [
            "Justin Zhan",
            " Sweta Gurung",
            " Sai Phani Krishna Pars"
        ],
        "date_article": "05-2017",
        "short_description": "Network theory concepts form the core of algorithms that are designed to uncover valuable insights from various datasets. Especially, network centrality measures such as Eigenvector centrality, Katz centrality, PageRank centrality etc., are used in retrieving top-K viral information propagators in social networks,while web page ranking in efficient information retrieval, etc. In...",
        "keywords": [
            "Top-K nodes",
            "Katz centrality",
            "Social networks"
        ],
        "number_of_pages": "19",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-017-0076-5.pdf",
        "abstract": "Network theory concepts form the core of algorithms that are designed to uncover valuable insights from various datasets. Especially, network centrality measures such as Eigenvector centrality, Katz centrality, PageRank centrality etc., are used in retrieving top-K viral information propagators in social networks,while web page ranking in effi-cient information retrieval, etc. In this paper, we propose a novel method for identifying top-K viral information propagators from a reduced search space. Our algorithm com-putes the Katz centrality and Local average centrality values of each node and tests the values against two threshold (constraints) values. Only those nodes, which satisfy these constraints, form the search space for top-K propagators. Our proposed algo-rithm is tested against four datasets and the results show that the proposed algorithm is capable of reducing the number of nodes in search space at least by 70%. We also considered the parameter (α and β) dependency of Katz centrality values in our experi-ments and established a relationship between the α values, number of nodes in search space and network characteristics. Later, we compare the top-K results of our approach against the top-K results of degree centrality."
    },
    {
        "url": "https://paperity.org/p/79874738/impact-of-reviewer-social-interaction-on-online-consumer-review-fraud-detection",
        "title": "Impact of reviewer social interaction on online consumer review fraud detection",
        "authors": [
            "Kunal Goswami",
            " Younghee Park",
            " Chungsik Song"
        ],
        "date_article": "05-2017",
        "short_description": "Background Online consumer reviews have become a baseline for new consumers to try out a business or a new product. The reviews provide a quick look into the application and experience of the business/product and market it to new customers. However, some businesses or reviewers use these reviews to spread fake information about the business/product. The fake information can be...",
        "keywords": [
            "Opinion spam",
            "Neural networks",
            "Social interactive behavior"
        ],
        "number_of_pages": "19",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-017-0075-6.pdf",
        "abstract": "Background:  Online consumer reviews have become a baseline for new consum-ers to try out a business or a new product. The reviews provide a quick look into the application and experience of the business/product and market it to new customers. However, some businesses or reviewers use these reviews to spread fake information about the business/product. The fake information can be used to promote a relatively average product/business or can be used to malign their competition. This activity is known as reviewer fraud or opinion spam. The paper proposes a feature set, capturing the user social interaction behavior to identify fraud. The problem being solved is one of the characteristics that lead to fraud rather than detecting fraud.Methods:  Neural network algorithm is used to evaluate the proposed feature set and compare it against the state-of-the-art feature sets in detecting fraud. The feature set considers the user’s social interaction on the Yelp platform to determine if the user is committing fraud. The neural network algorithm helps in comparing the feature set with other feature sets used to detect fraud. Any attempt to find the characteristics that lead to fraud has a prerequisite to be good enough to detect fraud as well.Results:  The F1 score obtained using neural networks is on par with all the well-known methods for detecting fraud, a value of 0.95. The effectiveness of the feature set is in rivaling the other approaches to fraud detection.Conclusions:  A user’s social interaction on a digital platform such as Yelp is equally important in evaluating the user as social interaction is in real life. The characteristics that lead to fraud can be intuitively captured. The characteristics such as number of friends, number of followers and the number of times the user has provided a review which was helpful to multiple people provide the neural network with a base to form a relationship between opinion fraud and social interaction characteristics."
    },
    {
        "url": "https://paperity.org/p/79835090/botnet-detection-using-graph-based-feature-clustering",
        "title": "Botnet detection using graph-based feature clustering",
        "authors": [
            "Sudipta Chowdhury",
            " Mojtaba Khanzadeh",
            " Ravi Aku"
        ],
        "date_article": "05-2017",
        "short_description": "Detecting botnets in a network is crucial because bots impact numerous areas such as cyber security, finance, health care, law enforcement, and more. Botnets are becoming more sophisticated and dangerous day-by-day, and most of the existing rule based and flow based detection methods may not be capable of detecting bot activities in an efficient and effective manner. Hence...",
        "keywords": [
            "Cyber security",
            "Bot detection",
            "Graph-based features",
            "Clustering"
        ],
        "number_of_pages": "23",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-017-0074-7.pdf",
        "abstract": "Detecting botnets in a network is crucial because bots impact numerous areas such as cyber security, finance, health care, law enforcement, and more. Botnets are becoming more sophisticated and dangerous day-by-day, and most of the existing rule based and flow based detection methods may not be capable of detecting bot activities in an efficient and effective manner. Hence, designing a robust and fast botnet detec-tion method is of high significance. In this study, we propose a novel botnet detection methodology based on topological features of nodes within a graph: in degree, out degree, in degree weight, out degree weight, clustering coefficient, node between-ness, and eigenvector centrality. A self-organizing map clustering method is applied to establish clusters of nodes in the network based on these features. Our method is capable of isolating bots in clusters of small sizes while containing the majority of normal nodes in the same big cluster. Thus, bots can be detected by searching a lim-ited number of nodes. A filtering procedure is also developed to further enhance the algorithm efficiency by removing inactive nodes from consideration. The methodology is verified using the CTU-13 datasets, and benchmarked against a classification-based detection method. The results show that our proposed method can efficiently detect the bots despite their varying behaviors."
    },
    {
        "url": "https://paperity.org/p/79687070/big-data-management-in-smart-grid-concepts-requirements-and-implementation",
        "title": "Big Data management in smart grid: concepts, requirements and implementation",
        "authors": [
            "Houda Daki",
            " Asmaa El Hannani",
            " Abdelhak Aqq"
        ],
        "date_article": "04-2017",
        "short_description": "A smart grid is an intelligent electricity grid that optimizes the generation, distribution and consumption of electricity through the introduction of Information and Communication Technologies on the electricity grid. In essence, smart grids bring profound changes in the information systems that drive them: new information flows coming from the electricity grid, new players such...",
        "keywords": [
            "Smart grid",
            "SCADA",
            "AMI",
            "Demand response",
            "Communication systems",
            "Big Data",
            "Real time processing",
            "Batch processing",
            "Hybrid processing",
            "Customer analytics"
        ],
        "number_of_pages": "19",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-017-0070-y.pdf",
        "abstract": "A smart grid is an intelligent electricity grid that optimizes the generation, distribution and consumption of electricity through the introduction of Information and Commu-nication Technologies on the electricity grid. In essence, smart grids bring profound changes in the information systems that drive them: new information flows coming from the electricity grid, new players such as decentralized producers of renewable energies, new uses such as electric vehicles and connected houses and new com-municating equipments such as smart meters, sensors and remote control points. All this will cause a deluge of data that the energy companies will have to face. Big Data technologies offers suitable solutions for utilities, but the decision about which Big Data technology to use is critical. In this paper, we provide an overview of data man-agement for smart grids, summarise the added value of Big Data technologies for this kind of data, and discuss the technical requirements, the tools and the main steps to implement Big Data solutions in the smart grid context."
    },
    {
        "url": "https://paperity.org/p/79497346/outlier-edge-detection-using-random-graph-generation-models-and-applications",
        "title": "Outlier edge detection using random graph generation models and applications",
        "authors": [
            "Honglei Zhang",
            " Serkan Kiranyaz",
            " Moncef Gabbouj"
        ],
        "date_article": "04-2017",
        "short_description": "Outliers are samples that are generated by different mechanisms from other normal data samples. Graphs, in particular social network graphs, may contain nodes and edges that are made by scammers, malicious programs or mistakenly by normal users. Detecting outlier nodes and edges is important for data mining and graph analytics. However, previous research in the field has merely...",
        "keywords": [
            "Outlier detection",
            "Graph mining",
            "Outlier edge"
        ],
        "number_of_pages": "25",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-017-0073-8.pdf",
        "abstract": "Outliers are samples that are generated by different mechanisms from other normal data samples. Graphs, in particular social network graphs, may contain nodes and edges that are made by scammers, malicious programs or mistakenly by normal users. Detecting outlier nodes and edges is important for data mining and graph analytics. However, previous research in the field has merely focused on detecting outlier nodes. In this article, we study the properties of edges and propose effective outlier edge detection algorithm. The proposed algorithms are inspired by community structures that are very common in social networks. We found that the graph structure around an edge holds critical information for determining the authenticity of the edge. We evalu-ated the proposed algorithms by injecting outlier edges into some real-world graph data. Experiment results show that the proposed algorithms can effectively detect out-lier edges. In particular, the algorithm based on the Preferential Attachment Random Graph Generation model consistently gives good performance regardless of the test graph data. More important, by analyzing the authenticity of the edges in a graph, we are able to reveal underlying structure and properties of a graph. Thus, the proposed algorithms are not limited in the area of outlier edge detection. We demonstrate three different applications that benefit from the proposed algorithms: (1)  a preprocessing tool that improves the performance of graph clustering algorithms; (2) an outlier node detection algorithm; and (3) a novel noisy data clustering algorithm. These applica-tions show the great potential of the proposed outlier edge detection techniques. They also address the importance of analyzing the edges in graph mining—a topic that has been mostly neglected by researchers."
    },
    {
        "url": "https://paperity.org/p/79468593/defining-the-execution-semantics-of-stream-processing-engines",
        "title": "Defining the execution semantics of stream processing engines",
        "authors": [
            "Lorenzo Affetti",
            " Riccardo Tommasini",
            " Alessandro Margar"
        ],
        "date_article": "04-2017",
        "short_description": "The ability to process large volumes of data on the fly, as soon as they become available, is a fundamental requirement in today’s information systems. Modern distributed stream processing engines (SPEs) address this requirement and provide low-latency and high-throughput data stream processing in cluster platforms, offering high-level programming interfaces that abstract from...",
        "keywords": [
            "Stream processing",
            "Stream processing engines",
            "Modeling",
            "Execution semantics",
            "SECRET",
            "Time semantics",
            "Windows"
        ],
        "number_of_pages": "24",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-017-0072-9.pdf",
        "abstract": "The ability to process large volumes of data on the fly, as soon as they become avail-able, is a fundamental requirement in today’s information systems. Modern distributed stream processing engines (SPEs) address this requirement and provide low-latency and high-throughput data stream processing in cluster platforms, offering high-level programming interfaces that abstract from low-level details such as data distribution and hardware failures. The last decade saw a rapid increase in the number of available SPEs. However, each SPE defines its own processing model and standardized execu-tion semantics have not emerged yet. This paper tackles this problem and analyzes the execution semantics of some widely adopted modern SPEs, namely Flink, Storm, Spark Streaming, Google Dataflow, and Azure Stream Analytics. We specifically target the notions of windowing and time, traditionally considered the key distinguishing factors that characterize the behavior of SPEs. We rely on the SECRET model, intro-duced in 2010 to analyze the windowing semantics for the SPEs available at that time. We show that SECRET models well some aspects of the behavior of modern SPEs, and we shed light on the evolution of SPEs after the introduction of SECRET by analyzing the elements that SECRET cannot fully capture. In this way, the paper contributes to the research in the area of stream processing by: (1) contrasting and comparing some widely used modern SPEs based on a formal model of their execution semantics; (2) discussing the evolution of SPEs since the introduction of the SECRET model; (3) sug-gesting promising research directions to direct further modeling efforts."
    },
    {
        "url": "https://paperity.org/p/79657270/big-data-driven-co-occurring-evidence-discovery-in-chronic-obstructive-pulmonary-disease",
        "title": "Big data driven co-occurring evidence discovery in chronic obstructive pulmonary disease patients",
        "authors": [
            "Christopher Baechle",
            " Ankur Agarwal",
            " Xingquan Zhu"
        ],
        "date_article": "04-2017",
        "short_description": "Background Chronic Obstructive Pulmonary Disease (COPD) is a chronic lung disease that affects airflow to the lungs. Discovering the co-occurrence of COPD with other diseases, symptoms, and medications is invaluable to medical staff. Building co-occurrence indexes and finding causal relationships with COPD can be difficult because often times disease prevalence within a...",
        "keywords": [
            "Big data",
            "Decision support system",
            "Data mining",
            "Health informatics"
        ],
        "number_of_pages": "18",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-017-0067-6.pdf",
        "abstract": "Background:  Chronic Obstructive Pulmonary Disease (COPD) is a chronic lung disease that affects airflow to the lungs. Discovering the co-occurrence of COPD with other diseases, symptoms, and medications is invaluable to medical staff. Building co-occur-rence indexes and finding causal relationships with COPD can be difficult because often times disease prevalence within a population influences results. A method which can better separate occurrence within COPD patients from population prevalence would be desirable. Large hospital systems may potentially have tens of millions of patient records spanning decades of collection and a big data approach that is scalable is desirable. The presented method, Co-Occurring Evidence Discovery (COED), presents a methodology and framework to address these issues.Methods:  Natural Language Processing methods are used to examine 64,371 deidenti-fied clinical notes and discover associations between COPD and medical terms. Apache cTAKES is leveraged to annotate and structure clinical notes. Several extensions to cTAKES have been written to parallelize the annotation of large sets of clinical notes.   A co-occurrence score is presented which can penalize scores based on term prevalence, as well as a baseline method traditionally used for finding co-occurrence.  These scoring systems are implemented using Apache Spark. Dictionaries of ground truth terms for diseases, medications, and symptoms have been created using clinical domain knowl-edge. COED and baseline methods are compared using precision, recall, and F1 score.Results:  The highest scoring diseases using COED are lung and respiratory diseases. In contrast, baseline methods for co-occurrence rank diseases with high population prevalence highest. Medications and symptoms evaluated with COED share similar results.  When evaluated against ground truth dictionaries, the maximum improve-ments in recall for symptoms, diseases, and medications were 0.212, 0.130, and 0.174. The maximum improvements in precision for symptoms, diseases, and medications were 0.303, 0.333, and 0.180. Median increase in F1 score for symptoms, diseases, and medications were 38.1%, 23.0%, and 17.1%. A paired t-test was performed and F1 score increases were found to be statistically significant, where p < 0.01.Conclusion:  Penalizing terms which are highly frequent in the corpus results in better precision and recall performance. Penalizing frequently occurring terms gives a better picture of the diseases, symptoms, and medications co-occurring with COPD. Using a mathematical and computational approach rather than purely expert driven approach, large dictionaries of COPD related terms can be assembled in a short amount of time."
    },
    {
        "url": "https://paperity.org/p/79628517/toward-building-rdb-to-hbase-conversion-rules",
        "title": "Toward building RDB to HBase conversion rules",
        "authors": [
            "R. Ouanouki",
            " A. April",
            " A. Abran"
        ],
        "date_article": "04-2017",
        "short_description": "Cloud data stores that can handle very large amounts of data, such as Apache HBase, have accelerated the use of non-relational databases (coined as NoSQL databases) as a way of addressing RDB database limitations with regards to scalability and performance of existing systems. Converting existing systems and their databases to this technology is not trivial, given that RDB...",
        "keywords": [
            "Cloud computing",
            "RDB to NoSQL conversion",
            "Conversion rules",
            "Conversion designs",
            "Distributed databases",
            "Database conversion"
        ],
        "number_of_pages": "21",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-017-0071-x.pdf",
        "abstract": "Cloud data stores that can handle very large amounts of data, such as Apache HBase, have accelerated the use of non-relational databases (coined as NoSQL databases) as a way of addressing RDB database limitations with regards to scalability and perfor-mance of existing systems. Converting existing systems and their databases to this technology is not trivial, given that RDB practitioners tend to use the relational model design mindset when converting existing databases. This can result in inefficient NoSQL design, leading to suboptimal query speed or inefficient database schema. This paper reports on a two-phase experiment: (1) a conversion from RDB to HBase database, a NoSQL type of database, without the use of conversion rules and based on a heuristic approach and (2) an experiment with different schema designs to uncover and extract conversion rules that could be useful conversion guidelines for the industry."
    },
    {
        "url": "https://paperity.org/p/79560168/the-design-of-an-adaptive-column-store-system",
        "title": "The design of an adaptive column-store system",
        "authors": [
            "George Chernishev"
        ],
        "date_article": "04-2017",
        "short_description": "A fully self-managed DBMS which does not require administrator intervention is the ultimate goal of database developers. This system should automate deploying, configuration, administration, monitoring, and tuning tasks. Although there are some advances in this field, self-managed technology is largely not ready for industrial use and remains an active area of research. One of...",
        "keywords": [
            "Column-stores",
            "Physical design",
            "Self-management",
            "On-line tuning"
        ],
        "number_of_pages": "21",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-017-0069-4.pdf",
        "abstract": "A fully self-managed DBMS which does not require administrator intervention is the ultimate goal of database developers. This system should automate deploying, configuration, administration, monitoring, and tuning tasks. Although there are some advances in this field, self-managed technology is largely not ready for industrial use and remains an active area of research. One of the most crucial tasks for such a system is automated physical design tuning. A self-managed approach for this task implies that the physical design of a database should be automatically adapted to changing workloads. The problems of materialized view and index selection, data allocation, horizontal and vertical partitioning were studied for a long time, and hundreds of approaches were developed. However, most of these approaches were static, thus, unsuitable for self-managed systems. In this paper we discuss the prospects of an adaptive distributed relational column-store. We show that the column-store approach holds a great promise for construction of an efficient self-managed database. At first, we present a short survey of existing physical design studies and provide a classifica-tion of approaches. In the survey, we highlight the self-managed aspects. Then, we provide some views on the organization of a self-managed distributed column-store system. We discuss its three core components: an alerter, a reorganization controller and a set of physical design options (actions) available to such a system. We present possible approaches for each of these components and evaluate them. Several physi-cal design problems are formulated and discussed. This study is the first step towards a creation of an adaptive distributed column-store system."
    },
    {
        "url": "https://paperity.org/p/79531415/quality-management-architecture-for-social-media-data",
        "title": "Quality management architecture for social media data",
        "authors": [
            "Pekka Pääkkönen",
            " Juha Jokitulppo"
        ],
        "date_article": "04-2017",
        "short_description": "Social media data has provided various insights into the behaviour of consumers and businesses. However, extracted data may be erroneous, or could have originated from a malicious source. Thus, quality of social media should be managed. Also, it should be understood how data quality can be managed across a big data pipeline, which may consist of several processing and analysis...",
        "keywords": [
            "Quality attribute",
            "Quality metric",
            "Quality policy",
            "Spark",
            "Cassandra",
            "Word2Vec"
        ],
        "number_of_pages": "26",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-017-0066-7.pdf",
        "abstract": "Social media data has provided various insights into the behaviour of consumers and businesses. However, extracted data may be erroneous, or could have originated from a malicious source. Thus, quality of social media should be managed. Also, it should be understood how data quality can be managed across a big data pipeline, which may consist of several processing and analysis phases. The contribution of this paper is eval-uation of data quality management architecture for social media data. The theoretical concepts based on previous work have been implemented for data quality evaluation of Twitter-based data sets. Particularly, reference architecture for quality management in social media data has been extended and evaluated based on the implementation architecture. Experiments indicate that 150–800 tweets/s can be evaluated with two cloud nodes depending on the configuration."
    },
    {
        "url": "https://paperity.org/p/79473909/modeling-temporal-aspects-of-sensor-data-for-mongodb-nosql-database",
        "title": "Modeling temporal aspects of sensor data for MongoDB NoSQL database",
        "authors": [
            "Nadeem Qaisar Mehmood",
            " Rosario Culmone",
            " Leonardo Mostard"
        ],
        "date_article": "03-2017",
        "short_description": "Proliferation of structural, semi-structural and no-structural data, has challenged the scalability, flexibility and processability of the traditional relational database management systems (RDBMS). The next generation systems demand horizontal scaling by distributing data over autonomously addable nodes to a running system. For schema flexibility, they also want to process and...",
        "keywords": [
            "NoSQL",
            "MongoDB",
            "Big data",
            "Schema modeling",
            "Time-series",
            "Real-time",
            "ANT+ protocol"
        ],
        "number_of_pages": "35",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-017-0068-5.pdf",
        "abstract": "Proliferation of structural, semi-structural and no-structural data, has challenged the scalability, flexibility and processability of the traditional relational database manage-ment systems (RDBMS). The next generation systems demand horizontal scaling by dis-tributing data over autonomously addable nodes to a running system. For schema flex-ibility, they also want to process and store different data formats along the sequence factor in the data. NoSQL approaches are solutions to these, hence big data solutions are vital nowadays. But in monitoring scenarios sensors transmit the data continuously over certain intervals of time and temporal factor is the main property of the data. Therefore the key research aspect is to investigate schema flexibility and temporal data integration aspects together. We need to know that: what data modelling should we adopt for a data driven real-time scenario; that we could store the data effectively and evolve the schema accordingly during data integration in NoSQL environments with-out losing big data advantages. In this paper we explain a middleware based schema model to support the temporal oriented storage of real-time data of ANT+ sensors as hierarchical documents. We explain how to adopt a schema for the data integration by using an algorithm based approach for flexible evolution of the model for a document oriented database, i.e, MongoDB. The proposed model is logical, compact for storage and evolves seamlessly upon new data integration."
    },
    {
        "url": "https://paperity.org/p/79502662/improving-deep-neural-network-design-with-new-text-data-representations",
        "title": "Improving deep neural network design with new text data representations",
        "authors": [
            "Joseph D. Prusa",
            " Taghi M. Khoshgoftaar"
        ],
        "date_article": "03-2017",
        "short_description": "Using traditional machine learning approaches, there is no single feature engineering solution for all text mining and learning tasks. Thus, researchers must determine and implement the best feature engineering approach for each text classification task; however, deep learning allows us to skip this step by extracting and learning high-level features automatically from low-level...",
        "keywords": [
            "Deep learning",
            "Big data",
            "Text mining",
            "Sentiment",
            "Convolutional neural networks",
            "Character embedding",
            "GPU computing"
        ],
        "number_of_pages": "16",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-017-0065-8.pdf",
        "abstract": "Using traditional machine learning approaches, there is no single feature engineer-ing solution for all text mining and learning tasks. Thus, researchers must determine and implement the best feature engineering approach for each text classification task; however, deep learning allows us to skip this step by extracting and learning high-level features automatically from low-level text representations. Convolutional neural networks, a popular type of neural network for deep learning, have been shown to be effective at performing feature extraction and classification for many domains includ-ing text. Recently, it was demonstrated that convolutional neural networks can be used to train classifiers from character-level representations of text. This approach achieved superior performance compared to classifiers trained on word-level text representa-tions, likely due to the use of character-level representations preserving more informa-tion from the data. Training neural networks from character level data requires a large volume of instances; however, the large volume of training data and model complex-ity makes training these networks a slow and computationally expensive task. In this paper, we propose a new method of creating character-level representations of text to reduce the computational costs associated with training a deep convolutional neural network. We demonstrate that our method of character embedding greatly reduces training time and memory use, while significantly improving classification perfor-mance. Additionally, we show that our proposed embedding can be used with padded convolutional layers to enable the use of current convolutional network architec-tures, while still facilitating faster training and higher performance than the previous approach for learning from character-level text."
    },
    {
        "url": "https://paperity.org/p/79390684/scalable-two-phase-co-occurring-sensitive-pattern-hiding-using-mapreduce",
        "title": "Scalable two-phase co-occurring sensitive pattern hiding using MapReduce",
        "authors": [
            "Shivani Sharma",
            " Durga Toshniw"
        ],
        "date_article": "03-2017",
        "short_description": "BackgroundExpansion of Internet and its use for on-line activities such as E-Commerce and social networking are producing large volumes of transactional data. This huge data volume resulted from these activities facilitates the analysis and understanding of global trends and interesting patterns used for several decisive purposes. Analytics involved in these processes expose...",
        "keywords": [
            "Privacy preservation",
            "MapReduce framework",
            "Big data",
            "Information hiding"
        ],
        "number_of_pages": "18",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-017-0064-9.pdf",
        "abstract": "Background:  Expansion of Internet and its use for on-line activities such as E-Com-merce and social networking are producing large volumes of transactional data. This huge data volume resulted from these activities facilitates the analysis and understand-ing of global trends and interesting patterns used for several decisive purposes. Analyt-ics involved in these processes expose sensitive information present in these datasets, which is a serious privacy threat. To overcome this challenge, few sequential heuristics have been used in past where volumes of data were comparatively accommodating to these sequential heuristics; the current situation is not that much in-line and often results in high execution time. This new challenge of scalability paves a way for experi-menting with Big Data approaches (e.g., MapReduce Framework). We have agglomer-ated the MapReduce framework with adopted heuristics to overcome this challenge of scalability along with much-needed privacy preservation and yields efficient analytic results within bounded execution times.Methods:  MapReduce is a parallel programming framework [16] which provides us the opportunity to leverage largely distributed resources to deal with the Big Data analytics. MapReduce allows the resource of a largely distributed system to be utilized in a parallel fashion. The simplicity and high fault-tolerance are the key features which make MapReduce a promising framework. Therefore, we have proposed a two-phase MapReduce version of these adopted heuristics. MapReduce framework divides the whole data into ‘n’ number of data chunks D = {d 1 d ∪ 2 ∪ d 3 ..... ∪ d n } and distrib-utes them over ‘n’ computing nodes to achieve the parallelization. The first phase of MapReduce job runs on each data chunk in order to generate intermediate results, which are further sorted and merged in the second phase to generate final sanitized dataset.Results:  We conducted three set of experiments, each with five different scenarios corresponding to the different cluster sizes i.e., n = 1,2,3,4,5 where ‘n’ is a number of computing nodes. We compared the approaches with respect to real as well as synthetically generated large datasets. For varying data sizes and varying number of computing nodes, it has been observed that sanitization time required by the MapReduce-based algorithm for same size dataset is much less than the sequential traditional approach. Further, the scalability can be improved by using more number of computing nodes. Lastly, another set of experiments explores the change in sanitiza-tion time with varying sizes of the sensitive content present in a dataset. We evaluated the effectiveness of proposed approach in different scenarios, with varying cluster size from 1 to 5 nodes. It has been observed that still the execution time of our approach is much less than traditional schemes. Further, no hiding failure, artifactual patterns Open Access©  The  Author(s)  2017.  This  article  is  distributed  under  the  terms  of  the  Creative  Commons  Attribution  4.0  International  License  (http://creativecommons.org/licenses/by/4.0/),  which  permits  unrestricted  use,  distribution,  and  reproduction  in  any  medium,  provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made.RESEARCHSharma and Toshniwal J Big Data  (2017) 4:4 DOI 10.1186/s40537‑017‑0064‑9*Correspondence:   shivani.vce@gmail.com Department of Computer Science and Engineering, Indian Institute of Technology, Roorkee, Uttrakhand 247667, India\nPage 2 of 18Sharma and Toshniwal J Big Data  (2017) 4:4 have been observed during the experiments as well as in terms of misses cost also the MapReduce version performance is same as of traditional approaches.Conclusion:Traditional approaches for data hiding primarily MaxFIA and SWA were lacking with due inability to tackle large voluminous data. To subjugate the new challenge of scalability, we have implemented these basic heuristics with Big Data approach i.e., MapReduce framework. Quantitative evaluations have shown that the fusion of MapReduce framework with these adopted heuristics fulfills its obligatory responsibility of being scalable and many-fold faster for yielding efficient analytic results."
    },
    {
        "url": "https://paperity.org/p/79044958/conceptualizing-big-social-data",
        "title": "Conceptualizing Big Social Data",
        "authors": [
            "Ekaterina Olshannikova",
            " Thomas Olsson",
            " Jukka Huhtamäki"
        ],
        "date_article": "01-2017",
        "short_description": "The popularity of social media and computer-mediated communication has resulted in high-volume and highly semantic data about digital social interactions. This constantly accumulating data has been termed as Big Social Data or Social Big Data, and various visions about how to utilize that have been presented. However, as relatively new concepts, there are no solid and commonly...",
        "keywords": [
            "Big Social Data",
            "Social Big Data",
            "Digital human",
            "Conceptualization",
            "Social Data",
            "Social media",
            "Computational social science",
            "Social computing",
            "Classification",
            "Big Social Data analysis"
        ],
        "number_of_pages": "19",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-017-0063-x.pdf",
        "abstract": "The popularity of social media and computer-mediated communication has resulted in high-volume and highly semantic data about digital social interactions. This constantly accumulating data has been termed as Big Social Data or Social Big Data, and vari-ous visions about how to utilize that have been presented. However, as relatively new concepts, there are no solid and commonly agreed definitions of them. We argue that the emerging research field around these concepts would benefit from understanding about the very substance of the concept and the different viewpoints to it. With our review of earlier research, we highlight various perspectives to this multi-disciplinary field and point out conceptual gaps, the diversity of perspectives and lack of consensus in what Big Social Data means. Based on detailed analysis of related work and earlier conceptualizations, we propose a synthesized definition of the term, as well as outline the types of data that Big Social Data covers. With this, we aim to foster future research activities around this intriguing, yet untapped type of Big Data."
    },
    {
        "url": "https://paperity.org/p/78998368/vaccination-allocation-in-large-dynamic-networks",
        "title": "Vaccination allocation in large dynamic networks",
        "authors": [
            "Justin Zhan",
            " Timothy Rafalski",
            " Gennady Stashkevich"
        ],
        "date_article": "01-2017",
        "short_description": "Network infections that are already in progress cause challenges to those officers trying to preserve those nodes not yet infected. Static solutions can take advantage of global knowledge of the network to produce quick and approximate answers for those members who should be vaccinated. In dynamic situations however, small changes can severely alter those static solutions making...",
        "keywords": [
            "Dynamic networks",
            "Immunization",
            "Dominance tree"
        ],
        "number_of_pages": "17",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-016-0061-4.pdf",
        "abstract": "Network infections that are already in progress cause challenges to those officers try-ing to preserve those nodes not yet infected. Static solutions can take advantage of global knowledge of the network to produce quick and approximate answers for those members who should be vaccinated. In dynamic situations however, small changes can severely alter those static solutions making them irrelevant. Yet in dynamic situa-tions it can not be known with certainty which small changes will affect the solution and those that will not. Computational resources are wasted recalculating a global solution for the entire network, when a local recalculation may be enough. This paper presents a dynamic node vaccination solution that seeks to take advantage of these local recalculations."
    },
    {
        "url": "https://paperity.org/p/79165761/a-new-method-of-large-scale-short-term-forecasting-of-agricultural-commodity-prices",
        "title": "A new method of large-scale short-term forecasting of agricultural commodity prices: illustrated by the case of agricultural markets in Beijing",
        "authors": [
            "Haoyang Wu",
            " Huaili Wu",
            " Minfeng Zhu"
        ],
        "date_article": "01-2017",
        "short_description": "In order to forecast prices of arbitrary agricultural commodity in different wholesale markets in one city, this paper proposes a mixed model, which combines ARIMA model and PLS regression method based on time and space factors. This mixed model is able to obtain the forecasting results of weekly prices of agricultural commodities in different markets. Meanwhile, this paper sets...",
        "keywords": [
            "Change warning",
            "Mixed model",
            "Neural networks",
            "Price forecasting"
        ],
        "number_of_pages": "22",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-016-0062-3.pdf",
        "abstract": "In order to forecast prices of arbitrary agricultural commodity in different wholesale markets in one city, this paper proposes a mixed model, which combines ARIMA model and PLS regression method based on time and space factors. This mixed model is able to obtain the forecasting results of weekly prices of agricultural commodities in differ-ent markets. Meanwhile, this paper sets up variables to measure the price changing trend based on the change of exogenous variables and prices, thus achieves the warn-ing of daily price changes using neural networks. The model is tested with the data of several types of agricultural commodities and error analysis is made. The result shows that the mixed model is more accurate in forecasting agricultural commodity prices than each single model does, and has better accuracy in warning values. The mixed model, to some extent, forecasts the daily price changes of agricultural commodities."
    },
    {
        "url": "https://paperity.org/p/78476476/big-data-privacy-a-technological-perspective-and-review",
        "title": "Big data privacy: a technological perspective and review",
        "authors": [
            "Priyank Jain",
            " Manasi Gyanchandani",
            " Nilay Khar"
        ],
        "date_article": "12-2016",
        "short_description": "Big data is a term used for very large data sets that have more varied and complex structure. These characteristics usually correlate with additional difficulties in storing, analyzing and applying further procedures or extracting results. Big data analytics is the term used to describe the process of researching massive amounts of complex data in order to reveal hidden patterns...",
        "keywords": [
            "Big data",
            "Privacy and security",
            "Privacy preserving: k-anonymity: T-closeness",
            "L-diversity",
            "HybrEx",
            "PPDP",
            "FADS"
        ],
        "number_of_pages": "25",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-016-0059-y.pdf",
        "abstract": "Big data is a term used for very large data sets that have more varied and complex structure. These characteristics usually correlate with additional difficulties in storing, analyzing and applying further procedures or extracting results. Big data analytics is the term used to describe the process of researching massive amounts of complex data in order to reveal hidden patterns or identify secret correlations. However, there is an obvious contradiction between the security and privacy of big data and the wide-spread use of big data. This paper focuses on privacy and security concerns in big data, differentiates between privacy and security and privacy requirements in big data. This paper covers uses of privacy by taking existing methods such as HybrEx, k-anonymity, T-closeness and L-diversity and its implementation in business. There have been a number of privacy-preserving mechanisms developed for privacy protection at differ-ent stages (for example, data generation, data storage, and data processing) of a big data life cycle. The goal of this paper is to provide a major review of the privacy pres-ervation mechanisms in big data and present the challenges for existing mechanisms. This paper also presents recent techniques of privacy preserving in big data like hiding a needle in a haystack, identity based anonymization, differential privacy, privacy-preserving big data publishing and fast anonymization of big data streams. This paper refer privacy and security aspects healthcare in big data. Comparative study between various recent techniques of big data privacy is also done as well."
    },
    {
        "url": "https://paperity.org/p/78447723/limited-random-walk-algorithm-for-big-graph-data-clustering",
        "title": "Limited random walk algorithm for big graph data clustering",
        "authors": [
            "Honglei Zhang",
            " Jenni Raitoharju",
            " Serkan Kiranyaz"
        ],
        "date_article": "12-2016",
        "short_description": "Graph clustering is an important technique to understand the relationships between the vertices in a big graph. In this paper, we propose a novel random-walk-based graph clustering method. The proposed method restricts the reach of the walking agent using an inflation function and a normalization function. We analyze the behavior of the limited random walk procedure and propose a...",
        "keywords": [
            "Graph clustering",
            "Random walk",
            "Big data",
            "Community finding"
        ],
        "number_of_pages": "22",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-016-0060-5.pdf",
        "abstract": "Graph clustering is an important technique to understand the relationships between the vertices in a big graph. In this paper, we propose a novel random-walk-based graph clustering method. The proposed method restricts the reach of the walking agent using an inflation function and a normalization function. We analyze the behavior of the limited random walk procedure and propose a novel algorithm for both global and local graph clustering problems. Previous random-walk-based algorithms depend on the chosen fitness function to find the clusters around a seed vertex. The proposed algorithm tackles the problem in an entirely different manner. We use the limited random walk procedure to find attractor vertices in a graph and use them as features to cluster the vertices. According to the experimental results on the simulated graph data and the real-world big graph data, the proposed method is superior to the state-of-the-art methods in solving graph clustering problems. Since the proposed method uses the embarrassingly parallel paradigm, it can be efficiently implemented and embedded in any parallel computing environment such as a MapReduce framework. Given enough computing resources, we are capable of clustering graphs with millions of vertices and hundreds millions of edges in a reasonable time."
    },
    {
        "url": "https://paperity.org/p/78124522/treenet-analysis-of-human-stress-behavior-using-socio-mobile-data",
        "title": "TreeNet analysis of human stress behavior using socio-mobile data",
        "authors": [
            "B. Padmaja",
            " V. V. Rama Prasad",
            " K. V. N. Sunith"
        ],
        "date_article": "11-2016",
        "short_description": "Human behavior is essentially social and humans start their daily routines by interacting with others. There are many forms of social interactions and we have used mobile phone based social interaction features and social surveys for finding human stress behavior. For this, we gathered mobile phone call logs data set containing 111,444 voice calls of 131 adult members of a living...",
        "keywords": [
            "Reality",
            "Mining",
            "Social network analysis",
            "Sensor data",
            "Human stress"
        ],
        "number_of_pages": "15",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-016-0054-3.pdf",
        "abstract": "Human behavior is essentially social and humans start their daily routines by interact-ing with others. There are many forms of social interactions and we have used mobile phone based social interaction features and social surveys for finding human stress behavior. For this, we gathered mobile phone call logs data set containing 111,444 voice calls of 131 adult members of a living community for a period of more than 5 months. And we identified that top 5 social network measures like hierarchy, den-sity, farness, reachability and eigenvector of individuals have profound influence on individuals’ stress levels in a social network. If an ego lies in the shortest path of all other alters then the ego receives more information and hence is more stressed. In this paper, we have used TreeNet machine learning algorithm for its speed and immunity to outliers. We have tested our results with another Random Forest classifier as well and yet, we found TreeNet to be more efficient. This research can be of vital importance to economists, professionals, analysts, and policy makers."
    },
    {
        "url": "https://paperity.org/p/78322107/understanding-big-data-themes-from-scientific-biomedical-literature-through-topic",
        "title": "Understanding big data themes from scientific biomedical literature through topic modeling",
        "authors": [
            "Allard J. van Altena",
            " Perry D. Moerland",
            " Aeilko H. Zwinderman"
        ],
        "date_article": "11-2016",
        "short_description": "Nowadays, big data is a key component in (bio)medical research. However, the meaning of the term is subject to a wide array of opinions, without a formal definition. This hampers communication and leads to missed opportunities. For example, in the (bio)medical field we have observed many different interpretations, some of which have a negative connotation, impeding exploitation...",
        "keywords": [
            "Text mining",
            "Topic modelling",
            "Big data",
            "Biomedical research"
        ],
        "number_of_pages": "21",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-016-0057-0.pdf",
        "abstract": "Nowadays, big data is a key component in (bio)medical research. However, the mean-ing of the term is subject to a wide array of opinions, without a formal definition. This hampers communication and leads to missed opportunities. For example, in the (bio)medical field we have observed many different interpretations, some of which have a negative connotation, impeding exploitation of big data approaches. In this paper we pursue a better understanding of the term big data through a data-driven systematic approach using text analysis of scientific (bio)medical literature. We attempt to find how existing big data definitions are expressed within the chosen application domain. We build upon findings of previous qualitative research by De Mauro et al. (Lib Rev 65: 122–135, 14), which analysed fifteen definitions and identified four key big data themes (i.e., information, methods, technology, and impact). We have revisited these and other definitions of big data, and consolidated them into eight additional themes, resulting in a total of twelve themes. The corpus was composed of paper abstracts extracted from (bio)medical literature databases, searching for ‘big data’. After text pre-processing and parameter selection, topic modelling was applied with 25 topics. The resulting top-20 words per topic were annotated with the twelve big data themes by seven observers. The analysis of these annotations show that the themes proposed by De Mauro et al. are strongly expressed in the corpus. Furthermore, several of the most popular big data V’s (i.e., volume, velocity, and value) also have a relatively high pres-ence. Other V’s introduced more recently (e.g. variability) were however hardly found in the 25 topics. These findings show that the current understanding of big data within the (bio)medical domain is in agreement with more general definitions of the term."
    },
    {
        "url": "https://paperity.org/p/78272729/an-optimized-approach-for-community-detection-and-ranking",
        "title": "An optimized approach for community detection and ranking",
        "authors": [
            "Matin Pirouz",
            " Justin Zhan",
            " Shahab Tayeb"
        ],
        "date_article": "11-2016",
        "short_description": "Community structures and relation patterns, and ranking them for social networks provide us with great knowledge about network. Such knowledge can be utilized for target marketing or grouping similar, yet distinct, nodes. The ever-growing variety of social networks necessitates detection of minute and scattered communities, which are important problems across different research...",
        "keywords": [
            "Betweenness",
            "Breadth-first search",
            "Community strength",
            "Complex networks",
            "Jaccard index",
            "Modularity"
        ],
        "number_of_pages": "12",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-016-0058-z.pdf",
        "abstract": "Community structures and relation patterns, and ranking them for social networks provide us with great knowledge about network. Such knowledge can be utilized for target marketing or grouping similar, yet distinct, nodes. The ever-growing variety of social networks necessitates detection of minute and scattered communities, which are important problems across different research fields including biology, social studies, physics, etc. Existing community detection algorithms such as fast and folding or mod-ularity based are either incapable of finding graph anomalies or too slow and impracti-cal for large graphs. The main contributions of this work are twofold: (i) we optimize the Attractor algorithm, speeding it up by a factor depending on complexity of the graph; i.e. the more complex a social graph is, the better result the algorithm will achieve, and (ii) we propose a community ranker algorithm for the first time. The former is achieved by amalgamating loops and incorporating breadth-first search (BFS) algorithm for edge alignments and to fill in the missing cache, preserving a constant of time equal to the number of edges in the graph. For the latter, we make the first attempt to enumer-ate how influential each community is in a given graph, ranking them based on their normalized impact factor."
    },
    {
        "url": "https://paperity.org/p/78294769/an-efficient-strategy-for-the-collection-and-storage-of-large-volumes-of-data-for",
        "title": "An efficient strategy for the collection and storage of large volumes of data for computation",
        "authors": [
            "Uthayanath Suthakar",
            " Luca Magnoni",
            " David Ryan Smith"
        ],
        "date_article": "10-2016",
        "short_description": "In recent years, there has been an increasing amount of data being produced and stored, which is known as Big Data. The social networks, internet of things, scientific experiments and commercial services play a significant role in generating a vast amount of data. Three main factors are important in Big Data; Volume, Velocity and Variety. One needs to consider all three factors...",
        "keywords": [
            "Big Data",
            "Data pipeline",
            "Hadoop",
            "Apache Flume",
            "MapReduce",
            "HDFS"
        ],
        "number_of_pages": "17",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-016-0056-1.pdf",
        "abstract": "In recent years, there has been an increasing amount of data being produced and stored, which is known as Big Data. The social networks, internet of things, scientific experiments and commercial services play a significant role in generating a vast amount of data. Three main factors are important in Big Data; Volume, Velocity and Variety. One needs to consider all three factors when designing a platform to sup-port Big Data. The Large Hadron Collider (LHC) particle accelerator at CERN consists of a number of data-intensive experiments, which are estimated to produce a volume of about 30 PB of data, annually. The velocity of these data that are propagated will be extremely fast. Traditional methods of collecting, storing and analysing data have become insufficient in managing the rapidly growing volume of data. Therefore, it is essential to have an efficient strategy to capture these data as they are produced. In this paper, a number of models are explored to understand what should be the best approach for collecting and storing Big Data for analytics. An evaluation of the perfor-mance of full execution cycles of these approaches on the monitoring of the World-wide LHC Computing Grid (WLCG) infrastructure for collecting, storing and analysing data is presented. Moreover, the models discussed are applied to a community driven software solution, Apache Flume, to show how they can be integrated, seamlessly."
    },
    {
        "url": "https://paperity.org/p/78174469/data-aware-optimization-of-bioinformatics-workflows-in-hybrid-clouds",
        "title": "Data-aware optimization of bioinformatics workflows in hybrid clouds",
        "authors": [
            "Athanassios M. Kintsakis",
            " Fotis E. Psomopoulos",
            " Pericles A. Mitkas"
        ],
        "date_article": "10-2016",
        "short_description": "Life Sciences have been established and widely accepted as a foremost Big Data discipline; as such they are a constant source of the most computationally challenging problems. In order to provide efficient solutions, the community is turning towards scalable approaches such as the utilization of cloud resources in addition to any existing local computational infrastructures...",
        "keywords": [
            "Cloud computing",
            "Component-based workflows",
            "Bioinformatics",
            "Big data management",
            "Hybrid cloud",
            "Comparative genomics"
        ],
        "number_of_pages": "26",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-016-0055-2.pdf",
        "abstract": "Life Sciences have been established and widely accepted as a foremost Big Data discipline; as such they are a constant source of the most computationally challenging problems. In order to provide efficient solutions, the community is turning towards scalable approaches such as the utilization of cloud resources in addition to any existing local computational infrastructures. Although bioinformatics workflows are generally amenable to parallelization, the challenges involved are however not only computationally, but also data intensive. In this paper we propose a data management methodology for achieving parallelism in bioinformatics workflows, while simultane-ously minimizing data-interdependent file transfers. We combine our methodology with a novel two-stage scheduling approach capable of performing load estimation and balancing across and within heterogeneous distributed computational resources. Beyond an exhaustive experimentation regime to validate the scalability and speed-up of our approach, we compare it against a state-of-the-art high performance comput-ing framework and showcase its time and cost advantages."
    },
    {
        "url": "https://paperity.org/p/78380449/analyzing-performance-of-apache-tez-and-mapreduce-with-hadoop-multinode-cluster-on-amazon",
        "title": "Analyzing performance of Apache Tez and MapReduce with hadoop multinode cluster on Amazon cloud",
        "authors": [
            "Rupinder Singh",
            " Puneet Jai Kaur"
        ],
        "date_article": "10-2016",
        "short_description": "Big Data is the term used for larger data sets that are very complex and not easily processed by the traditional devices. Today is the need of the new technology for processing these large data sets. Apache Hadoop is the good option and it has many components that worked together to make the hadoop ecosystem robust and efficient. Apache Pig is the core component of hadoop...",
        "keywords": [
            "Big Data; Hadoop; HDFS; MapReduce; Apache Tez; Apache Pig; Apache Hive"
        ],
        "number_of_pages": "10",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-016-0051-6.pdf",
        "abstract": "Big Data is the term used for larger data sets that are very complex and not easily pro-cessed by the traditional devices. Today is the need of the new technology for process-ing these large data sets. Apache Hadoop is the good option and it has many com-ponents that worked together to make the hadoop ecosystem robust and efficient. Apache Pig is the core component of hadoop ecosystem and it accepts the tasks in the form of scripts. To run these scripts Apache Pig may use MapReduce or Apache Tez framework. In our previous paper we analyze how these two frameworks different from each other on the basis of some parameters chosen. We compare both the frameworks in theoretical and empirical way on the single node cluster. Here, in this paper we try to perform the analysis on multinode cluster which is installed at Amazon cloud."
    },
    {
        "url": "https://paperity.org/p/77941487/big-data-analysis-for-financial-risk-management",
        "title": "Big data analysis for financial risk management",
        "authors": [
            "Paola Cerchiello",
            " Paolo Giudici"
        ],
        "date_article": "10-2016",
        "short_description": "A very important area of financial risk management is systemic risk modelling, which concerns the estimation of the interrelationships between financial institutions, with the aim of establishing which of them are more central and, therefore, more contagious/subject to contagion. The aim of this paper is to develop a novel systemic risk model. A model that, differently from...",
        "keywords": [
            "Twitter data analysis",
            "Graphical Gaussian models",
            "Graphical model selection",
            "Banking and finance applications",
            "Risk management"
        ],
        "number_of_pages": "12",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-016-0053-4.pdf",
        "abstract": "A very important area of financial risk management is systemic risk modelling, which concerns the estimation of the interrelationships between financial institutions, with the aim of establishing which of them are more central and, therefore, more conta-gious/subject to contagion. The aim of this paper is to develop a novel systemic risk model. A model that, differently from existing ones, employs not only the information contained in financial market prices, but also big data coming from financial tweets. From a methodological viewpoint, the novelty of our paper is the estimation of systemic risk models using two different data sources: financial markets and financial tweets, and a proposal to combine them, using a Bayesian approach. From an applied viewpoint, we present the first systemic risk model based on big data, and show that such a model can shed further light on the interrelationships between financial institutions."
    },
    {
        "url": "https://paperity.org/p/78003827/d2o-a-distributed-data-object-for-parallel-high-performance-computing-in-python",
        "title": "d2o: a distributed data object for parallel high-performance computing in Python",
        "authors": [
            "Theo Steininger",
            " Maksim Greiner",
            " Frederik Beaujean"
        ],
        "date_article": "09-2016",
        "short_description": "We introduce d2o, a Python module for cluster-distributed multi-dimensional numerical arrays. It acts as a layer of abstraction between the algorithm code and the data-distribution logic. The main goal is to achieve usability without losing numerical performance and scalability. d2o’s global interface is similar to the one of a numpy.ndarray, whereas the cluster node’s local data...",
        "keywords": [
            "Parallelization; Numerics; MPI; Python; Numpy; Open source"
        ],
        "number_of_pages": "34",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-016-0052-5.pdf",
        "abstract": "We introduce d2o, a Python module for cluster-distributed multi-dimensional numeri-cal arrays. It acts as a layer of abstraction between the algorithm code and the data-distribution logic. The main goal is to achieve usability without losing numerical performance and scalability. d2o’s global interface is similar to the one of a numpy.ndarray, whereas the cluster node’s local data is directly accessible for use in customized high-performance modules. d2o is written in pure Python which makes it portable and easy to use and modify. Expensive operations are carried out by dedi-cated external libraries like numpy and mpi4py. The performance of d2o is on a par with numpy for serial applications and scales well when moving to an MPI cluster. d2ois open-source software available under the GNU General Public License v3 (GPL-3) at https://gitlab.mpcdf.mpg.de/ift/D2O."
    },
    {
        "url": "https://paperity.org/p/77732771/identification-of-top-k-influential-communities-in-big-networks",
        "title": "Identification of top-K influential communities in big networks",
        "authors": [
            "Justin Zhan",
            " Vivek Guidibande",
            " Sai Phani Krishna Pars"
        ],
        "date_article": "09-2016",
        "short_description": "Because communities are the fundamental component of big data/large data network graphs, community detection in large-scale graphs is an important area to study. Communities are a collection of a set of nodes with similar features. In a given graph there can be many features for clustering the nodes to form communities. Varying the features of interest can form several...",
        "keywords": [
            "Graphs; Top-K communities; Katz centrality"
        ],
        "number_of_pages": "28",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-016-0050-7.pdf",
        "abstract": "Because communities are the fundamental component of big data/large data net-work graphs, community detection in large-scale graphs is an important area to study. Communities are a collection of a set of nodes with similar features. In a given graph there can be many features for clustering the nodes to form communities. Varying the features of interest can form several communities. Out of all communities that are formed, only a few communities are dominant and most influential for a given network graph. This might well contain influential nodes; i.e., for each possible feature of cluster-ing, there will be only a few influential communities in the graph. Identification of such communities is a salient subject for research. This paper present a technique to identify the top-K communities, based on the average Katz centrality of all the communities in a network of communities and the distinctive nature of the communities. One can use these top-K communities to spread information efficiently into the network, as these communities are capable of influencing neighboring communities and thus spreading the information into the network efficiently."
    },
    {
        "url": "https://paperity.org/p/77820067/multi-method-approach-to-wellness-predictive-modeling",
        "title": "Multi-method approach to wellness predictive modeling",
        "authors": [
            "Ankur Agarwal",
            " Christopher Baechle",
            " Ravi S. Behar"
        ],
        "date_article": "08-2016",
        "short_description": "Patient wellness and preventative care are increasingly becoming a concern for many patients, employers, and healthcare professionals. The federal government has increased spending for wellness alongside new legislation which gives employers and insurance providers some new tools for encouraging preventative care. Not all preventative care and wellness programs have a net...",
        "keywords": [
            "Decision support system; Wellness; Data mining; NHANES 2011–2012; Feature selection"
        ],
        "number_of_pages": "23",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-016-0049-0.pdf",
        "abstract": "Patient wellness and preventative care are increasingly becoming a concern for many patients, employers, and healthcare professionals. The federal government has increased spending for wellness alongside new legislation which gives employers and insurance providers some new tools for encouraging preventative care. Not all preventative care and wellness programs have a net positive savings however. Our research attempts to create a patient wellness score which integrates many lifestyle components and a holistic patient prospective. Using a large comprehensive survey conducted by the Centers for Disease Control and Prevention, models are built com-bining both medical professional input and machine learning algorithms. Models are compared and 8 out of 9 models are shown to have a statistically significant (p=0.05) increase in area under the receiver operating characteristic when using the hybrid approach when compared to expert-only models. Models are then aggregated and lin-early transformed for patient-friendly output. The resulting predictive models provide patients and healthcare providers a comprehensive numerical assessment of a patient’s health, which may be used to track patient wellness so at to help maintain or improve their current condition."
    },
    {
        "url": "https://paperity.org/p/77361711/reaping-the-benefits-of-big-data-in-telecom",
        "title": "Reaping the benefits of big data in telecom",
        "authors": [
            "Jacques Bughin"
        ],
        "date_article": "07-2016",
        "short_description": "We collect big data use cases for a representative sample of telecom companies worldwide and observe a wide and skewed distribution of big data returns, with a few companies reporting large impact for a long tail of telecom companies with limited returns. Using a joint model of adoption and returns to adoption, we find that the skewness of the distribution arises from a few...",
        "keywords": [
            "Information systems; Big data; Organization assets"
        ],
        "number_of_pages": "17",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-016-0048-1.pdf",
        "abstract": "We collect big data use cases for a representative sample of telecom companies worldwide and observe a wide and skewed distribution of big data returns, with a few companies reporting large impact for a long tail of telecom companies with limited returns. Using a joint model of adoption and returns to adoption, we find that the skewness of the distribution arises from a few telecom companies being able to follow key big data managerial and organizational practices. We also find that big data returns exhibit economies of scope, decreasing returns to scale, while big data talents are com-plementary to big data capex investments."
    },
    {
        "url": "https://paperity.org/p/77430525/analysis-of-hourly-road-accident-counts-using-hierarchical-clustering-and-cophenetic",
        "title": "Analysis of hourly road accident counts using hierarchical clustering and cophenetic correlation coefficient (CPCC)",
        "authors": [
            "Sachin Kumar",
            " Durga Toshniw"
        ],
        "date_article": "07-2016",
        "short_description": "Road and traffic accidents are an important concern around the world. Road accidents not only affects the public health with different level of injury but also results in property damage. Data analysis has the capability to identify the different reasons behind road accidents i.e. traffic characteristics, weather characteristics, road characteristics and etc. A variety of...",
        "keywords": [
            "Accident analysis; Clustering; Cophenetic correlation coefficient; Data mining"
        ],
        "number_of_pages": "11",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-016-0046-3.pdf",
        "abstract": "Road and traffic accidents are an important concern around the world. Road acci-dents not only affects the public health with different level of injury but also results in property damage. Data analysis has the capability to identify the different reasons behind road accidents i.e. traffic characteristics, weather characteristics, road charac-teristics and etc. A variety of research on road accident data analysis has already proves its importance. Some studies focused on identifying factors associated with accident severity while others focused on identifying the associated factors behind accident occurrence. These research analyses used traditional statistical methods as well as data mining methods. Data mining is frequently used method for analyzing road accident data in present research. Trend analysis is another important research area in road accident domain. Trend analysis can assist in identifying the increasing or decreasing accidents rate in different reasons. In this study, we have proposed a method to analyze hourly road accident data using Cophenetic correlation coefficient from Gujarat state in India. The motive of this study is to provide an efficient way to choose the best suit-able distance metric to cluster the series of counts data that provide a better clustering result. The result shows that the proposed method is capable of efficiently group the different districts with similar road accident patterns into single cluster or group which can be further used for trend analysis or similar tasks."
    },
    {
        "url": "https://paperity.org/p/77332001/optimized-relativity-search-node-reduction-in-personalized-page-rank-estimation-for-large",
        "title": "Optimized relativity search: node reduction in personalized page rank estimation for large graphs",
        "authors": [
            "Matin Pirouz",
            " Justin Zhan"
        ],
        "date_article": "07-2016",
        "short_description": "This paper proposes an algorithm called optimized relativity search to reduce the number of nodes in a graph when attempting to decrease the running time for personalized page rank (PPR) estimation. Even though similar estimations have been done, this method significantly increases the speed of computation, making it a feasible candidate for large graph solutions, such as search...",
        "keywords": [
            "Estimation; Graph reduction; Node reduction; PageRank"
        ],
        "number_of_pages": "17",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-016-0047-2.pdf",
        "abstract": "This paper proposes an algorithm called optimized relativity search to reduce the number of nodes in a graph when attempting to decrease the running time for per-sonalized page rank (PPR) estimation. Even though similar estimations have been done, this method significantly increases the speed of computation, making it a feasible candidate for large graph solutions, such as search engines and friend recommenda-tion techniques used in social media. In this study, the weighted page rank method was combined with the Monte-Carlo technique and a local update algorithm over a reduced map space; this algorithm was developed to achieve a more accurate and faster search method than FAST PPR. The experimental results showed that for nodes with a high degree of incoming nodes, the speed of estimation was twice as fast com-pared to FAST PPR, at the expense of a little accuracy."
    },
    {
        "url": "https://paperity.org/p/76376751/spatial-data-extension-for-cassandra-nosql-database",
        "title": "Spatial data extension for Cassandra NoSQL database",
        "authors": [
            "Mohamed Ben Brahim",
            " Wassim Drira",
            " Fethi Filali"
        ],
        "date_article": "06-2016",
        "short_description": "The big data phenomenon is becoming a fact. Continuous increase of digitization and connecting devices to Internet are making current solutions and services smarter, richer and more personalized. The emergence of the NoSQL databases, like Cassandra, with their massive scalability and high availability encourages us to investigate the management of the stored data within such...",
        "keywords": [
            "Big data; Spatial query; Geohash; Cassandra DB; NoSQL databases"
        ],
        "number_of_pages": "16",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-016-0045-4.pdf",
        "abstract": "The big data phenomenon is becoming a fact. Continuous increase of digitization and connecting devices to Internet are making current solutions and services smarter, richer and more personalized. The emergence of the NoSQL databases, like Cassandra, with their massive scalability and high availability encourages us to investigate the management of the stored data within such storage system. In our present work, we harness the geohashing technique to enable spatial queries as extension to Cassandra query language capabilities while preserving the native syntax. The developed frame-work showed the feasibility of this approach where basic spatial queries are under-pinned and the query response time is reduced by up to 70 times for a fairly large area."
    },
    {
        "url": "https://paperity.org/p/76410539/towards-shortest-path-identification-on-large-networks",
        "title": "Towards shortest path identification on large networks",
        "authors": [
            "Haysam Selim",
            " Justin Zhan"
        ],
        "date_article": "06-2016",
        "short_description": "The use of Big Data in today’s world has become a necessity due to the massive number of technologies developed recently that keeps on providing us with data such as sensors, surveillance system and even smart phones and smart wearable devices they all tend to produce a lot of information that need to be analyzed and studied in details to provide us with some insight to what...",
        "keywords": [
            "Network; Network analysis; Similarity graph reduction; Shortest path analysis; Dijkstra’s; Maximum similarity clique (MSC); Data similarity computation"
        ],
        "number_of_pages": "18",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-016-0042-7.pdf",
        "abstract": "The use of Big Data in today’s world has become a necessity due to the massive num-ber of technologies developed recently that keeps on providing us with data such as sensors, surveillance system and even smart phones and smart wearable devices they all tend to produce a lot of information that need to be analyzed and studied in details to provide us with some insight to what these data represent. In this paper we focus on the application of the techniques of data reduction based on data nodes in large net-works datasets by computing data similarity computation, maximum similarity clique (MSC) and then finding the shortest path in a quick manner due to the data reduction in the graph. As the number of vertices and edges tend to increase on large networks the aim of this article is to make the reduction of the network that will cause an impact on calculating the shortest path for a faster analysis in a shortest time."
    },
    {
        "url": "https://paperity.org/p/76276899/a-survey-of-transfer-learning",
        "title": "A survey of transfer learning",
        "authors": [
            "Karl Weiss",
            " Taghi M. Khoshgoftaar",
            " DingDing Wang"
        ],
        "date_article": "05-2016",
        "short_description": "Machine learning and data mining techniques have been used in numerous real-world applications. An assumption of traditional machine learning methodologies is the training data and testing data are taken from the same domain, such that the input feature space and data distribution characteristics are the same. However, in some real-world machine learning scenarios, this...",
        "keywords": [
            "Transfer learning; Survey; Domain adaptation; Machine learning; Data mining"
        ],
        "number_of_pages": "40",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-016-0043-6.pdf",
        "abstract": "Machine learning and data mining techniques have been used in numerous real-world applications. An assumption of traditional machine learning methodologies is the training data and testing data are taken from the same domain, such that the input feature space and data distribution characteristics are the same. However, in some real-world machine learning scenarios, this assumption does not hold. There are cases where training data is expensive or difficult to collect. Therefore, there is a need to cre-ate high-performance learners trained with more easily obtained data from different domains. This methodology is referred to as transfer learning. This survey paper for-mally defines transfer learning, presents information on current solutions, and reviews applications applied to transfer learning. Lastly, there is information listed on software downloads for various transfer learning solutions and a discussion of possible future research work. The transfer learning solutions surveyed are independent of data size and can be applied to big data environments."
    },
    {
        "url": "https://paperity.org/p/76171617/a-novel-framework-to-analyze-road-accident-time-series-data",
        "title": "A novel framework to analyze road accident time series data",
        "authors": [
            "Sachin Kumar",
            " Durga Toshniw"
        ],
        "date_article": "05-2016",
        "short_description": "Road accident data analysis plays an important role in identifying key factors associated with road accidents. These associated factors help in taking preventive measures to overcome the road accidents. Various studies have been done on road accident data analysis using traditional statistical techniques and data mining techniques. All these studies focused on identifying key...",
        "keywords": [
            "Road accidents; Time series; Data mining; Clustering; Trend analysis"
        ],
        "number_of_pages": "11",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-016-0044-5.pdf",
        "abstract": "Road accident data analysis plays an important role in identifying key factors associ-ated with road accidents. These associated factors help in taking preventive measures to overcome the road accidents. Various studies have been done on road accident data analysis using traditional statistical techniques and data mining techniques. All these studies focused on identifying key factors associated with road accidents in different countries. Road accident is uncertain and unpredictable events which can occur in any circumstances. Also, road accidents do not have similar impacts in every region of the districts. There are chances that road accident rate is increasing in a certain district but it has some lower impact in other districts. Hence, the more focus on road safety should be on those regions or districts where road accident trend is increasing. Time series analysis is an important area of study which can be helpful in identifying the increasing or decreasing trends in different districts. In this paper, we have proposed a framework to analyze road accident time series data that takes 39 time series data of 39 districts of Gujrat and Uttarakhand state of India. This framework segments the time series data into different clusters. A time series merging algorithm is proposed to find the representative time series (RTS) for each cluster. This RTS is further used for trend analysis of different clusters. The results reveals that road accident trend is going to increase in certain clusters and those districts should be the prime concern to take preventive measure to overcome the road accidents."
    },
    {
        "url": "https://paperity.org/p/75907871/topic-discovery-and-future-trend-forecasting-for-texts",
        "title": "Topic discovery and future trend forecasting for texts",
        "authors": [
            "Jose L. Hurtado",
            " Ankur Agarwal",
            " Xingquan Zhu"
        ],
        "date_article": "04-2016",
        "short_description": "Finding topics from a collection of documents, such as research publications, patents, and technical reports, is helpful for summarizing large scale text collections and the world wide web. It can also help forecast topic trends in the future. This can be beneficial for many applications, such as modeling the evolution of the direction of research and forecasting future trends of...",
        "keywords": [
            "Machine learning; Text mining; Topic forecast; Topic discovery; Community discovery; Topic identification; Label identification; Cluster labeling; Category labeling; Association rules"
        ],
        "number_of_pages": "21",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-016-0039-2.pdf",
        "abstract": "Finding topics from a collection of documents, such as research publications, patents, and technical reports, is helpful for summarizing large scale text collections and the world wide web. It can also help forecast topic trends in the future. This can be benefi-cial for many applications, such as modeling the evolution of the direction of research and forecasting future trends of the IT industry. In this paper, we propose using associa-tion analysis and ensemble forecasting to automatically discover topics from a set of text documents and forecast their evolving trend in a near future. In order to discover meaningful topics, we collect publications from a particular research area, data mining and machine learning, as our data domain. An association analysis process is applied to the collected data to first identify a set of topics, followed by a temporal correlation analysis to help discover correlations between topics, and identify a network of topics and communities. After that, an ensemble forecasting approach is proposed to predict the popularity of research topics in the future. Our experiments and validations on data with 9years of publication records validate the effectiveness of the proposed design."
    },
    {
        "url": "https://paperity.org/p/75984355/feasibility-analysis-of-asterixdb-and-spark-streaming-with-cassandra-for-stream-based",
        "title": "Feasibility analysis of AsterixDB and Spark streaming with Cassandra for stream-based processing",
        "authors": [
            "Pekka Pääkkönen"
        ],
        "date_article": "04-2016",
        "short_description": "For getting up-to-date insight into online services, extracted data has to be processed in near real time. For example, major big data companies (Facebook, LinkedIn, Twitter) analyse streaming data for development of new services. Several technologies have been developed, which could be selected for implementation of stream processing functionalities. The contribution of this...",
        "keywords": [
            "Sentiment; Tweet; Word count; AsterixDB; Spark; Performance; Eucalyptus; Cassandra"
        ],
        "number_of_pages": "25",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-016-0041-8.pdf",
        "abstract": "For getting up-to-date insight into online services, extracted data has to be processed in near real time. For example, major big data companies (Facebook, LinkedIn, Twitter) analyse streaming data for development of new services. Several technologies have been developed, which could be selected for implementation of stream processing functionalities. The contribution of this paper is feasibility analysis of technologies for stream-based processing of semi-structured data. Particularly, feasibility of a Big Data management system for semi-structured data (AsterixDB) will be compared to Spark streaming, which has been integrated with Cassandra NoSQL database for persistence. The study focuses on stream processing in a simulated social media use case (tweet analysis), which has been implemented to Eucalyptus cloud computing environment on a distributed shared memory multiprocessor platform. The results indicate that AsterixDB is able to provide significantly better performance both in terms of through-put and latency, when data feed functionality of AsterixDB is used, and stream process-ing has been implemented with Java. AsterixDB also scaled on the same level or better, when the amount of nodes on the cloud platform was increased. However, stream processing in AsterixDB was delayed by batching of data, when tweets were streamed into the database with data feeds."
    },
    {
        "url": "https://paperity.org/p/75544450/role-of-big-data-in-classification-and-novel-class-detection-in-data-streams",
        "title": "Role of big-data in classification and novel class detection in data streams",
        "authors": [
            "M. B. Chandak"
        ],
        "date_article": "03-2016",
        "short_description": "“Data streams” is defined as class of data generated over “text, audio and video” channel in continuous form. The streams are of infinite length and may comprise of structured or unstructured data. With these features, it is difficult to store and process data streams with simple and static strategies. The processing of data stream poses four main challenges to researchers. These...",
        "keywords": [
            "Data stream; Data mining; Concept-drift; Concept-evolution; Novel; Features"
        ],
        "number_of_pages": "9",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-016-0040-9.pdf",
        "abstract": "“Data streams” is defined as class of data generated over “text, audio and video” channel in continuous form. The streams are of infinite length and may comprise of structured or unstructured data. With these features, it is difficult to store and process data streams with simple and static strategies. The processing of data stream poses four main chal-lenges to researchers. These are infinite length, concept-evolution, concept-drift and feature evolution. Infinite-length is because the amount of data has no bounds. Con-cept-drift is due to slow changes in the concept of stream. Concept-evolution occurs due to presence of unknown classes in data. Feature-evolution is due to progression new features and regression of old features. To perform any analytics data streams, the conversion to knowledgable form is essential. The researcher in past have proposed various strategies, most of the research is focussed on problem of infinite-length and concept-drift. The research work presented in the paper describes a efficient string based methodology to process “data streams” and control the challenges of infinite-length, concept-evolution and concept-drift.Subject areas Data mining, Machine learning"
    },
    {
        "url": "https://paperity.org/p/75299734/an-appgallery-for-dataflow-computing",
        "title": "An AppGallery for dataflow computing",
        "authors": [
            "Nemanja Trifunovic",
            " Veljko Milutinovic",
            " Nenad Korolij"
        ],
        "date_article": "02-2016",
        "short_description": "This paper describes the vision behind and the mission of the Maxeler Application Gallery (AppGallery.Maxeler.com) project. First, it concentrates on the essence and performance advantages of the Maxeler dataflow approach. Second, it reviews the support technologies that enable the dataflow approach to achieve its maximum. Third, selected examples of the Maxeler Application...",
        "keywords": [
            "Dataflow; Supercomputing; Maxeler; Applications"
        ],
        "number_of_pages": "30",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-015-0038-8.pdf",
        "abstract": "This paper describes the vision behind and the mission of the Maxeler Application Gallery (AppGallery.Maxeler.com) project. First, it concentrates on the essence and per-formance advantages of the Maxeler dataflow approach. Second, it reviews the sup-port technologies that enable the dataflow approach to achieve its maximum. Third, selected examples of the Maxeler Application Gallery are presented; these examples are treated as the final achievement made possible when all the support technolo-gies are put to work together (internal infrastructure of the AppGallery.Maxeler.com is given in a follow-up paper). As last, the possible impact of the Application Gallery is presented and the major conclusions are drawn."
    },
    {
        "url": "https://paperity.org/p/75155071/mining-chinese-social-media-ugc-a-big-data-framework-for-analyzing-douban-movie-reviews",
        "title": "Mining Chinese social media UGC: a big-data framework for analyzing Douban movie reviews",
        "authors": [
            "Jie Yang",
            " Brian Yecies"
        ],
        "date_article": "01-2016",
        "short_description": "Analysis of online user-generated content is receiving attention for its wide applications from both academic researchers and industry stakeholders. In this pilot study, we address common Big Data problems of time constraints and memory costs involved with using standard single-machine hardware and software. A novel Big Data processing framework is proposed to investigate a niche...",
        "keywords": [
            "Social media; User-generated content; Big data analytics; Content mining; Parallel association-rule mining"
        ],
        "number_of_pages": "23",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-015-0037-9.pdf",
        "abstract": "Analysis of online user-generated content is receiving attention for its wide applica-tions from both academic researchers and industry stakeholders. In this pilot study, we address common Big Data problems of time constraints and memory costs involved with using standard single-machine hardware and software. A novel Big Data process-ing framework is proposed to investigate a niche subset of user-generated popular cul-ture content on Douban, a well-known Chinese-language online social network. Huge data samples are harvested via an asynchronous scraping crawler. We also discuss how to manipulate heterogeneous features from raw samples to facilitate analysis of various film details, review comments, and user profiles on Douban with specific regard to a wave of South Korean films (2003–2014), which have increased in popularity among Chinese film fans. In addition, an improved Apriori algorithm based on MapReduce is proposed for content-mining functions. An exploratory simulation of results dem-onstrates the flexibility and applicability of the proposed framework for extracting relevant information from complex social media data, knowledge which can in turn be extended beyond this niche dataset and used to inform producers and distributors of films, television shows, and other digital media content."
    },
    {
        "url": "https://paperity.org/p/75101740/big-data-big-bang",
        "title": "Big data, Big bang?",
        "authors": [
            "Jacques Bughin"
        ],
        "date_article": "01-2016",
        "short_description": "Using a random sample consisting of hundreds of companies worldwide, we are testing the impact on company performance of investing in big data projects targeted on three major business domains (namely, customer interface, company supply chain and competitors). The performance test relies on a so-called trans-logarithmic production function, allowing for a more direct test of the...",
        "keywords": [
            "Information systems",
            "Big data",
            "Data analytics",
            "Competitive performance",
            "Organization assets"
        ],
        "number_of_pages": "14",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-015-0014-3.pdf",
        "abstract": "Using a random sample consisting of hundreds of companies worldwide, we aretesting the impact on company performance of investing in big data projectstargeted on three major business domains (namely, customer interface, companysupply chain and competitors). The performance test relies on a so-calledtrans-logarithmic production function, allowing for a more direct test of thecomplementarity between big data capital and big data labour investments; further,we have used a Heckman correction to adjust for the fact that companies investingin big data are generally more productive than their peers.We confirm and extend early results of a productivity impact from big data. We findthat for the average of our sample, more productive firms are also faster adopters ofbig data than their industry peers (this explains 2.5% of productivity difference). Bigdata investments in labour and IT architecture are complements, with a totalproductivity growth effect of about 5.9%. Big data projects targeting customers andcompetitive intelligence domains bring slightly more performance than big dataprojects aimed at supply chain improvements."
    },
    {
        "url": "https://paperity.org/p/75126807/data-stream-clustering-by-divide-and-conquer-approach-based-on-vector-model",
        "title": "Data stream clustering by divide and conquer approach based on vector model",
        "authors": [
            "Madjid Khalilian",
            " Norwati Mustapha",
            " Nasir Sulaiman"
        ],
        "date_article": "01-2016",
        "short_description": "Recently, many researchers have focused on data stream processing as an efficient method for extracting knowledge from big data. Data stream clustering is an unsupervised approach that is employed for huge data. The continuous effort on data stream clustering method has one common goal which is to achieve an accurate clustering algorithm. However, there are some issues that are...",
        "keywords": [
            "Data mining; Data stream clustering; Vector space model; Divide-and-conquer"
        ],
        "number_of_pages": "21",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-015-0036-x.pdf",
        "abstract": "Recently, many researchers have focused on data stream processing as an efficient method for extracting knowledge from big data. Data stream clustering is an unsuper-vised approach that is employed for huge data. The continuous effort on data stream clustering method has one common goal which is to achieve an accurate clustering algorithm. However, there are some issues that are overlooked by the previous works in proposing data stream clustering solutions; (1) clustering dataset including big seg-ments of repetitive data, (2) monitoring clustering structure for ordinal data streams and (3) determining important parameters such as k number of exact clusters in stream of data. In this paper, DCSTREAM method is proposed with regard to the mentioned issues to cluster big datasets using the vector model and k-Means divide and conquer approach. Experimental results show that DCSTREAM can achieve superior quality and performance as compare to STREAM and ConStream methods for abrupt and gradual real world datasets. Results show that the usage of batch processing in DCSTREAM and ConStream is time consuming compared to STREAM but it avoids further analysis for detecting outliers and novel micro-clusters."
    },
    {
        "url": "https://paperity.org/p/74807566/the-ubiquitous-self-organizing-map-for-non-stationary-data-streams",
        "title": "The ubiquitous self-organizing map for non-stationary data streams",
        "authors": [
            "Bruno Silva",
            " Nuno Cavalheiro Marques"
        ],
        "date_article": "12-2015",
        "short_description": "The Internet of things promises a continuous flow of data where traditional database and data-mining methods cannot be applied. This paper presents improvements on the Ubiquitous Self-Organized Map (UbiSOM), a novel variant of the well-known Self-Organized Map (SOM), tailored for streaming environments. This approach allows ambient intelligence solutions using multidimensional...",
        "keywords": [
            "Self-organizing maps; Data streams; Non-stationary data; Clustering; Exploratory analysis; Sensor data"
        ],
        "number_of_pages": "22",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-015-0033-0.pdf",
        "abstract": "The Internet of things promises a continuous flow of data where traditional database and data-mining methods cannot be applied. This paper presents improvements on the Ubiquitous Self-Organized Map (UbiSOM), a novel variant of the well-known Self-Organized Map (SOM), tailored for streaming environments. This approach allows ambient intelligence solutions using multidimensional clustering over a continuous data stream to provide continuous exploratory data analysis. The average quantization error and average neuron utility over time are proposed and used to estimating the learning parameters, allowing the model to retain an indefinite plasticity and to cope with changes within a multidimensional data stream. We perform parameter sensitiv-ity analysis and our experiments show that UbiSOM outperforms existing proposals in continuously modeling possibly non-stationary data streams, converging faster to stable models when the underlying distribution is stationary and reacting accordingly to the nature of the change in continuous real world data streams."
    },
    {
        "url": "https://paperity.org/p/74880222/a-data-mining-framework-to-analyze-road-accident-data",
        "title": "A data mining framework to analyze road accident data",
        "authors": [
            "Sachin Kumar",
            " Durga Toshniw"
        ],
        "date_article": "11-2015",
        "short_description": "One of the key objectives in accident data analysis to identify the main factors associated with a road and traffic accident. However, heterogeneous nature of road accident data makes the analysis task difficult. Data segmentation has been used widely to overcome this heterogeneity of the accident data. In this paper, we proposed a framework that used K-modes clustering technique...",
        "keywords": [
            "Data mining; Accident analysis; Road accidents; Clustering"
        ],
        "number_of_pages": "18",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-015-0035-y.pdf",
        "abstract": "One of the key objectives in accident data analysis to identify the main factors associ-ated with a road and traffic accident. However, heterogeneous nature of road accident data makes the analysis task difficult. Data segmentation has been used widely to overcome this heterogeneity of the accident data. In this paper, we proposed a frame-work that used K-modes clustering technique as a preliminary task for segmentation of 11,574 road accidents on road network of Dehradun (India) between 2009 and 2014 (both included). Next, association rule mining are used to identify the various circum-stances that are associated with the occurrence of an accident for both the entire data set (EDS) and the clusters identified by K-modes clustering algorithm. The findings of cluster based analysis and entire data set analysis are then compared. The results reveal that the combination of k mode clustering and association rule mining is very inspiring as it produces important information that would remain hidden if no segmentation has been performed prior to generate association rules. Further a trend analysis have also been performed for each clusters and EDS accidents which finds different trends in different cluster whereas a positive trend is shown by EDS. Trend analysis also shows that prior segmentation of accident data is very important before analysis."
    },
    {
        "url": "https://paperity.org/p/74757630/an-industrial-big-data-pipeline-for-data-driven-analytics-maintenance-applications-in",
        "title": "An industrial big data pipeline for data-driven analytics maintenance applications in large-scale smart manufacturing facilities",
        "authors": [
            "P. O’Donovan",
            " K. Leahy",
            " K. Bruton"
        ],
        "date_article": "11-2015",
        "short_description": "The term smart manufacturing refers to a future-state of manufacturing, where the real-time transmission and analysis of data from across the factory creates manufacturing intelligence, which can be used to have a positive impact across all aspects of operations. In recent years, many initiatives and groups have been formed to advance smart manufacturing, with the most prominent...",
        "keywords": [
            "-"
        ],
        "number_of_pages": "26",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-015-0034-z.pdf",
        "abstract": "The term smart manufacturing refers to a future-state of manufacturing, where the real-time transmission and analysis of data from across the factory creates manufac-turing intelligence, which can be used to have a positive impact across all aspects of operations. In recent years, many initiatives and groups have been formed to advance smart manufacturing, with the most prominent being the Smart Manufacturing Leadership Coalition (SMLC), Industry 4.0, and the Industrial Internet Consortium. These initiatives comprise industry, academic and government partners, and con-tribute to the development of strategic policies, guidelines, and roadmaps relating to smart manufacturing adoption. In turn, many of these recommendations may be implemented using data-centric technologies, such as Big Data, Machine Learning, Simulation, Internet of Things and Cyber Physical Systems, to realise smart opera-tions in the factory. Given the importance of machine uptime and availability in smart manufacturing, this research centres on the application of data-driven analytics to industrial equipment maintenance. The main contributions of this research are a set of data and system requirements for implementing equipment maintenance applications in industrial environments, and an information system model that provides a scalable and fault tolerant big data pipeline for integrating, processing and analysing industrial equipment data. These contributions are considered in the context of highly regulated large-scale manufacturing environments, where legacy (e.g. automation controllers) and emerging instrumentation (e.g. internet-aware smart sensors) must be supported to facilitate initial smart manufacturing efforts.Open Access© 2015 O’Donovan etal. This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/),  which  permits  unrestricted  use,  distribution,  and  reproduction  in  any  medium,  provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made.RESEARCHO’Donovan et al. Journal of Big Data  (2015) 2:25 DOI 10.1186/s40537‑015‑0034‑z*Correspondence:   peter_odonovan@umail.ucc.ie IERG, University College Cork, Cork, Ireland\nPage 2 of 26O’Donovan et al. Journal of Big Data  (2015) 2:25 data  are  being  produced  each  day  [3].  This  exponential  growth  in  data  can  be  attrib-uted  to  a  number  of  technological  and  economic  factors,  including  the  emergence  of  cloud computing, increased mobile and electronic communication, as well as the overall decreased costs relating to compute and data resources. In addition, emerging technol-ogy  paradigms,  such  as  the  internet  of  things  (IoT),  which  focus  on  embedding  intelli-gent sensors in real-world environments and processes, will result in further exponential data  growth.  In  2011  it  was  estimated  that  more  than  7  billion  interconnected  devices  were in operation, which was greater than the world’s population at that time. However, given  the  potential  applications  of  IoT  across  numerous  sectors  and  industries,  includ-ing manufacturing, engineering, finance, medicine, and health, the number of intercon-nected devices in circulation is expected to rise to 24 billion by 2020 [4]. Therefore, given the anticipated shortage of personnel that are capable of managing this exponential data growth, there is a need for tools and frameworks that can simplify the process.As  big  data  analytics  permeates  different  sectors,  the  tools  and  frameworks  that  are  needed  to  address  domain-specific  challenges  will  emerge.  For  example,  modern  large-scale  manufacturing  facilities  utilise  sophisticated  sensors  and  networks  to  record  numerous  measurements  in  the  factory,  such  as  energy  consumption,  environmen-tal  impact  and  production  yield.  Given  the  existence  of  such  data  repositories,  these  facilities  should  be  in  a  position  to  leverage  big  data  analytics.  However,  a  number  of  domain-specific  challenges exist, including diverse communication standards, proprie-tary information and automation systems, heterogeneous data structures and interfaces, as well as inflexible governance policies regarding big data and cloud integration. These challenges coupled with the lack of inherent support for industrial devices, makes it dif-ficult  for  mainstream  big  data  tools  and  methods  (e.g.  Apache  Hadoop,  Spark,  etc.)  to  be  directly  applied  to  large-scale  manufacturing  facilities.  Although  some  of  the  afore-mentioned  challenges  are  addressed  by  different  commercial  tools,  their  scope  is  typi-cally limited to data (e.g. energy and environmental) that is needed to feed a particular application, rather than facilitate open access to data from across the factory. To address these  constraints,  as  well  as  many  more,  a  new  interdisciplinary  field  known  as  smart  manufacturing  has  emerged.  In  simple  terms,  smart  manufacturing  can  be  considered  the pursuit of data-driven manufacturing, where real-time data from sensors in the fac-tory  can  be  analysed  to  inform  decision-making.  More  generally,  smart  manufactur-ing  can  be  considered  a  specialisation  of  big  data,  whereby  big  data  technologies  and  methods are extended to meet the needs of manufacturing. Other prominent technology themes in smart manufacturing include machine learning, simulation, internet of things (IoT) and cyber physical systems (CPS).The  application  of  big  data  has  been  demonstrated  in  different  areas  of  manufactur-ing,  including  production,  supply  chain,  maintenance  and  diagnosis,  quality  manage-ment,  and  energy  [5].  This  paper  focuses  on  maintenance  and  diagnosis  because  of  the  role  it  plays  in  promoting  machine  uptime,  as  well  as  the  potential  impact  it  can  have  on  operating  costs,  with  some  estimates  claiming  equipment  maintenance  can  exceed  30  %  of  total  operating  costs,  or  between  60  and  75  %  of  equipment  lifecycle  cost  [6]. The  role  of  equipment  maintenance  is  an  important  component  in  smart  manufactur-ing.  Firstly,  smart  manufacturing  revolves  around  a  demand-driven,  customer-focused  and  highly-optimised  supply  chain.  Given  the  dynamic  and  optimised  nature  of  such  a  "
    },
    {
        "url": "https://paperity.org/p/74559854/a-survey-of-open-source-tools-for-machine-learning-with-big-data-in-the-hadoop-ecosystem",
        "title": "A survey of open source tools for machine learning with big data in the Hadoop ecosystem",
        "authors": [
            "Sara Landset",
            " Taghi M. Khoshgoftaar",
            " Aaron N. Richter"
        ],
        "date_article": "11-2015",
        "short_description": "With an ever-increasing amount of options, the task of selecting machine learning tools for big data can be difficult. The available tools have advantages and drawbacks, and many have overlapping uses. The world’s data is growing rapidly, and traditional tools for machine learning are becoming insufficient as we move towards distributed and real-time processing. This paper is...",
        "keywords": [
            "Machine learning; Big data; Hadoop; Mahout; MLlib; SAMOA; H2O; Spark; Flink; Storm"
        ],
        "number_of_pages": "36",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-015-0032-1.pdf",
        "abstract": "With an ever-increasing amount of options, the task of selecting machine learning tools for big data can be difficult. The available tools have advantages and drawbacks, and many have overlapping uses. The world’s data is growing rapidly, and traditional tools for machine learning are becoming insufficient as we move towards distributed and real-time processing. This paper is intended to aid the researcher or professional who understands machine learning but is inexperienced with big data. In order to evaluate tools, one should have a thorough understanding of what to look for. To that end, this paper provides a list of criteria for making selections along with an analysis of the advantages and drawbacks of each. We do this by starting from the beginning, and looking at what exactly the term “big data” means. From there, we go on to the Hadoop ecosystem for a look at many of the projects that are part of a typical machine learning architecture and an understanding of how everything might fit together. We discuss the advantages and disadvantages of three different processing paradigms along with a comparison of engines that implement them, including MapReduce, Spark, Flink, Storm, and H2O. We then look at machine learning libraries and frameworks includ-ing Mahout, MLlib, SAMOA, and evaluate them based on criteria such as scalability, ease of use, and extensibility. There is no single toolkit that truly embodies a one-size-fits-all solution, so this paper aims to help make decisions smoother by providing as much information as possible and quantifying what the tradeoffs will be. Additionally, throughout this paper, we review recent research in the field using these tools and talk about possible future directions for toolkit-based learning."
    },
    {
        "url": "https://paperity.org/p/74088200/survey-of-review-spam-detection-using-machine-learning-techniques",
        "title": "Survey of review spam detection using machine learning techniques",
        "authors": [
            "Michael Crawford",
            " Taghi M. Khoshgoftaar",
            " Joseph D. Prus"
        ],
        "date_article": "10-2015",
        "short_description": "Online reviews are often the primary factor in a customer’s decision to purchase a product or service, and are a valuable source of information that can be used to determine public opinion on these products or services. Because of their impact, manufacturers and retailers are highly concerned with customer feedback and reviews. Reliance on online reviews gives rise to the...",
        "keywords": [
            "Review spam",
            "Opinion mining",
            "Web mining",
            "Machine learning",
            "Big data",
            "Classification"
        ],
        "number_of_pages": "24",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-015-0029-9.pdf",
        "abstract": "Online reviews are often the primary factor in a customer’s decision to purchase aproduct or service, and are a valuable source of information that can be used todetermine public opinion on these products or services. Because of their impact,manufacturers and retailers are highly concerned with customer feedback andreviews. Reliance on online reviews gives rise to the potential concern thatwrongdoers may create false reviews to artificially promote or devalue productsand services. This practice is known as Opinion (Review) Spam, where spammersmanipulate and poison reviews (i.e., making fake, untruthful, or deceptive reviews)for profit or gain. Since not all online reviews are truthful and trustworthy, it isimportant to develop techniques for detecting review spam. By extractingmeaningful features from the text using Natural Language Processing (NLP), it ispossible to conduct review spam detection using various machine learningtechniques. Additionally, reviewer information, apart from the text itself, can beused to aid in this process. In this paper, we survey the prominent machinelearning techniques that have been proposed to solve the problem of reviewspam detection and the performance of different approaches for classification anddetection of review spam. The majority of current research has focused onsupervised learning methods, which require labeled data, a scarcity when it comesto online review spam. Research on methods for Big Data are of interest, sincethere are millions of online reviews, with many more being generated daily. Todate, we have not found any papers that study the effects of Big Data analytics forreview spam detection. The primary goal of this paper is to provide a strong andcomprehensive comparative study of current research on detecting review spamusing various machine learning techniques and to devise methodology forconducting further investigation."
    },
    {
        "url": "https://paperity.org/p/73931879/big-data-analytics-a-survey",
        "title": "Big data analytics: a survey",
        "authors": [
            "Chun-Wei Tsai",
            " Chin-Feng Lai",
            " Han-Chieh Chao"
        ],
        "date_article": "10-2015",
        "short_description": "The age of big data is now coming. But the traditional data analytics may not be able to handle such large quantities of data. The question that arises now is, how to develop a high performance platform to efficiently analyze big data and how to design an appropriate mining algorithm to find the useful things from big data. To deeply discuss this issue, this paper begins with a...",
        "keywords": [
            "Big data; data analytics; data mining"
        ],
        "number_of_pages": "32",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-015-0030-3.pdf",
        "abstract": "The age of big data is now coming. But the traditional data analytics may not be able to handle such large quantities of data. The question that arises now is, how to develop a high performance platform to efficiently analyze big data and how to design an appropriate mining algorithm to find the useful things from big data. To deeply discuss this issue, this paper begins with a brief introduction to data analytics, followed by the discussions of big data analytics. Some important open issues and further research directions will also be presented for the next step of big data analytics."
    },
    {
        "url": "https://paperity.org/p/74165270/visualizing-big-data-with-augmented-and-virtual-reality-challenges-and-research-agenda",
        "title": "Visualizing Big Data with augmented and virtual reality: challenges and research agenda",
        "authors": [
            "Ekaterina Olshannikova",
            " Aleksandr Ometov",
            " Yevgeni Koucheryavy"
        ],
        "date_article": "10-2015",
        "short_description": "This paper provides a multi-disciplinary overview of the research issues and achievements in the field of Big Data and its visualization techniques and tools. The main aim is to summarize challenges in visualization methods for existing Big Data, as well as to offer novel solutions for issues related to the current state of Big Data Visualization. This paper provides a...",
        "keywords": [
            "Big Data; Visualization; Virtual reality; Augmented reality; Mixed reality; Human interaction; Evolution"
        ],
        "number_of_pages": "27",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-015-0031-2.pdf",
        "abstract": "This paper provides a multi-disciplinary overview of the research issues and achieve-ments in the field of Big Data and its visualization techniques and tools. The main aim is to summarize challenges in visualization methods for existing Big Data, as well as to offer novel solutions for issues related to the current state of Big Data Visualization. This paper provides a classification of existing data types, analytical methods, visualization techniques and tools, with a particular emphasis placed on surveying the evolution of visualization methodology over the past years. Based on the results, we reveal disad-vantages of existing visualization methods. Despite the technological development of the modern world, human involvement (interaction), judgment and logical thinking are necessary while working with Big Data. Therefore, the role of human perceptional limitations involving large amounts of information is evaluated. Based on the results, a non-traditional approach is proposed: we discuss how the capabilities of Augmented Reality and Virtual Reality could be applied to the field of Big Data Visualization. We discuss the promising utility of Mixed Reality technology integration with applica-tions in Big Data Visualization. Placing the most essential data in the central area of the human visual field in Mixed Reality would allow one to obtain the presented informa-tion in a short period of time without significant data losses due to human perceptual issues. Furthermore, we discuss the impacts of new technologies, such as Virtual Reality displays and Augmented Reality helmets on the Big Data visualization as well as to the classification of the main challenges of integrating the technology."
    },
    {
        "url": "https://paperity.org/p/74039162/big-data-in-manufacturing-a-systematic-mapping-study",
        "title": "Big data in manufacturing: a systematic mapping study",
        "authors": [
            "Peter O’Donovan",
            " Kevin Leahy",
            " Ken Bruton"
        ],
        "date_article": "09-2015",
        "short_description": "The manufacturing industry is currently in the midst of a data-driven revolution, which promises to transform traditional manufacturing facilities in to highly optimised smart manufacturing facilities. These smart facilities are focused on creating manufacturing intelligence from real-time data to support accurate and timely decision-making that can have a positive impact across...",
        "keywords": [
            "Big data",
            "Manufacturing",
            "Smart manufacturing",
            "Industry 4.0",
            "Big data analytics",
            "Engineering informatics",
            "Machine learning",
            "Big data systems",
            "Distributed computing",
            "Cyber physical systems",
            "Internet of things",
            "loT"
        ],
        "number_of_pages": "22",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-015-0028-x.pdf",
        "abstract": "The manufacturing industry is currently in the midst of a data-driven revolution,which promises to transform traditional manufacturing facilities in to highlyoptimised smart manufacturing facilities. These smart facilities are focused oncreating manufacturing intelligence from real-time data to support accurate andtimely decision-making that can have a positive impact across the entireorganisation. To realise these efficiencies emerging technologies such as Internet ofThings (IoT) and Cyber Physical Systems (CPS) will be embedded in physicalprocesses to measure and monitor real-time data from across the factory, which willultimately give rise to unprecedented levels of data production. Therefore,manufacturing facilities must be able to manage the demands of exponentialincrease in data production, as well as possessing the analytical techniques neededto extract meaning from these large datasets. More specifically, organisations mustbe able to work with big data technologies to meet the demands of smartmanufacturing. However, as big data is a relatively new phenomenon and potentialapplications to manufacturing activities are wide-reaching and diverse, there hasbeen an obvious lack of secondary research undertaken in the area. Withoutsecondary research, it is difficult for researchers to identify gaps in the field, as wellas aligning their work with other researchers to develop strong research themes. Inthis study, we use the formal research methodology of systematic mapping toprovide a breadth-first review of big data technologies in manufacturing."
    },
    {
        "url": "https://paperity.org/p/73721686/erratum-to-structural-and-functional-analytics-for-community-detection-in-large-scale",
        "title": "Erratum to: Structural and functional analytics for community detection in large-scale complex networks",
        "authors": [
            "Pravin Chopade",
            " Justin Zhan"
        ],
        "date_article": "08-2015",
        "short_description": "",
        "keywords": [
            "-"
        ],
        "number_of_pages": "2",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-015-0026-z.pdf",
        "abstract": "MOpen AccessErratum to: Structural and functionalanalytics for community detection inlarge-scale complex networksPravin Chopade1*†and Justin Zhan2†* Correspondence:pvchopad@ncat.edu†Equal contributors1Department of Computer Science,College of Engineering, NorthCarolina A and T State University,305 Cherry Hall, 1601 East MarketStreet, Greensboro, NC 27411, USAFull list of author information isavailable at the end of the articleErratumAfter the publication of this work [1], we noticed that an incorrect version of Tabletwo (Table 1 here) was published. An incorrect version of Algorithm four (Algorithm 1here) was also published. The correct versions of Table two and Algorithm four areprovided here and have been updated in the original article.The publisher apologises for any inconvenience caused.Author details1Department of Computer Science, College of Engineering, North Carolina A and T State University, 305 Cherry Hall,1601 East Market Street, Greensboro, NC 27411, USA.2Department of Computer Science, College of Engineering,University of Nevada-Las Vegas, 4505 S. Maryland Pkwy, Las Vegas, NV 89154, USA.Received: 9 July 2015 Accepted: 29 July 2015Reference1.   Chopade Z (2015) Structural and functional analytics for community detection in large-scale complex networks.J Big Data 2:11Submit your manuscript to a journal and benefi  t  from:7Convenient online submission7Rigorous peer review7Immediate publication on acceptance7Open access: articles freely available online7High visibility within the fi eld7Retaining the copyright to your articleSubmit your next manuscript at 7 springeropen.com© 2015 Chopade and Zhan.Open AccessThis article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link tothe Creative Commons license, and indicate if changes were made.Chopade and ZhanJournal of Big Data (2015) 2:19 DOI 10.1186/s40537-015-0026-z\nTable 1Modularity ComparisonAlgorithms→FNDGAFDMSTABMMOCNetworks↓Size↓QFNQDGAQFDQMSTAB(QOur method)PhD’s in CS18820.96100.96100.92950.96010.9755Facebook18990.27170.25670.37510.37420.3860SciMet30840.54690.59490.61460.61460.6502US Power Grid49410.93410.93580.93470.93480.9587FNFast Newman based on a greedy agglomerative methodDGAModularity optimization based on Danon greedy agglomerative methodFDFast detection of communities using modularity optimizationMSTABModularity based on stabilityMMOCModified Modularity for Overlapping Community Detection (Our method)Chopade and ZhanJournal of Big Data (2015) 2:19 Page 2 of 2"
    },
    {
        "url": "https://paperity.org/p/73823348/choosing-the-right-nosql-database-for-the-job-a-quality-attribute-evaluation",
        "title": "Choosing the right NoSQL database for the job: a quality attribute evaluation",
        "authors": [
            "João Ricardo Lourenço",
            " Bruno Cabral",
            " Paulo Carreiro"
        ],
        "date_article": "08-2015",
        "short_description": "For over forty years, relational databases have been the leading model for data storage, retrieval and management. However, due to increasing needs for scalability and performance, alternative systems have emerged, namely NoSQL technology. The rising interest in NoSQL technology, as well as the growth in the number of use case scenarios, over the last few years resulted in an...",
        "keywords": [
            "NoSQL databases",
            "Key-value",
            "Document store",
            "Columnar",
            "Graph",
            "Software engineering",
            "Quality attributes",
            "Software architecture"
        ],
        "number_of_pages": "26",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-015-0025-0.pdf",
        "abstract": "For over forty years, relational databases have been the leading model for data storage,retrieval and management. However, due to increasing needs for scalability andperformance, alternative systems have emerged, namely NoSQL technology. The risinginterest in NoSQL technology, as well as the growth in the number of use casescenarios, over the last few years resulted in an increasing number of evaluations andcomparisons among competing NoSQL technologies. While most research workmostly focuses on performance evaluation using standard benchmarks, it is importantto notice that the architecture of real world systems is not only driven by performancerequirements, but has to comprehensively include many other quality attributerequirements. Software quality attributes form the basis from which software engineersand architects develop software and make design decisions. Yet, there has been noquality attribute focused survey or classification of NoSQL databases where databasesare compared with regards to their suitability for quality attributes common on thedesign of enterprise systems. To fill this gap, and aid software engineers and architects,in this article, we survey and create a concise and up-to-date comparison of NoSQLengines, identifying their most beneficial use case scenarios from the softwareengineer point of view and the quality attributes that each of them is most suited to."
    },
    {
        "url": "https://paperity.org/p/73830727/a-novel-algorithm-for-fast-and-scalable-subspace-clustering-of-high-dimensional-data",
        "title": "A novel algorithm for fast and scalable subspace clustering of high-dimensional data",
        "authors": [
            "Amardeep Kaur",
            " Amitava D"
        ],
        "date_article": "08-2015",
        "short_description": "Rapid growth of high dimensional datasets in recent years has created an emergent need to extract the knowledge underlying them. Clustering is the process of automatically finding groups of similar data points in the space of the dimensions or attributes of a dataset. Finding clusters in the high dimensional datasets is an important and challenging data mining problem. Data group...",
        "keywords": [
            "Data mining",
            "High dimensional data",
            "Subspace clustering",
            "Scalable data mining"
        ],
        "number_of_pages": "24",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-015-0027-y.pdf",
        "abstract": "Rapid growth of high dimensional datasets in recent years has created an emergentneed to extract the knowledge underlying them. Clustering is the process ofautomatically finding groups of similar data points in the space of the dimensions orattributes of a dataset. Finding clusters in the high dimensional datasets is an importantand challenging data mining problem. Data group together differently under differentsubsets of dimensions, called subspaces. Quite often a dataset can be better understoodby clustering it in its subspaces, a process called subspace clustering. But the exponentialgrowth in the number of these subspaces with the dimensionality of data makes thewhole process of subspace clustering computationally very expensive. There is agrowing demand for efficient and scalable subspace clustering solutions in many Bigdata application domains like biology, computer vision, astronomy and social networking.Apriori based hierarchical clustering is a promising approach to find all possible higherdimensional subspace clusters from the lower dimensional clusters using a bottom-upprocess. However, the performance of the existing algorithms based on this approachdeteriorates drastically with the increase in the number of dimensions. Most of thesealgorithms require multiple database scans and generate a large number of redundantsubspace clusters, either implicitly or explicitly, during the clustering process. In thispaper, we present SUBSCALE, a novel clustering algorithm to find non-trivial subspaceclusters with minimal cost and it requires onlykdatabase scans for ak-dimensionaldata set. Our algorithm scales very well with the dimensionality of the dataset and ishighly parallelizable. We present the details of the SUBSCALE algorithm and itsevaluation in this paper."
    },
    {
        "url": "https://paperity.org/p/73823681/database-application-model-and-its-service-for-drug-discovery-in-model-driven",
        "title": "Database application model and its service for drug discovery in Model-driven architecture",
        "authors": [
            "Noriko Etani"
        ],
        "date_article": "08-2015",
        "short_description": "Big data application has many data resources and data. In the first stage of software engineering, a service overview or a system overview cannot be seen. In this paper, we propose that two processes of “Big data analytics” and “Implementation of data modeling” should be collaborated with Model-driven architecture (MDA). Data modeling with those two process in MDA should be...",
        "keywords": [
            "PLS regression analysis",
            "Discriminant analysis",
            "Support vector machine",
            "Prediction model",
            "Drug side effect",
            "Drug discovery",
            "Model-driven architecture (MDA)"
        ],
        "number_of_pages": "17",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-015-0024-1.pdf",
        "abstract": "Big data application has many data resources and data. In the first stage of softwareengineering, a service overview or a system overview cannot be seen. In this paper, wepropose that two processes of “Big data analytics” and “Implementation of datamodeling” should be collaborated with Model-driven architecture (MDA). Datamodeling with those two process in MDA should be repeated fast in order to verify thedata model and to find a new data resource for a service. Our first research goal of bigdata application is to predict side effect of drug which is one of screening methods indrug discovery. This prediction model is constructed with data mining methods at theintersection of statistics, machine learning and database system. Moreover, a newservice for drug discovery by new uses for old drugs can be found in data modelingand developed. We demonstrate that the prediction model and the data model fordrug discovery are implemented as a prototype system to verify those models andtheir practicality."
    },
    {
        "url": "https://paperity.org/p/73577439/cabinet-tree-an-orthogonal-enclosure-approach-to-visualizing-and-exploring-big-data",
        "title": "Cabinet Tree: an orthogonal enclosure approach to visualizing and exploring big data",
        "authors": [
            "Yalong Yang",
            " Kang Zhang",
            " Jianrong Wang"
        ],
        "date_article": "07-2015",
        "short_description": "Treemaps are well-known for visualizing hierarchical data. Most related approaches have been focused on layout algorithms and paid little attention to other display properties and interactions. Furthermore, the structural information in conventional Treemaps is too implicit for viewers to perceive. This paper presents Cabinet Tree, an approach that: i) draws branches explicitly...",
        "keywords": [
            "Orthogonal enclosure",
            "Tree drawing",
            "Hierarchical visualization",
            "Big data"
        ],
        "number_of_pages": "18",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-015-0022-3.pdf",
        "abstract": "Treemaps are well-known for visualizing hierarchical data. Most related approacheshave been focused on layout algorithms and paid little attention to other displayproperties and interactions. Furthermore, the structural information in conventionalTreemaps is too implicit for viewers to perceive. This paper presents Cabinet Tree, anapproach that: i) draws branches explicitly to show relational structures, ii) adapts aspace-optimized layout for leaves and maximizes the space utilization, iii) uses coloringand labeling strategies to clearly reveal patterns and contrast different attributesintuitively. We also apply the continuous node selection and detail window techniquesto support user interaction with different levels of the hierarchies. Our quantitativeevaluations demonstrate that Cabinet Tree achieves good scalability for increasedresolutions and big datasets."
    },
    {
        "url": "https://paperity.org/p/73426354/meta-mapreduce-for-scalable-data-mining",
        "title": "Meta-MapReduce for scalable data mining",
        "authors": [
            "Xuan Liu",
            " Xiaoguang Wang",
            " Stan Matwin"
        ],
        "date_article": "07-2015",
        "short_description": "W e h a v e e n t e r e d t h e b i g data age. Knowledge extraction from massive data is becoming more and more urgent. MapReduce provides a feasible framework for programming machine learning algorithms in Map and Reduce functions. The relatively simple programming interface has helped to solve machine learning algorithms’ scalability problems. However, this framework suffers...",
        "keywords": [
            "Hadoop",
            "MapReduce",
            "Meta-learning",
            "Big data",
            "Parallel computing",
            "Adaboost"
        ],
        "number_of_pages": "21",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-015-0021-4.pdf",
        "abstract": "We have entered the big data age. Knowledge extraction from massive data is becomingmore and more urgent. MapReduce provides a feasible framework for programmingmachine learning algorithms in Map and Reduce functions. The relatively simpleprogramming interface has helped to solve machine learning algorithms’ scalabilityproblems. However, this framework suffers from an obvious weakness: it does notsupport iterations. This makes it difficult for algorithms requiring iterations to fullyexplore the efficiency of MapReduce. In this paper, we propose to apply Meta-learningprogrammed with MapReduce to avoid parallelizing machine learning algorithmswhile also improving their scalability to big datasets. The experiments conducted onHadoop’s fully distributed mode on Amazon EC2 demonstrate that our algorithmMeta-MapReduce (MMR) reduces the training computational complexity significantlywhen the number of computing nodes increases while obtaining smaller error ratesthan those on a single node. The comparison of MMR with the contemporaryparallelized AdaBoost algorithm, AdaBoost.PL, shows that MMR obtains lower error rates."
    },
    {
        "url": "https://paperity.org/p/73422141/socr-data-dashboard-an-integrated-big-data-archive-mashing-medicare-labor-census-and",
        "title": "SOCR data dashboard: an integrated big data archive mashing medicare, labor, census and econometric information",
        "authors": [
            "Syed S Husain",
            " Alexandr Kalinin",
            " Anh Truong"
        ],
        "date_article": "07-2015",
        "short_description": "IntroductionIntuitive formulation of informative and computationally-efficient queries on big and complex datasets present a number of challenges. As data collection is increasingly streamlined and ubiquitous, data exploration, discovery and analytics get considerably harder. Exploratory querying of heterogeneous and multi-source information is both difficult and necessary to...",
        "keywords": [
            "-"
        ],
        "number_of_pages": "18",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-015-0018-z.pdf",
        "abstract": "Introduction:Intuitive formulation of informative and computationally-efficient querieson big and complex datasets present a number of challenges. As data collection isincreasingly streamlined and ubiquitous, data exploration, discovery and analytics getconsiderably harder. Exploratory querying ofheterogeneous and multi-source informationis both difficult and necessary to advance our knowledge about the world around us.Research design:We developed a mechanism to integrate dispersed multi-source dataand service the mashed information via human and machine interfaces in a secure,scalable manner. This process facilitates the exploration of subtle associations betweenvariables, population strata, or clusters of data elements, which may be opaque tostandard independent inspection of the individual sources. This a new platform includesa device agnostic tool (Dashboard webapp, http://socr.umich.edu/HTML5/Dashboard/)for graphical querying, navigating and exploring the multivariate associations incomplex heterogeneous datasets.Results:The paper illustrates this core functionality and serviceoriented infrastructureusing healthcare data (e.g., US data from the 2010 Census, Demographic and Economicsurveys, Bureau of Labor Statistics, and Centerfor Medicare Services) as well as Parkinson’sDisease neuroimaging data. Both the back-end data archive and the front-end dashboardinterfaces are continuously expanded to include additional dataelements and new waysto customize the human and machine interactions.Conclusions:A client-side data import utility allows for easy and intuitive integration ofuser-supplied datasets. This completely open-science framework may be used forexploratory analytics, confirmatory analyses, meta-analyses, and education and trainingpurposes in a wide variety of fields.IntroductionState of open-scienceOpen-science refers to a new paradigm liberalizing access, advancement, control, ac-creditation and ownership of scientific knowledge, information resources (e.g., data),and decision-making instruments (e.g., software tools). In open-science settings, theentire community has open unrestricted access to resources incentivizing user partici-pation at different levels (e.g., consumption, mashing, development), enabling social in-teractions where the collective outcome is more than the sum of its individual parts,novice learners and experts can choice, contribute to and debate concepts, algorithms,© 2015 Husain et al. This is an Open Access article distributed under the terms of the Creative Commons Attribution License (http://creativecommons.org/licenses/by/4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided theoriginal work is properly credited.Husainet al. Journal of Big Data (2015) 2:13 DOI 10.1186/s40537-015-0018-z\nanalytics, results and theoretic principles. This paradigm requires a critical mass ofparticipation, commitment for trust, diversity, sharing and cooperation. Outcome productsoften take unexpected and innovative turns, looking at problems from different anglesand employing expertise, methods and services, which initially may appear as not-interoperable. There are many examples of successful open-science initiatives. Two of theseare the Polymath project [1] and the Mozilla Open-Science [2]. Polymath1 problem identi-fied by the Polymath community involved searching for a new combinatorial proof to theHales–Jewett theorem [3]. The project morphed into multiple independent threads, whichled to a solution of the problem within several months, using constructive contributionsfrom dozens of people. An international teamof scientists and engineers participated inMozilla Open-Science 52-h coding marathon to enhance open science lessons, learningmaterials, teaching tools, and software resources and establish minimal standards for openscience education. This virtual activity used open-resources (e.g., GitHub) to establish re-producible research guidelines, develop and enhance teaching materials in a diverse arrayof scientific disciplines (bioinformatics, medical imaging, oceanography, social science, etc.)There are now over 1 billion web-sites hosted on millions of Internet servers [4]. Thiswidespread availability of information provides web access to resources that can be effi-ciently shared with minimal barriers to content and data. Yet, there are significant bar-riers to effective open-science. Some of the challenges include lack of interoperabilityor compatibility of resources, licensing restrictions, federal, state and local regulations,mistrust in attribution, ineffective infrastructure, and discipline boundaries.Challenges in managing, fusing, processing, servicing and understandingheterogeneous dataBig Data is ubiquitous in many, if not all, scientific disciplines, applied studies and researchexplorations. Its characteristics include heterogeneity, multiple-scales, incongruent space-timesampling, format complexity, privacy restrictions, and multi-source provenance. There aresignificant, unique and impeding challenges associated with Big Data modelling, handling, an-alytics, interpretation and progress of extracting information and gaining knowledge from it.Each step in the complete workflow from data acquisition, to data storage, servicing, archival,manipulation and processing present problems that need to be overcome to enable the courseof information extraction and decision-making. Lack of standards, unstructured data formatsand aggregation of data (elements and cases) inhibit the semantic content processing, searchand holistic data understanding. Data volume is not the only bottleneck. In many applications,thedatacomplexityandheterogeneityfrequentlyconstraintheuseofestablishedmethods,techniques, software tools and services. One of the paramount feature of Big Data is itstemporal dynamism. The value of models, inference, and findings presented using a stale datarepositories rapidly depreciate with time [5]. Just like the formulation of conditional probabil-ity and Bayesian inference [6] advanced our understanding of marginal and joint probabilities[7], data mashing, the process of fusion of information from disparate resources, has a poten-tial to revolutionize our understanding of natural processes and complex phenomena.Big data analytic infrastructureThere are many alternative models outlining the Big Data infrastructure componentsand their interrelations. The core of most of them include hardware resources forHusainet al. Journal of Big Data (2015) 2:13 Page 2 of 18"
    },
    {
        "url": "https://paperity.org/p/73450894/actionable-knowledge-as-a-service-akaas-leveraging-big-data-analytics-in-cloud-computing",
        "title": "Actionable Knowledge As A Service (AKAAS): Leveraging big data analytics in cloud computing environments",
        "authors": [
            "Audrey Depeige",
            " Dimitri Doyencour"
        ],
        "date_article": "07-2015",
        "short_description": "Knowledge-as-a service is an emerging research trend that constitutes a promising path for organizations aiming to achieve better customer support and decision-making across a wide range of content providers. However, only few studies have explored how traditional knowledge management processes and practices are challenged and evolve in relation to social and technological...",
        "keywords": [
            "Cloud computing",
            "Knowledge management",
            "Knowledge as a service",
            "Data analytics",
            "Business intelligence",
            "User-centered practices"
        ],
        "number_of_pages": "16",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-015-0023-2.pdf",
        "abstract": "Knowledge-as-a service is an emerging research trend that constitutes a promisingpath for organizations aiming to achieve better customer support anddecision-making across a wide range of content providers. However, only few studieshave explored how traditional knowledge management processes and practices arechallenged and evolve in relation to social and technological transformationstemming from the new age of cloud-computing environments. This research paperattempts to answer this gap by introducing a new framework for the managementof knowledge in the cloud.The objective of this study is twofold. First, it aims to develop a knowledge-baseddecision framework using recent knowledge-as-as-service paradigms for creating,retrieving and reusing technical knowledge in cloud environments. Second, andin relation to this view, the current state of data analysis (business intelligence, bigdata analytics) is assessed in light of cloud computing environments and thesimultaneous emergence of new knowledge management paradigms. An overviewof cloud computing challenges is given as well as a representation of supportingknowledge management processes, followed by the description of relatedapplications of the AKAAS (Actionable Knowledge As A Service) operatingframework.The first part of the research develops upon literature on Knowledge ManagementSystems (KMS) frameworks. The second part of this study examines and assesses thenature of data and analytics required to provide on-demand delivery of knowledgein the cloud. Finally, the third part of the paper explores practical applications of theframework to guide the development of knowledge management processes andpractices towards internal and external users adoption. We conclude by suggestingfuture research directions, including case studies examining differential knowledgecloud-based designs."
    },
    {
        "url": "https://paperity.org/p/73586115/structural-and-functional-analytics-for-community-detection-in-large-scale-complex",
        "title": "Structural and functional analytics for community detection in large-scale complex networks",
        "authors": [
            "Pravin Chopade",
            " Justin Zhan"
        ],
        "date_article": "07-2015",
        "short_description": "Community structure is thought to be one of the main organizing principles in most complex networks. Big data and complex networks represent an area which researchers are analyzing worldwide. Of special interest are groups of vertices within which connections are dense. In this paper we begin with discussing community dynamics and exploring complex network structural parameters...",
        "keywords": [
            "Community",
            "Big data",
            "Complex network",
            "Laplacian",
            "Centrality",
            "Robustness",
            "Modularity"
        ],
        "number_of_pages": "28",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-015-0019-y.pdf",
        "abstract": "Community structure is thought to be one of the main organizing principles in mostcomplex networks. Big data and complex networks represent an area whichresearchers are analyzing worldwide. Of special interest are groups of vertices withinwhich connections are dense. In this paper we begin with discussing communitydynamics and exploring complex network structural parameters. We put forwardstructural and functional models for analyzing complex networks under situations ofperturbations. We introduce modified adjacency and modified Laplacian matrices. Wefurther introduce network or degree centrality (weighted Laplacian centrality) based onmodified Laplacian, weighted micro-community centrality. We discuss its robustnessand importance for micro-community detection for social and technological complexnetworks with overlapping communities. We also introduce ’k-clique sub-community’overlapping community detection based on degree and weighted micro-communitycentrality. The proposed algorithms use optimal partition of k-clique sub-communityfor modularity optimization. We establish relationship between degree centrality andmodularity. This proposed method with modified adjacency matrixhelps us solveNP-hard problem."
    },
    {
        "url": "https://paperity.org/p/73427176/performance-analysis-of-concurrent-workflows",
        "title": "Performance analysis of concurrent workflows",
        "authors": [
            "Andreas Kempa-Liehr"
        ],
        "date_article": "07-2015",
        "short_description": "Automated workflows are the key concept of big data pipelines in science, engineering and enterprise applications. The performance analysis of automated workflows is an important topic of the continuous improvement process and the foundation of designing new workflows. This paper introduces the concept of process evolution functions and event reduction policies, which allow for...",
        "keywords": [
            "Big data pipeline",
            "High performance computing",
            "Complex event processing",
            "Communicating sequential processes",
            "Continuous improvement process"
        ],
        "number_of_pages": "14",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-015-0017-0.pdf",
        "abstract": "Automated workflows are the key concept of big data pipelines in science, engineeringand enterprise applications. The performance analysis of automated workflows is animportant topic of the continuous improvement process and the foundation ofdesigning new workflows. This paper introduces the concept of process evolutionfunctions and event reduction policies, which allow for the time resolved visualizationof an unlimited number of concurrent workflows by means of aggregated task views.The visualization allows for an intuitive approach to the performance analysis ofconcurrent workflows. The theoretical foundation of this approach is applicable forworkflows represented by directed acyclic graphs. It is explained on the basis of asimple IO-workflow model, which is typically found for distributed resourcemanagement systems utilized for many-task computing.AMS subject classification:68Mxx"
    },
    {
        "url": "https://paperity.org/p/73627920/sharing-big-biomedical-data",
        "title": "Sharing big biomedical data",
        "authors": [
            "Arthur W Toga",
            " Ivo D Dinov"
        ],
        "date_article": "06-2015",
        "short_description": "Background The promise of Big Biomedical Data may be offset by the enormous challenges in handling, analyzing, and sharing it. In this paper, we provide a framework for developing practical and reasonable data sharing policies that incorporate the sociological, financial, technical and scientific requirements of a sustainable Big Data dependent scientific community. Findings Many...",
        "keywords": [
            "Big data",
            "Policy",
            "Sharing",
            "Analytics",
            "Privacy"
        ],
        "number_of_pages": "12",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-015-0016-1.pdf",
        "abstract": "Background:The promise of Big Biomedical Data may be offset by the enormouschallenges in handling, analyzing, and sharing it. In this paper, we provide aframework for developing practical and reasonable data sharing policies thatincorporate the sociological, financial, technical and scientific requirements of asustainable Big Data dependent scientific community.Findings:Many biomedical and healthcare studies may be significantly impacted byusing large, heterogeneous and incongruent datasets; however there are significanttechnical, social, regulatory, and institutional barriers that need to be overcome toensure the power of Big Data overcomes these detrimental factors.Conclusions:Pragmatic policies that demand extensive sharing of data, promotionof data fusion, provenance, interoperability and balance security and protection ofpersonal information are critical for the long term impact of translational Big Dataanalytics."
    },
    {
        "url": "https://paperity.org/p/73508903/summarizing-large-text-collection-using-topic-modeling-and-clustering-based-on-mapreduce",
        "title": "Summarizing large text collection using topic modeling and clustering based on MapReduce framework",
        "authors": [
            "N K Nagwani"
        ],
        "date_article": "06-2015",
        "short_description": "Document summarization provides an instrument for faster understanding the collection of text documents and has a number of real life applications. Semantic similarity and clustering can be utilized efficiently for generating effective summary of large text collections. Summarizing large volume of text is a challenging and time consuming problem particularly while considering the...",
        "keywords": [
            "Summarizing large text",
            "Semantic similarity",
            "Text clustering",
            "Clustering based summarization",
            "Big Text Data analysis"
        ],
        "number_of_pages": "18",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-015-0020-5.pdf",
        "abstract": "Document summarization provides an instrument for faster understanding the collectionof text documents and has a number of real life applications. Semantic similarity andclustering can be utilized efficiently for generating effective summary of large textcollections. Summarizing large volume of text is a challenging and time consumingproblem particularly while considering the semantic similarity computation insummarization process. Summarization of text collection involves intensive textprocessing and computations to generate the summary. MapReduce is provenstate of art technology for handling Big Data. In this paper, a novel framework based onMapReduce technology is proposed for summarizing large text collection. The proposedtechnique is designed using semantic similarity based clustering and topicmodeling using Latent Dirichlet Allocation (LDA) for summarizing the large textcollection over MapReduce framework. The summarization task is performed infour stages and provides amodular implementation of multiple documentssummarization. The presented technique is evaluated in terms of scalability andvarious text summarization parameters namely, compression ratio, retention ratio,ROUGE and Pyramid score are also measured. The advantages of MapReduceframework are clearly visible from the experiments and it is also demonstratedthat MapReduce provides afaster implementation of summarizing large textcollections and is a powerful tool in Big Text Data analysis."
    },
    {
        "url": "https://paperity.org/p/73244106/sentiment-analysis-using-product-review-data",
        "title": "Sentiment analysis using product review data",
        "authors": [
            "Xing Fang",
            " Justin Zhan"
        ],
        "date_article": "06-2015",
        "short_description": "Sentiment analysis or opinion mining is one of the major tasks of NLP (Natural Language Processing). Sentiment analysis has gain much attention in recent years. In this paper, we aim to tackle the problem of sentiment polarity categorization, which is one of the fundamental problems of sentiment analysis. A general process for sentiment polarity categorization is proposed with...",
        "keywords": [
            "-"
        ],
        "number_of_pages": "-",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-015-0015-2.pdf",
        "abstract": "-"
    },
    {
        "url": "https://paperity.org/p/73367735/dcms-a-data-analytics-and-management-system-for-molecular-simulation",
        "title": "DCMS: A data analytics and management system for molecular simulation",
        "authors": [
            "Anand Kumar",
            " Vladimir Grupcev",
            " Meryem Berrad"
        ],
        "date_article": "06-2015",
        "short_description": "Molecular Simulation (MS) is a powerful tool for studying physical/chemical features of large systems and has seen applications in many scientific and engineering domains. During the simulation process, the experiments generate a very large number of atoms and intend to observe their spatial and temporal relationships for scientific analysis. The sheer data volumes and their...",
        "keywords": [
            "-"
        ],
        "number_of_pages": "22",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-014-0009-5.pdf",
        "abstract": "Molecular Simulation (MS) is a powerful tool for studying physical/chemical features oflarge systems and has seen applications in many scientific and engineering domains.During the simulation process, the experiments generate a very large number of atomsand intend to observe their spatial and temporal relationships for scientific analysis. Thesheer data volumes and their intensive interactions impose significant challenges fordata accessing, managing, and analysis. To date, existing MS software systems fall shorton storage and handling of MS data, mainly because of the missing of a platform tosupport applications that involve intensive data access and analytical process. In thispaper, we present the database-centric molecular simulation (DCMS) system our teamdeveloped in the past few years. The main idea behind DCMS is to store MS data in arelational database management system (DBMS) to take advantage of the declarativequery interface (i.e., SQL), data access methods, query processing, and optimizationmechanisms of modern DBMSs. A unique challenge is to handle the analytical queriesthat are often compute-intensive. For that, we developed novel indexing and queryprocessing strategies (including algorithms running on modern co-processors) asintegrated components of the DBMS. As a result, researchers can upload and analyzetheir data using efficient functions implemented inside the DBMS. Index structures aregenerated to store analysis results that may be interesting to other users, so that theresults are readily available without duplicating the analysis. We have developed aprototype of DCMS based on the PostgreSQL system and experiments using real MSdata and workload show that DCMS significantly outperforms existing MS softwaresystems. We also used it as a platform to test other data management issues such assecurity and compression."
    },
    {
        "url": "https://paperity.org/p/61713789/paradigm-shift-in-big-data-supercomputing-dataflow-vs-controlflow",
        "title": "Paradigm Shift in Big Data SuperComputing: DataFlow vs. ControlFlow",
        "authors": [
            "Nemanja Trifunovic",
            " Veljko Milutinovic",
            " Jakob Salom"
        ],
        "date_article": "05-2015",
        "short_description": "The paper discusses the shift in the computing paradigm and the programming model for Big Data problems and applications. We compare DataFlow and ControlFlow programming models through their quantity and quality aspects. Big Data problems and applications that are suitable for implementation on DataFlow computers should not be measured using the same measures as ControlFlow...",
        "keywords": [
            "-"
        ],
        "number_of_pages": "9",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-014-0010-z.pdf",
        "abstract": "The paper discusses the shift in the computing paradigm and the programmingmodel for Big Data problems and applications. We compare DataFlow andControlFlow programming models through their quantity and quality aspects. BigData problems and applications that are suitable for implementation on DataFlowcomputers should not be measured using the same measures as ControlFlowcomputers. We propose a new methodology for benchmarking, which takes intoaccount not only the execution time, but also the power and space, needed tocomplete the task. Recent research shows that if the TOP500 ranking was based onthe new performance measures, DataFlow machines would outperform ControlFlowmachines. To support the above claims, we present eight recent implementationsof various algorithms using the DataFlow paradigm, which show considerablespeed-ups, power reductions and space savings over their implementation usingthe ControlFlow paradigm.IntroductionBig Data is becoming a reality in more and more research areas every year. Also, BigData applications are becoming morevisibleas they are slowly entering areas concern-ing the general public. In other words, Big Data applications that were up to nowpresent mainly in the highly specialized areas of research, like geophysics [1,2] andfinancial engineering [3], are making its way into more general areas, like medicineand pharmacy [4], biology, aviation [5], politics, acoustics [6], etc.In the last years the ratio of data volume increase is higher than the ratio of process-ing power increase. With the growing adoption of data-collecting technologies, likesensor networks, Internet of Things, and others, the data volume growth ratio isexpected to continue to increase.Among others, one important question arises: how do we process such quantities ofdata. One possible answer lies is in the shift of the computing paradigm and theprogramming model. With Big Data problems, it is many times more reasonable toconcentrate on data rather than on the process. This can be achieved by employingDataFlow computing paradigm, programming model, and computers.Background and literature reviewThe strength of DataFlow, compared to ControlFlow computers is in the fact that theyaccelerate the data flows and application loops from 10× to 1000×. How many orders© 2015 Trifunovic et al.; licensee Springer. This is an Open Access article distributed under the terms of the Creative CommonsAttribution License (http://creativecommons.org/licenses/by/4.0), which permits unrestricted use, distribution, and reproduction in anymedium, provided the original work is properly credited.Trifunovicet al. Journal of Big Data (2015) 2:4 DOI 10.1186/s40537-014-0010-z\nof magnitude depends on the amount of data reusability within the loops. This featureis enabled by compiling down to levels much below the machine code, which bringsimportant additional effects: much lower execution time, equipment size, and powerdissipation.The above strengths can prove especially important in Big Data applications that canbenefit from one or more of the DataFlow advantages. For instance:–A daily periodic Big Data application, which would not finish in time, if executedon a ControlFlow computer, executes in time on a DataFlow computer of the sameequipment size and power dissipation,–A Big Data application with limited space and/or power resources (remote locationssuch as ships, research stations, etc.) executes in a reasonable amount of time,–With Big Data applications, where execution time is not a prime concern, DataFlowcomputers can save space and energy.The previous paper [7] argues that time has come to redefineTOP500benchmarking.Concrete measurement data from real applications in geophysics [1,2], financial engin-eering [3], and some other research fields [8,9,10-12], shows that a DataFlow machine(for example, the Maxeler MAX series) rates better than a ControlFlow machine (forexample, Cray Titan), if a different benchmark is used (e.g., a Big Data benchmark), aswell as a different ranking methodology (e.g., the benchmark execution time multipliedby the number of 1U boxes needed to accomplish the given execution time - 1U boxrepresents one rack unit or equivalent - it is assumed, no matter what technology isinside, the 1U box always has the same size and always uses the same power).In reaction to the previous paper [7], scientific community insists that more light isshed on two issues: (a) Programming paradigm and (b) Benchmarking methodology.Consequently the stress of this viewpoint is on these two issues.DiscussionWhat is the fastest, the least complex, and the least power consuming way to do(Big Data) computing?Answer: Rather than writing one program to control the flow of data through thecomputer, one has to write a program to configure the hardware of the computer, sothat input data, when it arrives, can flow through the computer hardware in only oneway - the way how the computer hardware has been configured. This is best achieved ifthe serial part of the application (the transactions) continues to run on the ControlFlowhost and the parallel part of the application is migrated into a DataFlow accelerator. ADataFlow part of the application does (parallel) Big Data crunching and execution ofloops.The early works of Dennis [13] and Arvind [14] could prove the concept, but couldnot result in commercial successes for three reasons: (a) Reconfigurable hardware tech-nology was not yet ready. Contemporary ASIC was fast enough but not reconfigurable,while reconfigurable FPGA was nonexistent; (b) System software technology was notyet ready. Methodologies for fast creation of system software did exist, but effectivetools for large scale efforts of this sort did not; and (c) Applications of those days werenot of the Big Data type, so the streaming capabilities of the DataFlow computingTrifunovicet al. Journal of Big Data (2015) 2:4 Page 2 of 9"
    },
    {
        "url": "https://paperity.org/p/59791832/a-survey-on-platforms-for-big-data-analytics",
        "title": "A survey on platforms for big data analytics",
        "authors": [
            "Dilpreet Singh",
            " Chandan K Reddy"
        ],
        "date_article": "04-2015",
        "short_description": "The primary purpose of this paper is to provide an in-depth analysis of different platforms available for performing big data analytics. This paper surveys different hardware platforms available for big data analytics and assesses the advantages and drawbacks of each of these platforms based on various metrics such as scalability, data I/O rate, fault tolerance, real-time...",
        "keywords": [
            "Big data",
            "MapReduce",
            "graphics processing units",
            "scalability",
            "big data analytics",
            "big data platforms",
            "k-means clustering",
            "real-time processing"
        ],
        "number_of_pages": "20",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-014-0008-6.pdf",
        "abstract": "The primary purpose of this paper is to provide an in-depth analysis of differentplatforms available for performing big data analytics. This paper surveys differenthardware platforms available for big data analytics and assesses the advantages anddrawbacks of each of these platforms based on various metrics such as scalability,data I/O rate, fault tolerance, real-time processing, data size supported and iterativetask support. In addition to the hardware, a detailed description of the softwareframeworks used within each of these platforms is also discussed along with theirstrengths and drawbacks. Some of the critical characteristics described here canpotentially aid the readers in making an informed decision about the right choice ofplatforms depending on their computational needs. Using a star ratings table, arigorous qualitative comparison between different platforms is also discussed foreach of the six characteristics that are critical for the algorithms of big data analytics.In order to provide more insights into the effectiveness of each of the platform inthe context of big data analytics, specific implementation level details of the widelyused k-means clustering algorithm on various platforms are also described in theform pseudocode."
    },
    {
        "url": "https://paperity.org/p/60025223/deep-learning-applications-and-challenges-in-big-data-analytics",
        "title": "Deep learning applications and challenges in big data analytics",
        "authors": [
            "Maryam M Najafabadi",
            " Flavio Villanustre",
            " Taghi M Khoshgoftaar"
        ],
        "date_article": "04-2015",
        "short_description": "Big Data Analytics and Deep Learning are two high-focus of data science. Big Data has become important as many organizations both public and private have been collecting massive amounts of domain-specific information, which can contain useful information about problems such as national intelligence, cyber security, fraud detection, marketing, and medical informatics. Companies...",
        "keywords": [
            "-"
        ],
        "number_of_pages": "21",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-014-0007-7.pdf",
        "abstract": "Big Data Analytics and Deep Learning are two high-focus of data science. Big Data hasbecome important as many organizations both public and private have beencollecting massive amounts of domain-specific information, which can contain usefulinformation about problems such as national intelligence, cyber security, frauddetection, marketing, and medical informatics. Companies such as Google andMicrosoft are analyzing large volumes of data for business analysis and decisions,impacting existing and future technology. Deep Learning algorithms extract high-level,complex abstractions as data representations through a hierarchical learning process.Complex abstractions are learnt at a given level based on relatively simpler abstractionsformulated in the preceding level in the hierarchy. A key benefit of Deep Learning is theanalysis and learning of massive amounts of unsupervised data, making it a valuabletool for Big Data Analytics where raw data is largely unlabeled and un-categorized. Inthe present study, we explore how Deep Learning can be utilized for addressing someimportant problems in Big Data Analytics, including extracting complex patterns frommassive volumes of data, semantic indexing, data tagging, fast information retrieval,and simplifying discriminative tasks. We also investigate some aspects of Deep Learningresearch that need further exploration to incorporate specific challenges introduced byBig Data Analytics, including streaming data, high-dimensional data, scalability ofmodels, and distributed computing. We conclude by presenting insights into relevantfuture works by posing some questions, including defining data sampling criteria,domain adaptation modeling, defining criteria for obtaining useful data abstractions,improving semantic indexing, semi-supervised learning, and active learning."
    },
    {
        "url": "https://paperity.org/p/59996470/intrusion-detection-and-big-heterogeneous-data-a-survey",
        "title": "Intrusion detection and Big Heterogeneous Data: a Survey",
        "authors": [
            "Richard Zuech",
            " Taghi M Khoshgoftaar",
            " Randall Wald"
        ],
        "date_article": "04-2015",
        "short_description": "Intrusion Detection has been heavily studied in both industry and academia, but cybersecurity analysts still desire much more alert accuracy and overall threat analysis in order to secure their systems within cyberspace. Improvements to Intrusion Detection could be achieved by embracing a more comprehensive approach in monitoring security events from many different heterogeneous...",
        "keywords": [
            "-"
        ],
        "number_of_pages": "41",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-015-0013-4.pdf",
        "abstract": "Intrusion Detection has been heavily studied in both industry and academia, butcybersecurity analysts still desire much more alert accuracy and overall threat analysisin order to secure their systems within cyberspace. Improvements to IntrusionDetection could be achieved by embracing a more comprehensive approach inmonitoring security events from many different heterogeneous sources. Correlatingsecurity events from heterogeneous sources can grant a more holistic view and greatersituational awareness of cyber threats. One problem with this approach is thatcurrently, even a single event source (e.g., network traffic) can experience Big Datachallenges when considered alone. Attempts to use more heterogeneous data sourcespose an even greater Big Data challenge. Big Data technologies for Intrusion Detectioncan help solve these Big Heterogeneous Data challenges. In this paper, we review thescope of works considering the problem of heterogeneous data and in particular BigHeterogeneous Data. We discuss the specific issues of Data Fusion, HeterogeneousIntrusion Detection Architectures, and Security Information and Event Management(SIEM) systems, as well as presenting areas where more research opportunities exist.Overall, both cyber threat analysis and cyber intelligence could be enhanced bycorrelating security events across many diverse heterogeneous sources."
    },
    {
        "url": "https://paperity.org/p/59967717/contextual-anomaly-detection-framework-for-big-sensor-data",
        "title": "Contextual anomaly detection framework for big sensor data",
        "authors": [
            "Michael A Hayes",
            " Miriam AM Capretz"
        ],
        "date_article": "02-2015",
        "short_description": "The ability to detect and process anomalies for Big Data in real-time is a difficult task. The volume and velocity of the data within many systems makes it difficult for typical algorithms to scale and retain their real-time characteristics. The pervasiveness of data combined with the problem that many existing algorithms only consider the content of the data source; e.g. a...",
        "keywords": [
            "-"
        ],
        "number_of_pages": "22",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2Fs40537-014-0011-y.pdf",
        "abstract": "The ability to detect and process anomalies for Big Data in real-time is a difficult task.The volume and velocity of the data within many systems makes it difficult for typicalalgorithms to scale and retain their real-time characteristics. The pervasiveness of datacombined with the problem that many existing algorithms only consider the content ofthe data source; e.g. a sensor reading itself without concern for its context, leaves roomfor potential improvement. The proposed work defines a contextual anomaly detectionframework. It is composed of two distinct steps: content detection and contextdetection. The content detector is used to determine anomalies in real-time, whilepossibly, and likely, identifying false positives. The context detector is used to prune theoutput of the content detector, identifying those anomalies which are considered bothcontent and contextually anomalous. The context detector utilizes the concept ofprofiles, which are groups of similarly grouped data points generated by a multivariateclustering algorithm. The research has been evaluated against two real-world sensordatasets provided by a local company in Brampton, Canada. Additionally, theframework has been evaluated against the open-source Dodgers dataset, available atthe UCI machine learning repository, and against the R statistical toolbox."
    },
    {
        "url": "https://paperity.org/p/61767661/comparative-study-between-incremental-and-ensemble-learning-on-data-streams-case-study",
        "title": "Comparative study between incremental and ensemble learning on data streams: Case study",
        "authors": [
            "Wenyu Zang",
            " Peng Zhang",
            " Chuan Zhou"
        ],
        "date_article": "12-2014",
        "short_description": "With unlimited growth of real-world data size and increasing requirement of real-time processing, immediate processing of big stream data has become an urgent problem. In stream data, hidden patterns commonly evolve over time (i.e.,concept drift), where many dynamic learning strategies have been proposed, such as the incremental learning and ensemble learning. To the best of our...",
        "keywords": [
            "-"
        ],
        "number_of_pages": "16",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2F2196-1115-1-5.pdf",
        "abstract": "With unlimited growth of real-world data size and increasing requirement of real-timeprocessing, immediate processing of big stream data has become an urgent problem.In stream data, hidden patterns commonly evolve over time (i.e.,concept drift), wheremany dynamic learning strategies have been proposed, such as the incrementallearning and ensemble learning. To the best of our knowledge, there is no worksystematically compare these two methods. In this paper we conduct comparativestudy between theses two learning methods. We first introduce the concept of“concept drift”, and propose how to quantitatively measure it. Then, we recall thehistory of incremental learning and ensemble learning, introducing milestones of theirdevelopments. In experiments, we comprehensively compare and analyze theirperformancesw.r.t.accuracy and time efficiency, under various concept drift scenarios.We conclude with several future possible research problems."
    },
    {
        "url": "https://paperity.org/p/61738908/airline-new-customer-tier-level-forecasting-for-real-time-resource-allocation-of-a-miles",
        "title": "Airline new customer tier level forecasting for real-time resource allocation of a miles program",
        "authors": [
            "Jose Berengueres",
            " Dmitry Efimov"
        ],
        "date_article": "12-2014",
        "short_description": "This is a case study on an airline’s miles program resource optimization. The airline had a large miles loyalty program but was not taking advantage of recent data mining techniques. As an example, to predict whether in the coming month(s), a new passenger would become a privileged frequent flyer or not, a linear extrapolation of the miles earned during the past months was used...",
        "keywords": [
            "Airline",
            "Customer equity",
            "Customer profitability",
            "Life time value",
            "Loyalty program",
            "Frequent flyer",
            "FFP",
            "Miles program",
            "Churn",
            "Data mining"
        ],
        "number_of_pages": "13",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2F2196-1115-1-3.pdf",
        "abstract": "This is a case study on an airline’s miles program resource optimization. The airlinehad a large miles loyalty program but was not taking advantage of recent data miningtechniques. As an example, to predict whether in the coming month(s), a new passengerwould become a privileged frequent flyer or not, a linear extrapolation of themiles earned during the past months was used. This information was then usedin CRM interactions between the airline and the passenger. The correlation ofextrapolation with whether a new userwould attain a privileged miles statuswas 39% when one month of data was used to make a prediction. In contrast,when GBM and other blending techniques were used, a correlation of 70% wasachieved. This corresponded to a prediction accuracy of 87% with less than 3%false positives. The accuracy reached 97% if three months of data instead ofone were used. An application that ranks users according to their probability to becomepart of privileged miles-tier was proposed. The application performs real time allocationof limited resources such as available upgrades on a given flight. Moreover, the airlinecan assign now those resources to the passengers with the highest revenue potentialthus increasing the perceived value of the program at no extra cost."
    },
    {
        "url": "https://paperity.org/p/61710155/cultivating-a-research-agenda-for-data-science",
        "title": "Cultivating a research agenda for data science",
        "authors": [
            "Chris A Mattmann"
        ],
        "date_article": "12-2014",
        "short_description": "I describe a research agenda for data science based on a decade of research and operational work in data-intensive systems at NASA, the University of Southern California, and in the context of open source work at the Apache Software Foundation. My vision is predicated on understanding the architecture for grid computing; on flexible and automated approaches for selecting data...",
        "keywords": [
            "-"
        ],
        "number_of_pages": "8",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2F2196-1115-1-6.pdf",
        "abstract": "I describe a research agenda for data science based on a decade of research andoperational work in data-intensive systems at NASA, the University of SouthernCalifornia, and in the context of open source work at the Apache Software Foundation.My vision is predicated on understanding the architecture for grid computing; onflexible and automated approaches for selecting data movement technologies and ontheir use in data systems; on the recent emergence of cloud computing for processingand storage, and on the unobtrusive and automated integration of scientificalgorithms into data systems. Advancements in each of these areas are a core need,and they will fundamentally improve our understanding of data science, and big data.This paper identifies and highlights my own personal experience and opinion growinginto a data scientist."
    },
    {
        "url": "https://paperity.org/p/61542911/v-terrafly-large-scale-distributed-spatial-data-visualization-with-autonomic-resource",
        "title": "v-TerraFly: large scale distributed spatial data visualization with autonomic resource management",
        "authors": [
            "Yun Lu",
            " Ming Zhao",
            " Lixi Wang"
        ],
        "date_article": "12-2014",
        "short_description": "GIS application hosts are becoming more and more complicated. Theses hosts’ management is becoming more time consuming and less reliabale decreases with the increase in complexity of GIS applications. The resource management of GIS applications is becoming increasingly important in order to deliver to the user the desired Quality of Service. Map systems often serve dynamic web...",
        "keywords": [
            "-"
        ],
        "number_of_pages": "19",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2F2196-1115-1-4.pdf",
        "abstract": "GIS application hosts are becoming more and more complicated. Theses hosts’management is becoming more time consuming and less reliabale decreases withthe increase in complexity of GIS applications. The resource management of GISapplications is becoming increasingly important in order to deliver to the user thedesired Quality of Service. Map systems often serve dynamic web workloads andinvolve multiple CPU- and I/O-intensive tiers, which makes it challenging to meet theresponse time targets of map requests while using the resources efficiently. Thispaper proposes a virtualized web map service system, v-TerraFly, and its autonomicresource management in order to address this challenge. Virtualization facilitates thedeployment of web map services and improves their resource utilization throughencapsulation and consolidation. Autonomic resource management allows resourcesto be automatically provisioned to a map service and its internal tiers on demand.Specifically, this paper proposes new techniques to predict the demand of mapworkloads online and optimize resource allocations considering both response timeand data freshness as the QoS target. The proposed v-TerraFly system is prototypedon TerraFly, a production web map service, and evaluated using real TerraFlyworkloads. The results show that v-TerraFly can accurately predict the workloaddemands: 18.91% more accurate; and efficiently allocate resources to meet the QoStarget: improves the QoS by 26.19% and saves resource usages by 20.83% comparedto traditional peak-load-based resource allocation.IntroductionWith the exponential growth of the World Wide Web, there are more domains opento Geographic Information System (GIS) applications. Internet can provide informa-tion to a multitude of users, making GIS available to a wider range of public users thanever before. Web-based map services are the most important application of modernGIS systems. For example, Google Maps has more than 350 million users. There is alsoa rapidly growing number of geo-enabled applications which consume web map ser-vices on traditional computing platforms as well as the emerging mobile devices.Virtual machines (VM) are powerful platforms for hosting web map service systems.VMs support flexible resource allocation to both meet web map services systemdemands and share resources with other applications. Virtualization is also enablingtechnology for the emerging cloud computing paradigm, which further allows highly© 2014 Lu et al.; licensee Springer. This is an Open Access article distributed under the terms of the Creative Commons AttributionLicense (http://creativecommons.org/licenses/by/2.0), which permits unrestricted use, distribution, and reproduction in any medium,provided the original work is properly credited.Luet al. Journal of Big Data2014,1:4http://www.journalofbigdata.com/content/1/1/4\nscalable and cost-effective web map services hosting leveraging its elastic resourceavailability and pay-as-you-go economic model [1].However, due to the highly complex and dynamic nature of web map service systems,it is challenging to efficiently host them using virtualized resources. First, typical webmap services have to serve dynamically changing workloads, which makes it difficult tohost map services on shared resources without compromising performance or wastingresources. Second, a web map service often consists of several tiers which have differentintensive resource needs and result in dynamic internal resource contention. Third, fora typical web map service, both response time for requests and the freshness of thereturned data are critical factors of the Quality of Service (QoS) required by users.To address the above challenges, this paper presents v-TerraFly, an autonomic re-source management approach for virtualized map service systems, which can automat-ically optimize the QoS (considering both response time and data freshness) whileminimizing the resource cost [2-4].First, v-TerraFly can accurately predict the work-load demands of a web map service online based on a novel two-way forecasting algo-rithm that considers both historical hourly patterns and daily patterns.Second, basedon the predicted workload, v-TerraFly can automatically estimate the resource demandsof its various tiers based on performance profiles created using machine learning tech-niques.Third, v-TerraFly employs a new QoS model that captures the balance betweenresponse time and data freshness and uses this model to automatically optimize the re-source allocation of a web map service system [5].This proposed v-TerraFly system is realized on Hyper-V virtual machine envi-ronments and evaluated by experiments using real workloads collected from the pro-duction TerraFly system. The results show that the proposed two-level workloadprediction method is outperforms traditional exponential smoothing prediction by18.91%, and the system improves the QoS by 26.19% compared to traditional staticallynode allocation. In the meantime, it saves resource usages by 20.83% compared to trad-itional peak-load-based resource allocation.In summary, this paper’s main contributions are: (1) created a VM-based map servicesystem, v-TerraFly, which virtualizes all tiers of a typical web map service and supportsdynamic resource allocations to the different tiers; (2) proposed a novel autonomic re-source management approach for virtualized map services, which automatically allo-cates resources to different tiers of the service and optimizes the allocations based onthe performance and data freshness tradeoff; (3) evaluated v-TerraFly using real work-loads collected from production web map service system, which shows substantial im-provement on QoS and resource efficiency.The rest of this paper is organized as follows: Section Background presents the backgroundand motivations; Sections v-TerraFly describesthe architecture of v-TerraFly; Section Auto-nomic Resource Management in v-TerraFly explains the autonomic resource managementof v-TerraFly; Section Evaluation presents an experimental evaluation; Section Related workexamines related work; and Section Conclusions and future work concludes the paper.BackgroundWeb map servicesAs a promising new application of GIS, web map service exhibits its excellence in serv-ing online map requests responsively and delivering geographical information preciselyLuet al. Journal of Big Data2014,1:4Page 2 of 19http://www.journalofbigdata.com/content/1/1/4"
    },
    {
        "url": "https://paperity.org/p/61514158/a-review-of-data-mining-using-big-data-in-health-informatics",
        "title": "A review of data mining using big data in health informatics",
        "authors": [
            "Matthew Herland",
            " Taghi M Khoshgoftaar",
            " Randall Wald"
        ],
        "date_article": "12-2014",
        "short_description": "The amount of data produced within Health Informatics has grown to be quite vast, and analysis of this Big Data grants potentially limitless possibilities for knowledge to be gained. In addition, this information can improve the quality of healthcare offered to patients. However, there are a number of issues that arise when dealing with these vast quantities of data, especially...",
        "keywords": [
            "-"
        ],
        "number_of_pages": "35",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2F2196-1115-1-2.pdf",
        "abstract": "The amount of data produced within Health Informatics has grown to be quite vast,and analysis of this Big Data grants potentially limitless possibilities for knowledge to begained. In addition, this information can improve the quality of healthcare offered topatients. However, there are a number of issues that arise when dealing with these vastquantities of data, especially how to analyze this data in a reliable manner. The basicgoal of Health Informatics is to take in real world medical data from all levels of humanexistence to help advance our understanding of medicine and medical practice. Thispaper will present recent research using Big Data tools and approaches for the analysisof Health Informatics data gathered at multiple levels, including the molecular, tissue,patient, and population levels. In addition to gathering data at multiple levels, multiplelevels of questions are addressed: human-scale biology, clinical-scale, andepidemic-scale. We will also analyze and examine possible future work for each ofthese areas, as well as how combining data from each level may provide the mostpromising approach to gain the most knowledge in Health Informatics."
    },
    {
        "url": "https://paperity.org/p/61485405/a-big-data-methodology-for-categorising-technical-support-requests-using-hadoop-and",
        "title": "A big data methodology for categorising technical support requests using Hadoop and Mahout",
        "authors": [
            "Arantxa Duque Barrachina",
            " Aisling O’Drisco"
        ],
        "date_article": "12-2014",
        "short_description": "Technical Support call centres frequently receive several thousand customer queries on a daily basis. Traditionally, such organisations discard data related to customer enquiries within a relatively short period of time due to limited storage capacity. However, in recent years, the value of retaining and analysing this information has become clear, enabling call centres to...",
        "keywords": [
            "-"
        ],
        "number_of_pages": "11",
        "url_pdf": "https://link.springer.com/content/pdf/10.1186%2F2196-1115-1-1.pdf",
        "abstract": "Technical Support call centres frequently receive several thousand customer querieson a daily basis. Traditionally, such organisations discard data related to customerenquiries within a relatively short period of time due to limited storage capacity.However, in recent years, the value of retaining and analysing this information hasbecome clear, enabling call centres to identify customer patterns, improve first callresolution and maximise daily closure rates. This paper proposes a Proof of Concept(PoC) end to end solution that utilises the Hadoop programming model, extendedecosystem and the Mahout Big Data Analytics library for categorising similar supportcalls for large technical support data sets. The proposed solution is evaluated on aVMware technical support dataset."
    }
]